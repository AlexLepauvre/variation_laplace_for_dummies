{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Note</h4><p> \n",
    "    This notebook is still work in progress and the content has not been fact checked! <a href=\"url\">here</a>.\n",
    "</p></div>\n",
    "\n",
    "# Variational Laplace\n",
    "\n",
    "We have finally made it to the main topic of the book: **variational Laplace**. We have seen in the previous section that when trying to calculate the posterior to know the distribution of the parameters of our linear model given the data, we face a problem: there is an integral in the **marginal likelihood** formula that we can't solve analytically. This means that we can't simply plug in some numbers and get a result. We would need to compute one by one the results of a function for an infinity of values, and then multiply all these values together, which is of course impossible to do. \n",
    "\n",
    "The technique of variational laplace is a method we can use to **approximate** the **marginal likelihood**. This method relies of several mathematical tricks to get there. The name Laplace comes from the fact that one of these tricks is the [Laplace Method](https://en.wikipedia.org/wiki/Laplace%27s_method), which according to the wikipedia definition is a \"technique used to approximate integrals of the form $\\int_{a}^{b} exp^(Mf(x))dx$\". The variational part comes from the fact that we use variational inference to approximate the posterior distribution by finding a simpler distribution (typically a Gaussian) that is closest to the true posterior in terms of some divergence measure, like the Kullback-Leibler divergence. Don't worry if that doesn't make sense just yet, I promise it will eventually.\n",
    "\n",
    "Variational Laplace isn't a single thing, it is a series of mathematical \"tricks\" used to approximate the posterior. We will introduce each of these tricks one at a time and in a logical order, to make clear what problem a particular trick is addressing. I will most of what I am going to be talking about from this paper [\"A primer on variational Laplace\"](https://www.sciencedirect.com/science/article/pii/S1053811923004615) (Peter Zeidman, Karl Friston, Thomas Parr, 2023). In fact most of the information is the same as in that paper, just packaged in a slightly different format that is more intuitive to me, and hopefully to you as well. One key difference is that we stick to our simple linear model to illustrate what there is to solve and how we solve it. In contrast, in the book, they use dynamical causal model (DCM) as the primary example. It is also a form of generalized linear model (GLM) that can be used to estimate connectivity between brain regions in neuroscience and it is incredibly useful. But the model itself is much more complicated. So in this book, we will stick in a first time to our simple and familiar linear model. Only in later chapters, we will extend to more complicated things like hierarchical models and whatnot.\n",
    "\n",
    "Let's dive in."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
