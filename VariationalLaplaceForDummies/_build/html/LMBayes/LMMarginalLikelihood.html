
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Marginal likelihood &#8212; Variational Laplace For Dummies</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!-- 
    this give us a css class that will be invisible only if js is disabled 
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=26a4bc78f4c0ddb94549"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549" />

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'LMBayes/LMMarginalLikelihood';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Variational Laplace" href="../VariationalLaplace.html" />
    <link rel="prev" title="Intermediary recap" href="LMIntermediaryRecap.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Variational Laplace For Dummies - Home"/>
    <img src="../_static/logo.png" class="logo__image only-dark pst-js-only" alt="Variational Laplace For Dummies - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Variational Laplace for Dummies
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../SomeIntuitions.html">Some intuitions: answering questions when faced with uncertainty</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ProbabilityDistribution.html">Probablities and probability distribution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../BayesTheorem.html">The Bayes theorem</a></li>

<li class="toctree-l1 current active has-children"><a class="reference internal" href="../LMBayes.html">Linear models and Bayes theorem</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="LMAndBayesTheorem.html">Bayes theorem applied to the linear model</a></li>
<li class="toctree-l2"><a class="reference internal" href="LMLikelihood.html">The likelihood of the estimated parameters of a linear model:</a></li>
<li class="toctree-l2"><a class="reference internal" href="LMPriors.html">The prior of the linear model</a></li>
<li class="toctree-l2"><a class="reference internal" href="LMIntermediaryRecap.html">Intermediary recap</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Marginal likelihood</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../VariationalLaplace.html">Variational Laplace</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../VL/JensenInequality.html">Jensen inequality: from an intractable integral to an optimization problem</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/AlexLepauvre/variation_laplace_for_dummies.git" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/AlexLepauvre/variation_laplace_for_dummies.git/issues/new?title=Issue%20on%20page%20%2FLMBayes/LMMarginalLikelihood.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/LMBayes/LMMarginalLikelihood.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Marginal likelihood</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-marginal-likelihood-and-the-issue-of-integration">The marginal likelihood and the issue of integration</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-do-we-do-now">What do we do now?</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <div class="alert alert-info"><h4>Note</h4><p> 
    This notebook is still work in progress and the content has not been fact checked! <a href="url">here</a>.
</p></div>
<section class="tex2jax_ignore mathjax_ignore" id="marginal-likelihood">
<h1>Marginal likelihood<a class="headerlink" href="#marginal-likelihood" title="Link to this heading">#</a></h1>
<p>With everything we have seen so far, we are ready to tackle the marginal likelihhood of our linear model. The marginal likelihood is the denominator of the Bayes theorem:</p>
<div class="math notranslate nohighlight">
\[P(\Theta|y) = \frac{P(y|\theta)P(\Theta)}{P(y)}\]</div>
<p>And it is equivalent to the integral of the numerator, so that the whole function integrates to 1, so:</p>
<div class="math notranslate nohighlight">
\[P(\Theta|y) = \frac{P(y|\theta)P(\Theta)}{\int P(y|\theta)P(\Theta)}\]</div>
<p>By now, we know that the numerator is:</p>
<div class="math notranslate nohighlight">
\[P(y|\theta)P(\Theta) = (\frac{1}{\sqrt{2\pi\sigma^2}})^nexp(-\frac{1}{2\sigma^2}(y - \bold{X}\bold{\beta})^T(y-\bold{X}\bold{\beta})) \times \Bigg(\frac{1}{(2\pi)^{p/2}|\mathcal{\Sigma}|^{1/2}}exp(-\frac{1}{2}(\mathcal{\beta} - \mathcal{\mu})^T\Sigma^{-1}(\mathcal{\beta}-\mathcal{\mu})) \times \frac{b^\alpha}{\Gamma(\alpha)}(1/x)^{\alpha+1}exp(-b/x)\Bigg)\]</div>
<p>(<span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(b\)</span> are the parameter of the prior distribution of <span class="math notranslate nohighlight">\(\sigma^2\)</span>)</p>
<p>That’s a big scary formula, but that’s not a problem. Note that here, everything is expressed in vector form, otherwise it would be way too long. We can plug in the various bits and pieces and compute the numerator for all values of the parameters, given our data. In fact, we can write a simple python function to express it, by simply combining the various functions we have already created!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">gamma</span>

<span class="k">def</span> <span class="nf">lm_likelihood</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the likelihood of observing y given X for parameters beta and sigma.</span>
<span class="sd">    </span>
<span class="sd">    Parameters:</span>
<span class="sd">    - y : array-like of shape (n,), observed values</span>
<span class="sd">    - X : array-like of shape (n, p), predictor values (should include a column of ones if intercept is included in beta)</span>
<span class="sd">    - beta : array-like of shape (p,), regression coefficients (including intercept if X includes a column of ones)</span>
<span class="sd">    - sigma : float, standard deviation of the error term</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    - likelihood : float, the likelihood of the observed data given the parameters</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Ensure inputs are numpy arrays</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span>
    
    <span class="c1"># Check dimensions</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">n</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The number of rows in X must match the length of y.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">beta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The number of columns in X must match the length of beta.&quot;</span><span class="p">)</span>
    
    <span class="c1"># Calculate the predicted values using the linear model</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">beta</span>  <span class="c1"># Matrix multiplication</span>
    
    <span class="c1"># Compute the squared residuals</span>
    <span class="n">squared_residuals</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
    
    <span class="c1"># Calculate the likelihood</span>
    <span class="n">norm_const</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span> <span class="o">**</span> <span class="n">n</span>
    <span class="n">likelihood</span> <span class="o">=</span> <span class="n">norm_const</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">squared_residuals</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
    
    <span class="k">return</span> <span class="n">likelihood</span>


<span class="k">def</span> <span class="nf">multivariate_normal_pdf</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">Sigma</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the probability density function of a multivariate normal distribution.</span>
<span class="sd">    </span>
<span class="sd">    Parameters:</span>
<span class="sd">    - beta : np.ndarray</span>
<span class="sd">        A 1D array of shape (p,) representing the point at which to evaluate the PDF.</span>
<span class="sd">    - mu : np.ndarray</span>
<span class="sd">        A 1D array of shape (p,) representing the mean vector of the distribution.</span>
<span class="sd">    - Sigma : np.ndarray</span>
<span class="sd">        A 2D array of shape (p, p) representing the covariance matrix of the distribution.</span>
<span class="sd">        </span>
<span class="sd">    Returns:</span>
<span class="sd">    - float</span>
<span class="sd">        The value of the PDF evaluated at beta.</span>
<span class="sd">        </span>
<span class="sd">    Notes:</span>
<span class="sd">    The multivariate normal PDF is given by:</span>
<span class="sd">    </span>
<span class="sd">        P(beta) = (1 / ((2 * pi)^(p/2) * |Sigma|^(1/2))) * </span>
<span class="sd">                  exp(-0.5 * (beta - mu)^T * Sigma^{-1} * (beta - mu))</span>
<span class="sd">                  </span>
<span class="sd">    where:</span>
<span class="sd">    - p is the dimensionality of beta,</span>
<span class="sd">    - |Sigma| is the determinant of the covariance matrix,</span>
<span class="sd">    - Sigma^{-1} is the inverse of the covariance matrix.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span>
    <span class="n">Sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">Sigma</span><span class="p">)</span>
    
    <span class="c1"># Ensure that beta and mu are 1D arrays</span>
    <span class="k">if</span> <span class="n">beta</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">mu</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;beta and mu must be 1-dimensional arrays.&quot;</span><span class="p">)</span>
    
    <span class="c1"># Ensure that Sigma is a 2D square matrix</span>
    <span class="k">if</span> <span class="n">Sigma</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">2</span> <span class="ow">or</span> <span class="n">Sigma</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">Sigma</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Sigma must be a 2-dimensional square matrix.&quot;</span><span class="p">)</span>
    
    <span class="n">p</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="c1"># Check that the dimensions match</span>
    <span class="k">if</span> <span class="n">mu</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">p</span> <span class="ow">or</span> <span class="n">Sigma</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">p</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Dimensions of beta, mu, and Sigma do not match.&quot;</span><span class="p">)</span>
    
    <span class="c1"># Compute the determinant and inverse of Sigma</span>
    <span class="n">det_Sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">det</span><span class="p">(</span><span class="n">Sigma</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">det_Sigma</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The covariance matrix Sigma must be positive definite.&quot;</span><span class="p">)</span>
    
    <span class="n">inv_Sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">Sigma</span><span class="p">)</span>
    
    <span class="c1"># Compute the normalization constant</span>
    <span class="n">norm_const</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">p</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">det_Sigma</span><span class="p">))</span>
    
    <span class="c1"># Compute the exponent</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">-</span> <span class="n">mu</span>
    <span class="n">exponent</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">diff</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">inv_Sigma</span><span class="p">,</span> <span class="n">diff</span><span class="p">))</span>
    
    <span class="c1"># Compute the PDF value</span>
    <span class="n">pdf</span> <span class="o">=</span> <span class="n">norm_const</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">exponent</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">pdf</span>


<span class="k">def</span> <span class="nf">inv_gamma_pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the probability density function of the inverse gamma distribution.</span>
<span class="sd">    </span>
<span class="sd">    Parameters:</span>
<span class="sd">    - x : float or np.ndarray</span>
<span class="sd">        The value(s) at which to evaluate the PDF. Must be positive.</span>
<span class="sd">    - alpha : float</span>
<span class="sd">        The shape parameter of the inverse gamma distribution. Must be positive.</span>
<span class="sd">    - beta : float</span>
<span class="sd">        The scale parameter of the inverse gamma distribution. Must be positive.</span>
<span class="sd">        </span>
<span class="sd">    Returns:</span>
<span class="sd">    - float or np.ndarray</span>
<span class="sd">        The PDF of the inverse gamma distribution evaluated at x.</span>
<span class="sd">    </span>
<span class="sd">    Notes:</span>
<span class="sd">    The inverse gamma distribution PDF is given by:</span>
<span class="sd">    </span>
<span class="sd">        f(x; alpha, beta) = (beta ** alpha / gamma(alpha)) * x ** (-alpha - 1) * exp(-beta / x)</span>
<span class="sd">    </span>
<span class="sd">    where `alpha` &gt; 0 and `beta` &gt; 0.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">beta</span> <span class="o">**</span> <span class="n">alpha</span> <span class="o">/</span> <span class="n">gamma</span><span class="p">(</span><span class="n">alpha</span><span class="p">))</span> <span class="o">*</span> <span class="n">x</span> <span class="o">**</span> <span class="p">(</span><span class="o">-</span><span class="n">alpha</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">beta</span><span class="o">/</span><span class="n">x</span><span class="p">)</span> 


<span class="k">def</span> <span class="nf">bayes_numerator_lm</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">beta_prior_mu</span><span class="p">,</span> <span class="n">beta_prior_sigma</span><span class="p">,</span> <span class="n">sigma_priors</span><span class="p">):</span>
    <span class="n">likelihood</span> <span class="o">=</span> <span class="n">lm_likelihood</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
    <span class="n">beta_prior_proba</span> <span class="o">=</span> <span class="n">multivariate_normal_pdf</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">beta_prior_mu</span><span class="p">,</span> <span class="n">beta_prior_sigma</span><span class="p">)</span>
    <span class="n">sigma_prior_proba</span> <span class="o">=</span> <span class="n">inv_gamma_pdf</span><span class="p">(</span><span class="n">sigma</span><span class="p">,</span> <span class="n">sigma_priors</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">sigma_priors</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">likelihood</span> <span class="o">*</span> <span class="n">beta_prior_proba</span> <span class="o">*</span> <span class="n">sigma_prior_proba</span>
</pre></div>
</div>
</div>
</div>
<p>It’s a bit long but we get there eventually. Each bit of the numerator returns a single probability value reflecting how likely something is when the parameters are set to that value, and then we just need to multiply these probabilities together. In other words, we can pass in any values of beta and sigma, given that the data y, the regressors X and the priors stay the same, and we will get out a single value out. We can also change the data and keep everything else the same, and we will see how the results change depending on the data. Everything is possible here, we have no major issue to compute the numerator, only that the formula is a bit clunky and the code a bit long.</p>
<section id="the-marginal-likelihood-and-the-issue-of-integration">
<h2>The marginal likelihood and the issue of integration<a class="headerlink" href="#the-marginal-likelihood-and-the-issue-of-integration" title="Link to this heading">#</a></h2>
<p>That is unfortunately not the case for the marginal likelihood. Because of the integral, the marginal likelihood cannot be computed. As we saw in the case of our coin toss example, we could manipulate the various formulae to basically remove the integral symbol from the equation. We say that we found a <strong>closed form solution</strong> for the equation. If we do not find a <strong>closed form solution</strong>, we cannot solve the equation, meaning we can’t find the result of that equation. It is simple to understand why, as we briefly mentioned in the coin toss example. We are here dealing with continuous variables: the <span class="math notranslate nohighlight">\(\beta\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span> parameters can take any values between <span class="math notranslate nohighlight">\(-\infty\)</span> and <span class="math notranslate nohighlight">\(+\infty\)</span>. The integral term means that we need to take the sum of the value of the numerator for any possible values the <span class="math notranslate nohighlight">\(\beta\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span> parameter. But since there is an infinity of them (<span class="math notranslate nohighlight">\(\beta_1=0.2\)</span> but also <span class="math notranslate nohighlight">\(\beta_1=0.0000000001\)</span> and so on), we can’t possibly compute it.</p>
<p>But then, maybe we can find a trick, like in the case of the coin toss example, to get rid of that nasty integral? Let’s give it a try:</p>
<div class="math notranslate nohighlight">
\[P(y|\theta)P(\Theta) = \int(\frac{1}{\sqrt{2\pi\sigma^2}})^n\prod_{i=1}^{n}exp^{-\frac{[y_i-X\Beta]^2}{2\sigma^2}} \times \Bigg(\frac{1}{(2\pi)^{p/2}|\mathcal{\Sigma}|^{1/2}}exp(-\frac{1}{2}(\mathcal{\beta} - \mathcal{\mu})^T\Sigma^{-1}(\mathcal{\beta}-\mathcal{\mu})) \times \frac{\beta^\alpha}{\Gamma(\alpha)}(1/x)^{\alpha+1}exp(-\beta/x)\Bigg) d\beta d\sigma^2\]</div>
<p>In this case, we can’t get rid of the integral, so we can’t compute the denominator. And it’s not that I, the author of this book, failed to figure it out in this book and that it has been solved somewhere else. And it is also not such that mathematicians haven’t been able to solve it just yet and will eventually. It just is the case that this equation has no <strong>closed form solution</strong>, which is the same as saying that this equation has no <strong>analytical solution</strong>. It never will. Now maybe you will read this book and go on to pursue a successful career as a mathematician to prove me wrong and actually solve that equation analytically. If you actually were to do that, you would shatter everything we know about mathematics and be crowned the smartest person that has ever lived. It’s no exaggeration, it would be that big a deal.</p>
</section>
<section id="what-do-we-do-now">
<h2>What do we do now?<a class="headerlink" href="#what-do-we-do-now" title="Link to this heading">#</a></h2>
<p>So, we started with an easy question: “Is the length of penguin flipper correlated with their weight”? We wanted to use the Bayes theorem to know how confident we should be in the value of the <span class="math notranslate nohighlight">\(\beta_1\)</span> in our model given the data, to see how likely or unlikely it is to be 0 (or close thereof) to answer our question. To be precise, we want to compute the probability distribution of all possible values of <span class="math notranslate nohighlight">\(\beta_1\)</span> given the data, so that we can know the interval of likely values of <span class="math notranslate nohighlight">\(\beta_1\)</span>. And after all this complicated math, we arrive at a big let down: <strong>there is a bit of the Bayes theorem we can’t solve</strong> and accordingly, we can’t compute the posterior and therefore, we are stuck with our question.</p>
<p>The integral is not only a problem in the specific case of the linear model, it is the case for many statistical problems. In fact, there are probably fewer examples in which it is not a problem (such as our coin toss example) than the other way around. But lucky for us, mathematicians are stubborn and when they can’t solve an equation directly, they spend decades and centuries figuring out tricks and workarounds. To be clear: they do not eventually find a solution to an equation previoulsy thought unsolvable. They instead find ways to get to something that should be close enough, which they call approximations. In this specific case, there are two different families of strategies to adopt to deal with the unsolvable equation:</p>
<ol class="arabic simple">
<li><p>Computational approaches:</p></li>
</ol>
<ul class="simple">
<li><p>roughly speaking, throw a lot of computation resources at the problem. The least refined way you could go about is multiply the prior and likelihood for all combinations of the parameters (<span class="math notranslate nohighlight">\(\beta\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span>) ranging from large negative values to large positive ones and everyhing in between. This is never done in practice because it would be absurdly expansive computationally speaking, even with a few parameters to consider. Instead, fancier method are used, relying on so called samplers relying on Markov Monte Carlo Chains to generate numbers following complicated probabilistic rules.</p></li>
</ul>
<ol class="arabic simple" start="2">
<li><p>Variational approaches:</p></li>
</ol>
<ul class="simple">
<li><p>Solve even more complicated equations and use a very broad array of tricks to find some solvable equations that should approximate the integral. You will often hear these methods referred to as “approximate Bayes” or “approximate bayesian inference” or things along those lines. As the name suggest, these methods are not exact, but they approximate it well enough.</p></li>
</ul>
<p>As the name of the book indicates, we will focus on the second approach. And to be even more specific, we will use a specific (out of many) variational appraoch: <strong>Variational Laplace</strong>. The variational approach is tailored for linear problems, and assumes that the error are <strong>Gaussian</strong>, which is the same thing as saying normally distributed. Let’s get started</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./LMBayes"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="LMIntermediaryRecap.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Intermediary recap</p>
      </div>
    </a>
    <a class="right-next"
       href="../VariationalLaplace.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Variational Laplace</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-marginal-likelihood-and-the-issue-of-integration">The marginal likelihood and the issue of integration</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-do-we-do-now">What do we do now?</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Alex Lepauvre, Jan Gabriel Hartel
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>