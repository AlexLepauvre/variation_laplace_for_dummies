
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>The likelihood of the estimated parameters of a linear model: &#8212; Variational Laplace For Dummies</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!-- 
    this give us a css class that will be invisible only if js is disabled 
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=26a4bc78f4c0ddb94549"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549" />

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'LMBayes/LMLikelihood';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="The prior of the linear model" href="LMPriors.html" />
    <link rel="prev" title="Bayes theorem applied to the linear model" href="LMAndBayesTheorem.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Variational Laplace For Dummies - Home"/>
    <img src="../_static/logo.png" class="logo__image only-dark pst-js-only" alt="Variational Laplace For Dummies - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Variational Laplace for Dummies
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../SomeIntuitions.html">Some intuitions: answering questions when faced with uncertainty</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ProbabilityDistribution.html">Probablities and probability distribution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../BayesTheorem.html">The Bayes theorem</a></li>

<li class="toctree-l1 current active has-children"><a class="reference internal" href="../LMBayes.html">Linear models and Bayes theorem</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="LMAndBayesTheorem.html">Bayes theorem applied to the linear model</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">The likelihood of the estimated parameters of a linear model:</a></li>
<li class="toctree-l2"><a class="reference internal" href="LMPriors.html">The prior of the linear model</a></li>
<li class="toctree-l2"><a class="reference internal" href="LMIntermediaryRecap.html">Intermediary recap</a></li>
<li class="toctree-l2"><a class="reference internal" href="LMMarginalLikelihood.html">Marginal likelihood</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../VariationalLaplace.html">Variational Laplace</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../VL/JensenInequality.html">Jensen inequality: from an intractable integral to an optimization problem</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/AlexLepauvre/variation_laplace_for_dummies.git" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/AlexLepauvre/variation_laplace_for_dummies.git/issues/new?title=Issue%20on%20page%20%2FLMBayes/LMLikelihood.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/LMBayes/LMLikelihood.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>The likelihood of the estimated parameters of a linear model:</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-likelihood-of-one-observation">The likelihood of one observation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-likelihood-of-several-observation">The likelihood of several observation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-error-term-and-estimation-of-the-sigma-parameter">The error term and estimation of the <span class="math notranslate nohighlight">\(\sigma\)</span> parameter</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#likelihood-function-recap">Likelihood function: recap</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <div class="alert alert-info"><h4>Note</h4><p> 
    This notebook is still work in progress and the content has not been fact checked! <a href="url">here</a>.
</p></div>
<section class="tex2jax_ignore mathjax_ignore" id="the-likelihood-of-the-estimated-parameters-of-a-linear-model">
<h1>The likelihood of the estimated parameters of a linear model:<a class="headerlink" href="#the-likelihood-of-the-estimated-parameters-of-a-linear-model" title="Link to this heading">#</a></h1>
<p>The first need we need is a function for the likelihood of the data under given parameters. That’s the same concept as before, but it is a little bit more complicated, for several reasons. The first reason is that compared to our coin toss example where the probability of observing a given number of heads depended on a single parameter of interest (the probability of getting head), in our current problem we have several parameters: <span class="math notranslate nohighlight">\(\beta_0\)</span>, <span class="math notranslate nohighlight">\(\beta_1\)</span> (but also the error term). Accordingly, we need a likelihood function that tells us the probability of observing a given outcome <span class="math notranslate nohighlight">\(y_i\)</span> given the value of several parameters. In addition, compared to the previous example where the data were a single observation <span class="math notranslate nohighlight">\(y\)</span> (the number of head we observed in our experiment), in the current case we have many observations <span class="math notranslate nohighlight">\(y_i\)</span> (one per penguin). We will see how to deal with these two complications one by one before putting everything together.</p>
<section id="the-likelihood-of-one-observation">
<h2>The likelihood of one observation<a class="headerlink" href="#the-likelihood-of-one-observation" title="Link to this heading">#</a></h2>
<p>The first need we need is a function for the likelihood of the data under given parameters. It’s easy to get confused at this point, so let’s clarify. In Bayesian inference, the likelihood is the probability of observing the data given particular values of the parameters. It answers questions like:</p>
<ul class="simple">
<li><p>“If <span class="math notranslate nohighlight">\(\beta_0=0\)</span> and <span class="math notranslate nohighlight">\(\beta_1=0.21\)</span>, what is the probability of observing a penguin with flipper length 190mm that weight 70 kg?”</p></li>
</ul>
<p>Importantly, the likleihood is a probability density function, which is a function that can answer that question for any values of <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span>.</p>
<p>Before you continue reading, try to think about what that could look like.</p>
<p>An intuition you might have is that values of <span class="math notranslate nohighlight">\(yi\)</span>​ close to the prediction of your model should be more likely. Indeed, if you examine the error distribution histogram from earlier, you’ll notice there are fewer penguins with large errors and more with smaller errors. Formally, this suggests that the likelihood of observing the data given certain parameters <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span>​ is higher when the observed data are close to the model’s predictions.</p>
<p>Now, if you tried to look at the likelihood of the data separately for each parameter, you might expect to see something resembling a normal distribution: the data close to the expected value should be most likely, and the likelihood should decrease as you move further away. However, you cannot look at the likelihood of the data for each parameter separately; you need to consider them together. This is because the probability of observing a given data point <span class="math notranslate nohighlight">\(y_i\)</span>​ depends on both <span class="math notranslate nohighlight">\(\beta_0\)</span>​ and <span class="math notranslate nohighlight">\(\beta_1\)</span>​ simultaneously.</p>
<p>To proceed, it would be helpful to know the form of the distribution that represents the likelihood of observing a given data point given these parameters. We can achieve this by making an assumption about the error term <span class="math notranslate nohighlight">\(\epsilon\)</span>​ to ensure that the likelihood function is normally distributed.</p>
<p>Once again, our model is defined as:</p>
<div class="math notranslate nohighlight">
\[y_i = \beta_0 + \beta_1 x_i + \epsilon\]</div>
<p>So our data y_i is basically a constant (<span class="math notranslate nohighlight">\(beta_0\)</span>), plus a constant (<span class="math notranslate nohighlight">\(beta_1\)</span>) multiplied by so fixed values (flipper length, <span class="math notranslate nohighlight">\(x_i\)</span>), plus the error term. As we already mentioned, one assumption of the linear model is that the <span class="math notranslate nohighlight">\(\epsilon\)</span> is normally distributed around 0, so it is a normal distribution. This is stated as such:</p>
<div class="math notranslate nohighlight">
\[\epsilon \sim \mathcal{N}(0, \sigma^2)\]</div>
<p>Which means that the error follows a normal distribution centered on 0 and with standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span>. Actually, the very reason behind the assumption that the error should be normally distributed is so that the likelihood function also follows a normal distribution. If that’s not the case, everything breaks.</p>
<p><strong>So what’s the formula for the likelihood of a single observation <span class="math notranslate nohighlight">\(y_i\)</span> given the parameters? <span class="math notranslate nohighlight">\(P(y_i|\beta_0, \beta_1, \sigma^2)\)</span></strong></p>
<p>We won’t be able to go into the why here (perhaps we will write a separate chapter about it at some point), but the answer is this:</p>
<div class="math notranslate nohighlight">
\[P(y_i|\beta_0, \beta_1, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp{-\frac{(y_i-(\beta_0 + \beta_1*x_i))^2}{2\sigma^2}}\]</div>
<p>If you remember the normal distribution formulae from above, it’s basically the same, except we replaced the <span class="math notranslate nohighlight">\(\mu\)</span> term by: <span class="math notranslate nohighlight">\(\beta_0 + \beta_1*x_i\)</span>. This kind of makes sense when you think about it: the mean value in the case of a linear regression is the value predicted by the model. This is called the <strong>expected value</strong>. If you have your observation <span class="math notranslate nohighlight">\(y_i\)</span> very far away from your model, then <span class="math notranslate nohighlight">\((y_i-(\beta_0 + \beta_1*x_i))^2\)</span> increases, and because you have a minus in front, the exponent yields a smaller result. And so the further away your observation is from your model predicted value (<span class="math notranslate nohighlight">\(\beta_0 + \beta_1*x_i\)</span>), the more the likelihood decreases.</p>
<p>Let’s write some code to illustrate this function. We will first simulate some data then use the optimal least square method to retrive the fitted values of <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span>. Then, we will compute the error of each single point to find the data point which has the smallest error, and the one that has the largest error. Again, the likelihood function give us the likelihood of the observation given any values of the parameters. And we saw that the probability of observing the data should be concentrated around the values of <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> that result in prediction that are closest to that observation</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Load functions previously created:</span>
<span class="k">def</span> <span class="nf">linear_mdl</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">beta_0</span><span class="p">,</span> <span class="n">beta_1</span><span class="p">,</span> <span class="n">error_mu</span><span class="p">,</span> <span class="n">error_sigma</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">beta_0</span> <span class="o">+</span> <span class="n">beta_1</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">error_mu</span><span class="p">,</span> <span class="n">error_sigma</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">y</span>

<span class="k">def</span> <span class="nf">fit_linear_model</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="c1"># Calculate beta_1:</span>
    <span class="n">beta_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">x</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">*</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)))</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">x</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="c1"># Calcuate beta_0:</span>
    <span class="n">beta_0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">-</span> <span class="n">beta_1</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">beta_0</span><span class="p">,</span> <span class="n">beta_1</span>

<span class="k">def</span> <span class="nf">normal_pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
    <span class="n">p_x</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">)))</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">e</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="p">((</span><span class="n">x</span><span class="o">-</span><span class="n">mu</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">p_x</span>


<span class="c1"># Define the parameters for our simulation:</span>
<span class="n">flipper_length_mm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">170</span><span class="p">,</span> <span class="mi">230</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># Let&#39;s say we collected the flipper length of penguins and that these are between 170 and 230mm</span>

<span class="k">def</span> <span class="nf">single_obs_likelihood</span><span class="p">(</span><span class="n">yi</span><span class="p">,</span> <span class="n">xi</span><span class="p">,</span> <span class="n">b0</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the likelihood of observing yi given xi for parameters b0, b1, and sigma.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Compute square of the error</span>
    <span class="n">sq_err</span> <span class="o">=</span> <span class="p">(</span><span class="n">yi</span> <span class="o">-</span> <span class="p">(</span><span class="n">b0</span> <span class="o">+</span> <span class="n">b1</span> <span class="o">*</span> <span class="n">xi</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span>
    <span class="c1"># Compute normalization constant</span>
    <span class="n">norm_k</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

    <span class="c1"># Compute hte likelihood</span>
    <span class="k">return</span> <span class="n">norm_k</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">sq_err</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">)))</span> 

<span class="c1"># Set the parameters:</span>
<span class="n">beta_0_gt</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">beta_1_gt</span> <span class="o">=</span> <span class="mf">0.22</span>
<span class="n">error_mu</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">error_sigma</span> <span class="o">=</span> <span class="mi">2</span>

<span class="c1"># Simulate some data:</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">linear_mdl</span><span class="p">(</span><span class="n">flipper_length_mm</span><span class="p">,</span> <span class="n">beta_0_gt</span><span class="p">,</span> <span class="n">beta_1_gt</span><span class="p">,</span> <span class="n">error_mu</span><span class="p">,</span> <span class="n">error_sigma</span><span class="p">)</span>

<span class="c1"># Fit the model:</span>
<span class="n">beta_0</span><span class="p">,</span> <span class="n">beta_1</span> <span class="o">=</span> <span class="n">fit_linear_model</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">flipper_length_mm</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Estimated beta_1=</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">beta_1</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Estimated beta_0=</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">beta_0</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># Compute the error:</span>
<span class="n">err</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="p">(</span><span class="n">beta_0</span> <span class="o">+</span> <span class="n">beta_1</span> <span class="o">*</span> <span class="n">flipper_length_mm</span><span class="p">)</span>

<span class="c1"># Find the minimal error:</span>
<span class="n">min_error</span> <span class="o">=</span> <span class="n">err</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">err</span><span class="o">**</span><span class="mi">2</span><span class="p">)]</span>
<span class="n">predicted_val</span> <span class="o">=</span> <span class="n">beta_0</span> <span class="o">+</span> <span class="n">beta_1</span> <span class="o">*</span> <span class="n">flipper_length_mm</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">err</span><span class="o">**</span><span class="mi">2</span><span class="p">)]</span>
<span class="n">observed_val</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">err</span><span class="o">**</span><span class="mi">2</span><span class="p">)]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="o">*</span><span class="mi">40</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The smallest error is of </span><span class="si">{</span><span class="n">min_error</span><span class="si">}</span><span class="s2"> kg&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The model predicted </span><span class="si">{</span><span class="n">predicted_val</span><span class="si">}</span><span class="s2"> kg&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The observed weigth is </span><span class="si">{</span><span class="n">observed_val</span><span class="si">}</span><span class="s2"> kg&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Estimated beta_1=0.204
Estimated beta_0=3.642
----------------------------------------
The smallest error is of 0.009047247692230087 kg
The model predicted 43.202129988025746 kg
The observed weigth is 43.211177235717976 kg
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">cm</span>

<span class="c1"># Plot the probability of each observation across several values of b0 and b1:</span>
<span class="n">b0s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">beta_0_gt</span> <span class="o">-</span><span class="mi">200</span><span class="p">,</span> <span class="n">beta_0_gt</span> <span class="o">+</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">b1s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">beta_1_gt</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">beta_1_gt</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">B0s</span><span class="p">,</span> <span class="n">B1s</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">b0s</span><span class="p">,</span> <span class="n">b1s</span><span class="p">)</span>

<span class="c1"># Compute the likelihood for the value with the smallest error:</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Computing the likelihood of y=</span><span class="si">{</span><span class="n">y</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">err</span><span class="o">**</span><span class="mi">2</span><span class="p">)]</span><span class="si">}</span><span class="s2"> for the penguin with flipper length=</span><span class="si">{</span><span class="n">flipper_length_mm</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">err</span><span class="o">**</span><span class="mi">2</span><span class="p">)]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">likelihood</span> <span class="o">=</span> <span class="n">single_obs_likelihood</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">err</span><span class="o">**</span><span class="mi">2</span><span class="p">)],</span> <span class="n">flipper_length_mm</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">err</span><span class="o">**</span><span class="mi">2</span><span class="p">)],</span> <span class="n">B0s</span><span class="p">,</span> <span class="n">B1s</span><span class="p">,</span> <span class="n">error_sigma</span><span class="p">)</span>
<span class="c1"># Normalize by max to improve viz</span>
<span class="n">likelihood</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">likelihood</span><span class="p">)</span>
<span class="c1"># Create the plot</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
<span class="c1"># Plot the surface</span>
<span class="n">surf</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">B0s</span><span class="p">,</span> <span class="n">B1s</span><span class="p">,</span> <span class="n">likelihood</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cm</span><span class="o">.</span><span class="n">viridis</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">antialiased</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="c1"># Customize the axes</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\beta_0$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\beta_1$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$P(y|\beta_0, \beta_1, \sigma)$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Likelihood Function for a Single Data Point in Linear Regression&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="c1"># Adjust viewing angle for better visualization</span>
<span class="n">ax</span><span class="o">.</span><span class="n">view_init</span><span class="p">(</span><span class="n">elev</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">azim</span><span class="o">=-</span><span class="mi">30</span><span class="p">)</span>
<span class="c1"># Add a color bar</span>
<span class="n">fig</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">surf</span><span class="p">,</span> <span class="n">shrink</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Computing the likelihood of y=43.211177235717976 for the penguin with flipper length=194
</pre></div>
</div>
<img alt="../_images/e8a39dba382775fe6006a4cb0c6df9aa2719319d5cf0ae3a5e349149579db006.png" src="../_images/e8a39dba382775fe6006a4cb0c6df9aa2719319d5cf0ae3a5e349149579db006.png" />
</div>
</div>
<p>This probably doesn’t look like what you’d have expected (at least it doesn’t look like what I would have expected). The results look like there is a ridge along which the probability of the data is very high, and everyhing outside of it is very low. This shape is however to be expected. This is because with a single data point, there isn’t enough data to determine the likelihood of the data given the parameter. In the current example, we are looking at the likelihood of observing a penguin weighting 43.85Kg with flipper size of 196mm, given the values of <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> and our model <span class="math notranslate nohighlight">\(y = \beta_0 + \beta_1 * x\)</span>. But of course, there are many possible values of <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> that can yield 43.85:</p>
<div class="math notranslate nohighlight">
\[\beta_0 + \beta_1 * 196 = 43.85\]</div>
<p>There is an infinity of solution:</p>
<p>If we fix <span class="math notranslate nohighlight">\(\beta_0\)</span> at 1, then:</p>
<div class="math notranslate nohighlight">
\[\beta_1 = (43.85 - 1) / 196\]</div>
<p>This is why the probability density function of observing a given value given any value of <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(beta_1\)</span> does not have a single peak, because there are many configuration of parameters under which the data are equally likely.</p>
<p>This may seem confusing, but don’t worry if you don’t understand everything, it’s difficult to get good intuition for these things. What matters is that we have the formulae to get the likelihood of a single observation. So now we can tackle the second complication to get the likelihood for our model</p>
</section>
<section id="the-likelihood-of-several-observation">
<h2>The likelihood of several observation<a class="headerlink" href="#the-likelihood-of-several-observation" title="Link to this heading">#</a></h2>
<p>Now that we know the likelihood for a single observation, we can try and understand how we can deal with many observation. Indeed, for our current model, we want to know not only the likelihood of observing a value <span class="math notranslate nohighlight">\(y_i\)</span> but rather the likelihood of observing all the data <span class="math notranslate nohighlight">\(y\)</span> (all the penguins with their flipper length and weights) given the parameters. For this, we need to call upon an additional assumption of a linear model: the data should be independently and identically distributed, which relates ot our previous assumption about the normal distribution of the error. What that means is that we need to assume that each observation is independent from each other. If that is the case (which it should always be if you are running a linear model), then the likelihood of observing all data point is the product of the likelihood of each data point:</p>
<div class="math notranslate nohighlight">
\[P(y|\beta_0, \beta_1, \sigma) = P(y_1|\beta_0, \beta_1, \sigma) * P(y_2|\beta_0, \beta_1, \sigma) * ... P(y_n|\beta_0, \beta_1, \sigma)\]</div>
<p>Where <span class="math notranslate nohighlight">\(n\)</span> is the number of observations (i.e. the number of penguins). You can rewrite the expression above like so:</p>
<div class="math notranslate nohighlight">
\[P(y|\beta_0, \beta_1, \sigma) = \prod_{i=1}^{n} P(y_i|\beta_0, \beta_1, \sigma)\]</div>
<p>The symbol <span class="math notranslate nohighlight">\(\prod\)</span> means multiplication from <span class="math notranslate nohighlight">\(i \to n\)</span>, similar to the <span class="math notranslate nohighlight">\(\sum\)</span> operator.</p>
<p>And so if we replace <span class="math notranslate nohighlight">\(P(y_i|\beta_0, \beta_1, \sigma)\)</span> with the formulae we had before, we get the following:</p>
<div class="math notranslate nohighlight">
\[P(y|\beta_0, \beta_1, \sigma) = \prod_{i=1}^{n}[\frac{1}{\sqrt{2\pi\sigma^2}} exp^{-\frac{[y_i-(\beta_0 + \beta_1x_i)]^2}{2\sigma^2}}]\]</div>
<p>We can simplify this formulae a little, just as we did in the previous chapter. The term <span class="math notranslate nohighlight">\(\frac{1}{\sqrt{2\pi\sigma^2}}\)</span> doesn’t depend on <span class="math notranslate nohighlight">\(\beta_0\)</span> nor on <span class="math notranslate nohighlight">\(\beta_1\)</span>, sp we can take it out:</p>
<div class="math notranslate nohighlight">
\[P(y|\beta_0, \beta_1, \sigma) = (\frac{1}{\sqrt{2\pi\sigma^2}})^n\prod_{i=1}^{n}exp^{-\frac{[y_i-(\beta_0 + \beta_1x_i)]^2}{2\sigma^2}}\]</div>
<p>So again, not a crazy complicated formulae. We can implement it programmatically and then see what the likelihood of our data will look like:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">lm_likelihood</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">b0</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the likelihood of observing y given x for parameters b0, b1, and sigma.</span>
<span class="sd">    </span>
<span class="sd">        Parameters:</span>
<span class="sd">        - y : array-like, observed values</span>
<span class="sd">        - x : array-like, predictor values</span>
<span class="sd">        - b0 : float, intercept parameter</span>
<span class="sd">        - b1 : float, slope parameter</span>
<span class="sd">        - sigma : float, standard deviation of the error term</span>
<span class="sd">        </span>
<span class="sd">        Returns:</span>
<span class="sd">        - likelihood : float, the likelihood of the observed data given the parameters</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Check number of observations:</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Calculate the predicted values using the linear model</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">b0</span> <span class="o">+</span> <span class="n">b1</span> <span class="o">*</span> <span class="n">x</span>

    <span class="c1"># Compute the squared residuals (y - y_pred)^2</span>
    <span class="n">squared_residuals</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>

    <span class="c1"># Calculate the likelihood for each observation</span>
    <span class="n">norm_const</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span><span class="o">**</span><span class="n">n</span>

    <span class="c1"># Product of each likelihood:</span>
    <span class="n">likelihoods_prod</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">squared_residuals</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">))))</span>

    <span class="k">return</span> <span class="n">norm_const</span> <span class="o">*</span> <span class="n">likelihoods_prod</span>
</pre></div>
</div>
</div>
</div>
<p>Now that we have implemented the function for the likelihood, we can try plot the likelihood of our observation given various values of b0 and b1. We will do the same as before: simulate data (so that we know the ground truth), then compute the likelihood of our data given a range of <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set the parameters:</span>
<span class="n">beta_0_gt</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">beta_1_gt</span> <span class="o">=</span> <span class="mf">0.22</span>
<span class="n">error_mu</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">error_sigma</span> <span class="o">=</span> <span class="mi">2</span>

<span class="c1"># Simulate some data:</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">linear_mdl</span><span class="p">(</span><span class="n">flipper_length_mm</span><span class="p">,</span> <span class="n">beta_0_gt</span><span class="p">,</span> <span class="n">beta_1_gt</span><span class="p">,</span> <span class="n">error_mu</span><span class="p">,</span> <span class="n">error_sigma</span><span class="p">)</span>

<span class="c1"># Plot the probability of each observation across several values of b0 and b1:</span>
<span class="n">b0s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">beta_0_gt</span> <span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="n">beta_0_gt</span> <span class="o">+</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">b1s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">beta_1_gt</span> <span class="o">-</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">beta_1_gt</span> <span class="o">+</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">B0s</span><span class="p">,</span> <span class="n">B1s</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">b0s</span><span class="p">,</span> <span class="n">b1s</span><span class="p">)</span>
<span class="n">y_likelihood</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">B0s</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">b0</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">b0s</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">ii</span><span class="p">,</span> <span class="n">b1</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">b1s</span><span class="p">):</span>
        <span class="n">y_likelihood</span><span class="p">[</span><span class="n">ii</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">lm_likelihood</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">flipper_length_mm</span><span class="p">,</span> <span class="n">b0</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">error_sigma</span><span class="p">)</span>
<span class="c1"># Normalize by max to improve viz</span>
<span class="n">y_likelihood</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">y_likelihood</span><span class="p">)</span>
<span class="c1"># Create the plot</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
<span class="c1"># Plot the surface</span>
<span class="n">surf</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">B0s</span><span class="p">,</span> <span class="n">B1s</span><span class="p">,</span> <span class="n">y_likelihood</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cm</span><span class="o">.</span><span class="n">viridis</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">antialiased</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="c1"># Customize the axes</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\beta_0$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\beta_1$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$P(y|\beta_0, \beta_1, \sigma)$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Likelihood Function of all data in Linear Regression&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="c1"># Adjust viewing angle for better visualization</span>
<span class="n">ax</span><span class="o">.</span><span class="n">view_init</span><span class="p">(</span><span class="n">elev</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">azim</span><span class="o">=-</span><span class="mi">30</span><span class="p">)</span>
<span class="c1"># Add a color bar</span>
<span class="n">fig</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">surf</span><span class="p">,</span> <span class="n">shrink</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/d32485ae4df735175f5c35a430b560241ed81a023e3cb4c25ac8624c2e889786.png" src="../_images/d32485ae4df735175f5c35a430b560241ed81a023e3cb4c25ac8624c2e889786.png" />
</div>
</div>
<p>Hm, okay, that doesn’t look quite like what you may have expected. Let’s think about it for a sec. The likelihood is once again very elongated, which means that there is some sort of a correlation between <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(beta_1\)</span> in terms of likelihood. Indeed, the observed data are more likely for particular values of <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(beta_1\)</span>, but the pairs of values of <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(beta_1\)</span> for which the data are most likely clearly follow a line. Why is that? There is actually a very good reason. It’s because the different values of <span class="math notranslate nohighlight">\(beta_1\)</span>, you can adjust the intercept <span class="math notranslate nohighlight">\(\beta_0\)</span> to make them fit the data better. Or the other way around, you can offset changes in the intercept by changing the slope to make the data fit better. Here is a vizualization for why that’s the case</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot the data:</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">flipper_length_mm</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>  <span class="c1"># Plot the simulated data</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">flipper_length_mm</span><span class="p">,</span> <span class="n">beta_0</span> <span class="o">+</span> <span class="n">beta_1</span> <span class="o">*</span> <span class="n">flipper_length_mm</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Ground Truth&#39;</span><span class="p">)</span>  <span class="c1"># Plot the regression line</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">flipper_length_mm</span><span class="p">,</span> <span class="n">beta_0</span><span class="o">-</span><span class="mi">10</span> <span class="o">+</span> <span class="n">beta_1</span> <span class="o">*</span> <span class="n">flipper_length_mm</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Offset Intercept&#39;</span><span class="p">)</span>  <span class="c1"># Plot the regression line</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">flipper_length_mm</span><span class="p">,</span> <span class="n">beta_0</span><span class="o">-</span><span class="mi">10</span> <span class="o">+</span> <span class="p">(</span><span class="mf">0.05</span><span class="o">+</span><span class="n">beta_1</span><span class="p">)</span> <span class="o">*</span> <span class="n">flipper_length_mm</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-.&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Offset Intercept, corrected slope&#39;</span><span class="p">)</span>  <span class="c1"># Plot the regression line</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">flipper_length_mm</span><span class="p">,</span> <span class="n">beta_0</span> <span class="o">+</span> <span class="p">(</span><span class="o">-</span><span class="mf">0.05</span><span class="o">+</span><span class="n">beta_1</span><span class="p">)</span> <span class="o">*</span> <span class="n">flipper_length_mm</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Offset Slope&#39;</span><span class="p">)</span>  <span class="c1"># Plot the regression line</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">flipper_length_mm</span><span class="p">,</span> <span class="n">beta_0</span> <span class="o">+</span> <span class="mi">10</span> <span class="o">+</span> <span class="p">(</span><span class="o">-</span><span class="mf">0.05</span><span class="o">+</span><span class="n">beta_1</span><span class="p">)</span> <span class="o">*</span> <span class="n">flipper_length_mm</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-.&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Offset Slope, corrected intercept&#39;</span><span class="p">)</span>  <span class="c1"># Plot the regression line</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Flipper length (mm)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Body weight (kg)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/b60401cccea8a848385bfdb92aa8478f32f6a38be479f7f7ebc03676f4a366f8.png" src="../_images/b60401cccea8a848385bfdb92aa8478f32f6a38be479f7f7ebc03676f4a366f8.png" />
</div>
</div>
<p>You can see in the plot above that when the intercept parameter <span class="math notranslate nohighlight">\(\beta_0\)</span> is far from its ground truth value (as indicated by the red lines), you can still obtain a line that closely approximates the true relationship by adjusting the slope parameter <span class="math notranslate nohighlight">\(\beta_1\)</span>. Similarly, if β1β1​ deviates from its true value (as shown by the blue lines), adjusting <span class="math notranslate nohighlight">\(\beta_0\)</span> can compensate for this discrepancy, resulting in a line that aligns well with the observed data. This illustrates why there is a correlation between <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> in the likelihood function: there are multiple pairs of <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> that yield similar fits to the data. Adjusting one parameter can offset changes in the other, allowing the model to maintain a good approximation of the true relationship.</p>
<p>If you recall the formula for the likelihood function, the likelihood is higher when the squared error term <span class="math notranslate nohighlight">\([y_i-(\beta_0 + \beta_1x_i)]^2\)</span> is minimized. This means that parameter values producing predictions close to the actual observations result in a larger likelihood. Therefore, combinations of <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> that minimize this error term will make the data more probable under the model.</p>
<p>In other words, the data are most likely under parameters where the predicted values are least different from the actual observations. Because there are multiple pairs of <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> (not just the ground truth values) that can produce predictions close to the observed data—and because adjustments in one parameter can be compensated by changes in the other—we observe this elongated shape in the likelihood surface.</p>
<p>Whenever you see an elongated shape like this in a likelihood plot, it indicates that the two variables on the axes—in our case, <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span>-are correlated. It’s important to clearly understand what we mean by this correlation, so I will repeat it one last time: it means that the likelihood of the data remains high for certain combinations of <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span>, and that changing one parameter can be offset by adjusting the other to maintain a similar level of fit to the data.</p>
<p>Now you might think that in our current example, it is what it is: <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> are correlated in terms of likelihood. It is after all true that in what we saw above, there are specific pairs of <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> that yield a line that is a good approximation of the data, and simply because of how our model is constructed, these values of <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> are correlated: if we increase <span class="math notranslate nohighlight">\(\beta_0\)</span>, we can salvage it by reducing <span class="math notranslate nohighlight">\(\beta_1\)</span> and the other way around. Why would we want to do anything about it, if it is just the way it is? Well actually, we do want to do something about it, because as we will see below, it is a bit of a problem. In short, there are several related issues posed by this correlation:</p>
<ul class="simple">
<li><p>the estimates may become unreliable, because it makes it more likely when you are dealing with real data that the parameters you retrieve with the OLS method are off, because you get a wrong <span class="math notranslate nohighlight">\(\beta_0\)</span> and its compensatory <span class="math notranslate nohighlight">\(\beta_1\)</span> or the other way around</p></li>
<li><p>it increases the domain of <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> under which the data are likely, which impacts the computation of the <span class="math notranslate nohighlight">\(\sigma\)</span> parameter as we will see in a bit.</p></li>
</ul>
<p>And as it turns out, there is actually a simple way in which we can deal with this issue: centering <span class="math notranslate nohighlight">\(x\)</span>, meaning you subtract the mean flipper length from each penguin flipper length. This results in the mean of the centered <span class="math notranslate nohighlight">\(x\)</span> to be 0 instead of the actual mean value. That way, changed in <span class="math notranslate nohighlight">\(\beta_0\)</span> represents the mean of the data and <span class="math notranslate nohighlight">\(beta_1\)</span> the slope of the correlation between flipper length and weight, and <span class="math notranslate nohighlight">\(beta_0\)</span> and <span class="math notranslate nohighlight">\(beta_1\)</span> become independent. This is why it is always a good idea to center your predictors. We can see what that looks like:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set the parameters:</span>
<span class="n">beta_0_gt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">flipper_length_mm</span><span class="p">)</span>
<span class="n">beta_1_gt</span> <span class="o">=</span> <span class="mf">0.22</span>
<span class="n">error_mu</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">error_sigma</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">x_centered</span> <span class="o">=</span> <span class="n">flipper_length_mm</span> <span class="o">-</span> <span class="n">beta_0_gt</span>
<span class="c1"># Simulate some data:</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">linear_mdl</span><span class="p">(</span><span class="n">x_centered</span><span class="p">,</span> <span class="n">beta_0_gt</span><span class="p">,</span> <span class="n">beta_1_gt</span><span class="p">,</span> <span class="n">error_mu</span><span class="p">,</span> <span class="n">error_sigma</span><span class="p">)</span>

<span class="c1"># Plot the probability of each observation across several values of b0 and b1:</span>
<span class="n">b0s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">beta_0_gt</span> <span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="n">beta_0_gt</span> <span class="o">+</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">b1s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">beta_1_gt</span> <span class="o">-</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">beta_1_gt</span> <span class="o">+</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">B0s</span><span class="p">,</span> <span class="n">B1s</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">b0s</span><span class="p">,</span> <span class="n">b1s</span><span class="p">)</span>
<span class="n">y_likelihood</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">B0s</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">b0</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">b0s</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">ii</span><span class="p">,</span> <span class="n">b1</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">b1s</span><span class="p">):</span>
        <span class="n">y_likelihood</span><span class="p">[</span><span class="n">ii</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">lm_likelihood</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x_centered</span><span class="p">,</span> <span class="n">b0</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">error_sigma</span><span class="p">)</span>
<span class="c1"># Normalize by max to improve viz</span>
<span class="n">y_likelihood</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">y_likelihood</span><span class="p">)</span> 
<span class="c1"># Create the plot</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
<span class="c1"># Plot the surface</span>
<span class="n">surf</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">B0s</span><span class="p">,</span> <span class="n">B1s</span><span class="p">,</span> <span class="n">y_likelihood</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cm</span><span class="o">.</span><span class="n">viridis</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">antialiased</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="c1"># Customize the axes</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\beta_0$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\beta_1$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$P(y|\beta_0, \beta_1, \sigma)$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Likelihood Function of all data in Linear Regression&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="c1"># Adjust viewing angle for better visualization</span>
<span class="n">ax</span><span class="o">.</span><span class="n">view_init</span><span class="p">(</span><span class="n">elev</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">azim</span><span class="o">=-</span><span class="mi">30</span><span class="p">)</span>
<span class="c1"># Add a color bar</span>
<span class="n">fig</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">surf</span><span class="p">,</span> <span class="n">shrink</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/995e005150bfa2ca8b3540be7eac7bc660668d0e56afe353568a6509e233619b.png" src="../_images/995e005150bfa2ca8b3540be7eac7bc660668d0e56afe353568a6509e233619b.png" />
</div>
</div>
<p>Now that looks better: it is nice bell surface. This makes intuitive sense and is what we were expecting since the very beginning: the data are most likely around a given parameter of <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> (i.e. the peak of the bell) and they decrease when we go away from both <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> values in a way consistent with a normal distribution. Note that in order to generate this surface, we did not specify the fitted values of <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> from the optimal least square, nor the ground truth values of <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span>. Yet, the surface seems to be concentrated on a particular point that is close to what we would expect. We can find the parameters for which <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> the likelihood of our data is maximum:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The maximum likelihood of the data is observed at:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;b0=</span><span class="si">{</span><span class="n">B0s</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">y_likelihood</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">y_likelihood</span><span class="p">))][</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;b1=</span><span class="si">{</span><span class="n">B1s</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">y_likelihood</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">y_likelihood</span><span class="p">))][</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The ground truth values are:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;b0=</span><span class="si">{</span><span class="n">beta_0_gt</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;b1=</span><span class="si">{</span><span class="n">beta_1_gt</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Fitted values with OLS:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;b0=</span><span class="si">{</span><span class="n">fit_linear_model</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">x_centered</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;b1=</span><span class="si">{</span><span class="n">fit_linear_model</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">x_centered</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The maximum likelihood of the data is observed at:
b0=198.62
b1=0.21
The ground truth values are:
b0=199.00
b1=0.22
Fitted values with OLS:
b0=198.62
b1=0.21
</pre></div>
</div>
</div>
</div>
<p>The likelihood of the data is maximal for values of <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> that are almost the same as what we obtain with the OLS method. Well in fact, they would be exactly the same if we were to sample the <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> more (i.e. if instead of taking a 1000 samples in the interval we took 10000 or more). This makes sense: the OLS finds the values of <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> that minimize the error, and the likelihood of observing the data is maximum for the parameters of  <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> that minimize the error. So you can think about it that way: the OLS finds the values of  <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> under which the data are the most likely, or that the OLS finds the peak of the likelihood. It’s important to understand the distinction: the OLS returns single points, while the likelihood is a distribution.</p>
<p>This brings us to another important term you may have come across: <strong>Maximum Likelihood Estimates (MLE)</strong>. The  <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> values you get with the OLS are the maximum likelihood estimates of these parameters, because well, they are the estimates for which the likelihood is maximum. So that also adds up.</p>
<p>So here it is, we have figured out the likelihood function for our linear model:</p>
<div class="math notranslate nohighlight">
\[P(y|\beta_0, \beta_1, \sigma) = (\frac{1}{\sqrt{2\pi\sigma^2}})^n\prod_{i=1}^{n}exp^{-\frac{[y_i-(\beta_0 + \beta_1x_i)]^2}{2\sigma^2}}\]</div>
<p>and provided that the values of x are centered (i.e. there is no correlation between <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> anymore), the likelihood looks like a bell. There is one important thing we need to add for completeness’ sake. In our example, we have only one regressor. But in a linear model, we can have many (in which case we have a multiple regression). The likelihood formulae remains almost the same, only we need to add the additional betas:</p>
<div class="math notranslate nohighlight">
\[P(y|\beta_0, \beta_1, \sigma) = (\frac{1}{\sqrt{2\pi\sigma^2}})^n\prod_{i=1}^{n}exp^{-\frac{[y_i-(\sum_{k=0}^{m}{\beta_kx_k})]^2}{2\sigma^2}}\]</div>
<p>With <span class="math notranslate nohighlight">\(m\)</span> regressors <span class="math notranslate nohighlight">\(k\)</span></p>
<p>Now it becomes obvious why the matrix notation may become handy. The exponential term is a bit complicated and tough to read. We can reformulate the above like so:</p>
<div class="math notranslate nohighlight">
\[P(y|\beta_0, \beta_1, \sigma) = (\frac{1}{\sqrt{2\pi\sigma^2}})^n\prod_{i=1}^{n}exp^{-\frac{[y_i-X\Beta]^2}{2\sigma^2}}\]</div>
<p>That’s exactly the same thing, just a bit less clunky.</p>
</section>
<section id="the-error-term-and-estimation-of-the-sigma-parameter">
<h2>The error term and estimation of the <span class="math notranslate nohighlight">\(\sigma\)</span> parameter<a class="headerlink" href="#the-error-term-and-estimation-of-the-sigma-parameter" title="Link to this heading">#</a></h2>
<p>Unfortunately, we aren’t done just yet… There is one thing we haven’t talked about: what about the <span class="math notranslate nohighlight">\(\sigma\)</span> parameter? So far, we have seen that the likelihood of the data can be determined for each pair of <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> values and that we can use the OLS formula to find the peak of the likelihood (the values of <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> under which the data are most likely), but we always kept <span class="math notranslate nohighlight">\(\sigma\)</span> constant. And we always used the ground truth value of <span class="math notranslate nohighlight">\(\sigma\)</span>. Obviously, when we are dealing with actual data (rather than simulated ones), we don’t know the true value of <span class="math notranslate nohighlight">\(\sigma\)</span>, i.e. we don’t know what the true amount of variation around our predicted values (from our model) there is.</p>
<p>But we everything you have learned so far, you might already know where this is going. You might in fact wonder: “what about <span class="math notranslate nohighlight">\(\sigma\)</span>?” As in, do we actually need to do anything about it? Well the answer is no. In the previous example, we simply set value of <span class="math notranslate nohighlight">\(\sigma\)</span> to the ground truth just for conveniences’ sake and because it is not possible to draw a graph with more than 3 dimensions. But since the beginning, <span class="math notranslate nohighlight">\(\sigma\)</span> is an input to the likelihood function, which means that the likelihood function shows the likelihood of the data under any values of <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> but also <span class="math notranslate nohighlight">\(\sigma\)</span>. We can plot the likelihood function for different values of <span class="math notranslate nohighlight">\(\sigma\)</span> to see what that looks like:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set the parameters:</span>
<span class="n">beta_0_gt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">flipper_length_mm</span><span class="p">)</span>
<span class="n">beta_1_gt</span> <span class="o">=</span> <span class="mf">0.22</span>
<span class="n">error_mu</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">error_sigma</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">x_centered</span> <span class="o">=</span> <span class="n">flipper_length_mm</span> <span class="o">-</span> <span class="n">beta_0_gt</span>
<span class="c1"># Simulate some data:</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">linear_mdl</span><span class="p">(</span><span class="n">x_centered</span><span class="p">,</span> <span class="n">beta_0_gt</span><span class="p">,</span> <span class="n">beta_1_gt</span><span class="p">,</span> <span class="n">error_mu</span><span class="p">,</span> <span class="n">error_sigma</span><span class="p">)</span>

<span class="c1"># Plot the probability of each observation across several values of b0 and b1:</span>
<span class="n">b0s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">beta_0_gt</span> <span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="n">beta_0_gt</span> <span class="o">+</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">b1s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">beta_1_gt</span> <span class="o">-</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">beta_1_gt</span> <span class="o">+</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">B0s</span><span class="p">,</span> <span class="n">B1s</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">b0s</span><span class="p">,</span> <span class="n">b1s</span><span class="p">)</span>

<span class="c1"># Plot the data with different values of sigma:</span>
<span class="k">for</span> <span class="n">sigma</span> <span class="ow">in</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]:</span>
    <span class="n">y_likelihood</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">B0s</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">b0</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">b0s</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">ii</span><span class="p">,</span> <span class="n">b1</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">b1s</span><span class="p">):</span>
            <span class="n">y_likelihood</span><span class="p">[</span><span class="n">ii</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">lm_likelihood</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x_centered</span><span class="p">,</span> <span class="n">b0</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
    <span class="c1"># Normalize by max to improve viz</span>
    <span class="n">y_likelihood</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">y_likelihood</span><span class="p">)</span> 
    <span class="c1"># Create the plot</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
    <span class="c1"># Plot the surface</span>
    <span class="n">surf</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">B0s</span><span class="p">,</span> <span class="n">B1s</span><span class="p">,</span> <span class="n">y_likelihood</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cm</span><span class="o">.</span><span class="n">viridis</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">antialiased</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
    <span class="c1"># Customize the axes</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\beta_0$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\beta_1$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$P(y|\beta_0, \beta_1, \sigma)$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Likelihood Function with sigma=</span><span class="si">{</span><span class="n">sigma</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="c1"># Adjust viewing angle for better visualization</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">view_init</span><span class="p">(</span><span class="n">elev</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">azim</span><span class="o">=-</span><span class="mi">30</span><span class="p">)</span>
    <span class="c1"># Add a color bar</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">surf</span><span class="p">,</span> <span class="n">shrink</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="c1"># Print the MLE for beta 0 and beta 1:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The maximum likelihood of the data is observed at:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;b0=</span><span class="si">{</span><span class="n">B0s</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">y_likelihood</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">y_likelihood</span><span class="p">))][</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;b1=</span><span class="si">{</span><span class="n">B1s</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">y_likelihood</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">y_likelihood</span><span class="p">))][</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/55c7ed88a7d3e00c61cc78cffabd0fa6a5cc0eebe60f72d28d72e8126baaad4a.png" src="../_images/55c7ed88a7d3e00c61cc78cffabd0fa6a5cc0eebe60f72d28d72e8126baaad4a.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The maximum likelihood of the data is observed at:
b0=199.62
b1=0.20
</pre></div>
</div>
<img alt="../_images/c42e73c90f81fac586d9812d6819ddcb81c93b1350b39dd0fcf52d680b33facc.png" src="../_images/c42e73c90f81fac586d9812d6819ddcb81c93b1350b39dd0fcf52d680b33facc.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The maximum likelihood of the data is observed at:
b0=199.62
b1=0.20
</pre></div>
</div>
<img alt="../_images/be851120db26941fab95348d51338d5eea7ac36a7462a07d103e6cecd8f1d40b.png" src="../_images/be851120db26941fab95348d51338d5eea7ac36a7462a07d103e6cecd8f1d40b.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The maximum likelihood of the data is observed at:
b0=199.62
b1=0.20
</pre></div>
</div>
<img alt="../_images/eedf2f715dbfe30339041824d7676d014b78776ccbd69596d6fc6e71f904bffe.png" src="../_images/eedf2f715dbfe30339041824d7676d014b78776ccbd69596d6fc6e71f904bffe.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The maximum likelihood of the data is observed at:
b0=199.62
b1=0.20
</pre></div>
</div>
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">KeyboardInterrupt</span><span class="g g-Whitespace">                         </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">8</span><span class="p">],</span> <span class="n">line</span> <span class="mi">21</span>
<span class="g g-Whitespace">     </span><span class="mi">19</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">b0</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">b0s</span><span class="p">):</span>
<span class="g g-Whitespace">     </span><span class="mi">20</span>     <span class="k">for</span> <span class="n">ii</span><span class="p">,</span> <span class="n">b1</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">b1s</span><span class="p">):</span>
<span class="ne">---&gt; </span><span class="mi">21</span>         <span class="n">y_likelihood</span><span class="p">[</span><span class="n">ii</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">lm_likelihood</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x_centered</span><span class="p">,</span> <span class="n">b0</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">22</span> <span class="c1"># Normalize by max to improve viz</span>
<span class="g g-Whitespace">     </span><span class="mi">23</span> <span class="n">y_likelihood</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">y_likelihood</span><span class="p">)</span> 

<span class="nn">Cell In[3], line 28,</span> in <span class="ni">lm_likelihood</span><span class="nt">(y, x, b0, b1, sigma)</span>
<span class="g g-Whitespace">     </span><span class="mi">25</span> <span class="n">norm_const</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span><span class="o">**</span><span class="n">n</span>
<span class="g g-Whitespace">     </span><span class="mi">27</span> <span class="c1"># Product of each likelihood:</span>
<span class="ne">---&gt; </span><span class="mi">28</span> <span class="n">likelihoods_prod</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">squared_residuals</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">))))</span>
<span class="g g-Whitespace">     </span><span class="mi">30</span> <span class="k">return</span> <span class="n">norm_const</span> <span class="o">*</span> <span class="n">likelihoods_prod</span>

<span class="ne">KeyboardInterrupt</span>: 
</pre></div>
</div>
</div>
</div>
<p>You can see that when we increase the sigma, the distribution becomes wider, which means that the likelihood for values close to the MLE increases. That makes sense too: if <span class="math notranslate nohighlight">\(\sigma\)</span> was truly close to 0, then we would have almost 0 error and we should be very confident in our estimates. In comparison, if the true value of sigma is larger, then we should have more noisy measurement, and there would be more values of <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> under which the values would be likely. Another way to see what that is obvious is to look at the formulae of the distribution.</p>
<p>Importantly, that doesn’t mean that the likelihood of <span class="math notranslate nohighlight">\(y\)</span> is maxed when <span class="math notranslate nohighlight">\(\sigma\)</span> is 0. We can see that easily by comparing the likelihood observed at the MLE of <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot the data with different values of sigma:</span>
<span class="k">for</span> <span class="n">sigma</span> <span class="ow">in</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]:</span>
    <span class="n">y_likelihood</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">B0s</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">b0</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">b0s</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">ii</span><span class="p">,</span> <span class="n">b1</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">b1s</span><span class="p">):</span>
            <span class="n">y_likelihood</span><span class="p">[</span><span class="n">ii</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">lm_likelihood</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x_centered</span><span class="p">,</span> <span class="n">b0</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span> 
    <span class="c1"># Print the MLE for beta 0 and beta 1:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;P(y|</span><span class="si">{</span><span class="n">B0s</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">y_likelihood</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">y_likelihood</span><span class="p">))][</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">B1s</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">y_likelihood</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">y_likelihood</span><span class="p">))][</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">sigma</span><span class="si">}</span><span class="s2">) = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">y_likelihood</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>P(y|198.75, 0.19, 0.5) = 5.43811210748569e-120
P(y|198.75, 0.19, 1) = 8.843283594454096e-42
P(y|198.75, 0.19, 2) = 5.323901345637373e-29
P(y|198.75, 0.19, 3) = 3.0372233537737223e-30
P(y|198.75, 0.19, 4) = 1.4059154657678168e-32
</pre></div>
</div>
</div>
</div>
<p>The maximal likelihood values are really low overall, but we can see that the likelihood values peak at <span class="math notranslate nohighlight">\(\sigma=2\)</span>. This implies that the data are maximally likely at <span class="math notranslate nohighlight">\(beta_0=198\)</span>, <span class="math notranslate nohighlight">\(beta_0=0.24\)</span> and <span class="math notranslate nohighlight">\(\sigma=2\)</span> or close thereof. So this also adds up: just as the data were most likely for <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> parameter close to our ground truth, the data are also most likely when the <span class="math notranslate nohighlight">\(\sigma\)</span> is close to the ground truth. So that also makes sense.</p>
<p>And just as there was a function to estimate <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(beta_1\)</span> values with which the likelihood of the data are maximized, there is a function to find the value of <span class="math notranslate nohighlight">\(\sigma\)</span> that maximizes the likelihood of the data:</p>
<div class="math notranslate nohighlight">
\[\sigma^2 = \frac{1}{n}\sum_{i=1}^{n}[y_i-(\hat{\Beta}X)]\]</div>
<p>This is the matrix notation, which is equivalent to:</p>
<div class="math notranslate nohighlight">
\[\hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^{n}[y_i-(\sum_{k=1}^{m}\beta_k x_k)]\]</div>
<p>Where <span class="math notranslate nohighlight">\(\hat{\beta}\)</span> are the estimates of the beta parameters obtains with the OLS (which are teh same as the MLE of the <span class="math notranslate nohighlight">\(\beta\)</span> parameters). This formulae looks very familiar, because it is also exactly the same as the one used before to compute the <strong>residual sum of square error (RSS)</strong>. It is just adjusted by dividing by the number of observation. This is also very logical: the <span class="math notranslate nohighlight">\(\sigma\)</span> under which the data are the most likely is basically the observed amount of variation we have around our line of best fit, normalized by the number of observation, as otherwise the most likely value of <span class="math notranslate nohighlight">\(\sigma\)</span> would always be larger when we have more data. This wouldn’t be right, because the same value of <span class="math notranslate nohighlight">\(\sigma\)</span> could be used to generate 10 data points or a 1000.</p>
</section>
<section id="likelihood-function-recap">
<h2>Likelihood function: recap<a class="headerlink" href="#likelihood-function-recap" title="Link to this heading">#</a></h2>
<p>That was a bit long, but now you know everything there is to know about the likelihood function of the linear model. To recap, we saw that for a linear model defined as:</p>
<div class="math notranslate nohighlight">
\[y = \beta_0 + \beta_1x_i + \epsilon\]</div>
<p>Which is equivalent to:</p>
<div class="math notranslate nohighlight">
\[y = \Beta X + \epsilon\]</div>
<p>For which the error is normally distributed and independently and identically distributed (i.i.d.):</p>
<div class="math notranslate nohighlight">
\[\epsilon \sim \mathcal{N}(0, \sigma^2)\]</div>
<p>The maximal likelihood estimates of <span class="math notranslate nohighlight">\(\beta_0\)</span>, <span class="math notranslate nohighlight">\(\beta_1\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span> are:</p>
<div class="math notranslate nohighlight">
\[\hat{\beta_1} = \frac{\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^{n}(x_i-\bar{x})^2}\]</div>
<div class="math notranslate nohighlight">
\[\hat{\beta_0} = \bar{y} - \hat{\beta_1}\bar{x}\]</div>
<div class="math notranslate nohighlight">
\[\hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^{n}[y_i-(\sum_{k=1}^{m}\beta_k x_k)]\]</div>
<p>And finally, the likelihood of our observations given the parameters is defined as:</p>
<div class="math notranslate nohighlight">
\[P(y|\beta_0, \beta_1, \sigma) = (\frac{1}{\sqrt{2\pi\sigma^2}})^n\prod_{i=1}^{n}exp^{-\frac{[y_i-(\sum_{k=0}^{m}{\beta_kx_k})]^2}{2\sigma^2}}\]</div>
<p>Which is equivalent to:</p>
<div class="math notranslate nohighlight">
\[P(y|\beta_0, \beta_1, \sigma) = (\frac{1}{\sqrt{2\pi\sigma^2}})^n\prod_{i=1}^{n}exp^{-\frac{[y_i-X\Beta]^2}{2\sigma^2}}\]</div>
<p>One last thing: the current formula for the OLS assumes that there are only 2 regressors (intercept and  <span class="math notranslate nohighlight">\(x\)</span>), but we can write the formula to be a bit more general. In that case, it would be really cumbersome to write without relying on the matrix format, so we will stick to that:</p>
<div class="math notranslate nohighlight">
\[\hat{\Beta} = (\mathbf{X}^T \mathbf{X})^{-1}\mathbf{X}^Ty\]</div>
<p>Okay, with that we have everything we need for the likelihood. And just one last thing: notice that I have been very precise in the phrasing in this section, the likelihood tells us the likelihood of the data under various values of the parameters of our model.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./LMBayes"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="LMAndBayesTheorem.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Bayes theorem applied to the linear model</p>
      </div>
    </a>
    <a class="right-next"
       href="LMPriors.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">The prior of the linear model</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-likelihood-of-one-observation">The likelihood of one observation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-likelihood-of-several-observation">The likelihood of several observation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-error-term-and-estimation-of-the-sigma-parameter">The error term and estimation of the <span class="math notranslate nohighlight">\(\sigma\)</span> parameter</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#likelihood-function-recap">Likelihood function: recap</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Alex Lepauvre, Jan Gabriel Hartel
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>