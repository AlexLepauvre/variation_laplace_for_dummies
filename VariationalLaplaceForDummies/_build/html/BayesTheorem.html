
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>The Bayes theorem &#8212; Variational Laplace For Dummies</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'BayesTheorem';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Variational Laplace For Dummies - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Variational Laplace For Dummies - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Variational Laplace for Dummies
                </a>
            </li>
        </ul>
        <ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="SomeIntuitions.html">Some intuitions: answering questions when faced with uncertainty</a></li>
<li class="toctree-l1"><a class="reference internal" href="ProbabilityDistribution.html">Probablities and probability distribution</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2FBayesTheorem.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/BayesTheorem.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>The Bayes theorem</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prior-distribution">Prior distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#beta-distribution">Beta distribution</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#combining-the-prior-and-the-likelihood">Combining the prior and the likelihood</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#solving-the-numerator">Solving the numerator</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-little-aside-we-can-compute-the-empirical-probability-of-obtaining-3-10-heads">*A little aside: we can compute the empirical probability of obtaining 3/10 heads+</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="the-bayes-theorem">
<h1>The Bayes theorem<a class="headerlink" href="#the-bayes-theorem" title="Link to this heading">#</a></h1>
<p>In the previous chapter, we defined the concept of probability and probability distributions. We saw that in our experiment where we toss a coin many time, we can use the binomial distribution to obtain the likelihood of experimental outcomes given different probabilities of obtaining tails. However, as we saw in the end, while the likelihood computed with the binomial distribution function is related to how confident we should be that the coin isn’t biased, it is not exactly the same. We need to be able to go from <span class="math notranslate nohighlight">\(P(y|P(X=1))\)</span> to <span class="math notranslate nohighlight">\(P(P(X=1)|y)\)</span>. This is exacty what the Bayes Theorem enables us to do. But before introducing it, we need to revise our notation to be more general purpose.</p>
<p>In our coin toss example, we would like to know the probability of obtaining head, which we wrote as <span class="math notranslate nohighlight">\(P(X=1)\)</span>. However, this notation is very specific to a problem with a binary outcome. In Bayesian inference, we are generally interested in know the value of one or several parameters <span class="math notranslate nohighlight">\(\Theta\)</span>, which happens in our coin toss problem to be <span class="math notranslate nohighlight">\(P(X=1)\)</span>. To that end, we used the binomial distribution to investigate the probability of a number of head out of a total number of throw. These are our empirical data and these are generally written as <span class="math notranslate nohighlight">\(y\)</span>. So when you read <span class="math notranslate nohighlight">\(P(y|\Theta)\)</span>, you can read ‘The probability of our data given the value of our parameter of interest’ and in the specific case of our coin toss example as ‘The probability of getting k times head, given the probability of head’.</p>
<p>The Bayes theorem defined as:
$<span class="math notranslate nohighlight">\(P(\Theta|y) = \frac{P(y|\Theta)*P(\Theta)}{P(y)}\)</span>$</p>
<p>As you can see, it is a way to relate <span class="math notranslate nohighlight">\(P(y|\Theta)\)</span> to <span class="math notranslate nohighlight">\(P(\Theta|y)\)</span>. You will often hear that the Bayes theorem is a mathematical framework to update our beliefs about an unknown parameter based on empirical data. This is exactly what we have been trying to do since the beginning, just phrased in a different way. We want to know if our coin is biased, and for that we run an experiment to try to decide whether it is biased or not. This is the same as saying: I believe that the coin is balanced, and I want to know whether this belief is true based on something I have observed. Not that this is also (almost) the same as saying “I believe that this coin is not balanced, and I want to know whether this belief is correct based on my observations”.</p>
<p>To go from <span class="math notranslate nohighlight">\(P(y|\Theta)\)</span> to <span class="math notranslate nohighlight">\(P(\Theta|y)\)</span>, we need to multiply <span class="math notranslate nohighlight">\(P(y|\Theta)\)</span> with <span class="math notranslate nohighlight">\(P(\Theta)\)</span> and dividing it by <span class="math notranslate nohighlight">\(P(y)\)</span>. <span class="math notranslate nohighlight">\(P(\Theta)\)</span> is the <strong>prior</strong>, and <span class="math notranslate nohighlight">\(P(y)\)</span> is the <strong>marginal likelihood or model evidence</strong>. The <span class="math notranslate nohighlight">\(P(\Theta|y)\)</span> is called the <strong>posterior</strong>,  because it is our updated belief in the true value of <span class="math notranslate nohighlight">\(P(\Theta)\)</span> after seeing the data. We will now explain what these are and then we will see how we can solve the Bayes theorem for our simple problem.</p>
<section id="prior-distribution">
<h2>Prior distribution<a class="headerlink" href="#prior-distribution" title="Link to this heading">#</a></h2>
<p>The prior is the same thing as your belief, or your hypothesis about the true value of the parameter, which you want to test. If your starting hypothesis is that the coin is not biased, then you are basically saying that you believe the most probable value for theta is 0.5. If you are very certain about that, you would say: I believe the probability of <span class="math notranslate nohighlight">\(\Theta=0.5\)</span> is 1. So for this particular coin, you are a 100% sure that it is not biased and that any other values of <span class="math notranslate nohighlight">\(\Theta\)</span> are basically impossible. If you were to express it as a graph, then it would probably look something like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">theta_values</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]</span>  <span class="c1"># Values of theta between 0 and 1</span>
<span class="n">theta_proba</span>  <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]</span>  <span class="c1"># Probability of each value of theta</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">theta_values</span><span class="p">,</span> <span class="n">theta_proba</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">Theta$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;P(X=$</span><span class="se">\\</span><span class="s2">Theta$)&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;I believe that the only possible value for theta is 0.5</span><span class="se">\n</span><span class="s2"> I am certain that my coin is not biased!&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">NameError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">line</span> <span class="mi">4</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span> <span class="n">theta_values</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]</span>  <span class="c1"># Values of theta between 0 and 1</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="n">theta_proba</span>  <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]</span>  <span class="c1"># Probability of each value of theta</span>
<span class="ne">----&gt; </span><span class="mi">4</span> <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="g g-Whitespace">      </span><span class="mi">5</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">theta_values</span><span class="p">,</span> <span class="n">theta_proba</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">6</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">Theta$&quot;</span><span class="p">)</span>

<span class="ne">NameError</span>: name &#39;plt&#39; is not defined
</pre></div>
</div>
</div>
</div>
<p>As we can see, this is kind of representing what we want to say, but this is not perfect. For <span class="math notranslate nohighlight">\(\Theta=0.5\)</span>, we do have a value of 1, but it looks like at <span class="math notranslate nohighlight">\(\Theta=0.45\)</span>, we don’t have zero. That’s to be expected. The simple way we have implemented our belief only specified values for 0.1, 0.2…, but nothing in between, so in the plot above, the dots are connected by taking a straightline between the missing points. We could for sure try to define many more points per hand, but that wouldn’t be very efficient. And it would also never be perfect, except if we were to generate an infinity of points, which we of course can’t do either.</p>
<p>Here again, what we are looking for is a probability distribution. We want to define a function that specifies how likely we believe each value of <span class="math notranslate nohighlight">\(\Theta\)</span> is. We believe that <span class="math notranslate nohighlight">\(\Theta=0.5\)</span> is very likely because we believe that our coin isn’t biased. Note that we could also believe something else, for example that our coin isn’t balanced and that it is more likely to land on head than on tail. Either way, we need to find a function that allow us to specify a probability for any possible values of <span class="math notranslate nohighlight">\(\Theta\)</span>. And here again, we need the probability for each <span class="math notranslate nohighlight">\(\Theta&lt;=1\)</span> and the sum of all probablities to be one, because ultimately, there is only a single true value for <span class="math notranslate nohighlight">\(\Theta\)</span>.</p>
<p>For the sake of the example, we will say that our starting hypothesis is “I believe that the coin isn’t biased, therefore I believe that <span class="math notranslate nohighlight">\(\Theta\)</span> values close to 0.5 are most likely, and values far away from 0.5 are less likely”. We need to find a <strong>probability distribution</strong> to represent our belief about the likelihood of the <span class="math notranslate nohighlight">\(\Theta\)</span> values. There is an infinity of mathematical function that we can use to represent our belief. The question is which one to choose? Well first of all, we need a function that is as simple as possible. There is an infinity of functions we could use, but some of them would require specifying many unintuitive parameters with complex relationships making it difficult to specify what we believe. Ideally a function that takes only a few inputs, with each input corresponding to something that makes intuitive sense would be practical.</p>
<p>You might think, why not use the binomial distribution directly? The binomial distribution won’t work in that case. That’s because unlike the outcome of our experiment, our priors on the true value of theta doesn’t depend on the number of toss we make: the true value of <span class="math notranslate nohighlight">\(\Theta\)</span> is universaly true. There are other reasons as to why the Binomial distribution won’t work in that case, but we won’t go into it here, to keep things simple.</p>
<p>So we need to find a formulae that encodes our belief about the probability of each value of <span class="math notranslate nohighlight">\(\Theta\)</span>, and that function should be simple to work with. There is another thing to consider when selecting a prior: how well does it work with the likelihood we have defined? In the case of our coin toss example, we said that the likelihood is a binomial distribution. For other problems, we will use different likelihood functions. As you see in the Bayes theorem, to obtain the <strong>posterior</strong>, we will need to multiply the <strong>likelihood</strong> with our <strong>prior</strong> and divide the whole thing by the <strong>marginal likelihood</strong>. And as we will see below, depending on which pair of distribution we use as prior and likelihood, doing all of that might be easy, complicated or even impossible. This means that if we can, we should choose a distribution for the <strong>prior</strong> that will make the math down the line easy if we can. Pairs of distributions that work well together are called <strong>conjugate</strong>, and when trying to define a prior, you should first look at whether there is a prior that is the conjugate of the likelihood function you use for your problem. If there is, go for it. As we will see later, very often, there isn’t and it is for such cases that we need to use advanced maths like variational Laplace.</p>
<section id="beta-distribution">
<h3>Beta distribution<a class="headerlink" href="#beta-distribution" title="Link to this heading">#</a></h3>
<p>For the Binomial likelihood, a conjugate does exists, and it is the beta distribution. It is defined like so:</p>
<div class="math notranslate nohighlight">
\[f(\Theta) = \frac{x^{\alpha-1}(1-\Theta)^{\beta-1}}{B(\alpha, \beta)}\]</div>
<p>Where:</p>
<div class="math notranslate nohighlight">
\[B(\alpha, \Beta)=\frac{\Gamma(\alpha)\Gamma(\Beta)}{\Gamma(\alpha+\beta)}\]</div>
<p>Where:</p>
<div class="math notranslate nohighlight">
\[\Gamma(n) = (n-1)!\]</div>
<p>Okay, ouch. So the <span class="math notranslate nohighlight">\(\beta\)</span> distribution is a function which consists of another function, and that other function also contains another function, and we have three different greek letter… That looks intimidating. But in fact, it is really quite alright, you just need to spend time to look at it carefully. And in fact, the reason why we have three functions defined above is just because mathematicians are also frightned by long formulae, so they break them down in bits and pieces that makes it easier to manage for them as well. If that makes you feel better, you can also rewrite the beta distribution in one line:</p>
<div class="math notranslate nohighlight">
\[f(\Theta) = \frac{\Theta^{\alpha-1}(1-\Theta)^{\beta-1}}{\frac{(\alpha-1)!(\beta-1)!}{(\alpha + \beta -1)!}}\]</div>
<p>The way I have written the <span class="math notranslate nohighlight">\(\Gamma\)</span> function above is a bit of a simplification. The formulae I wrote will only work for integer values (1, 2, 3…). But there is a more general form that looks a little bit more complicated that will work for basically any number, but let’s keep the math simple for now.</p>
<p>And we can write the beta distribution as a piece of code:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">gamma</span>

<span class="k">def</span> <span class="nf">beta_distribution</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the probability density of the Beta distribution at a given value theta for parameters alpha and beta.</span>

<span class="sd">    The Beta distribution is defined as:</span>
<span class="sd">        Beta(theta, alpha, beta) = (theta^(alpha - 1) * (1 - theta)^(beta - 1)) / B(alpha, beta)</span>

<span class="sd">    where B(alpha, beta) = (Gamma(alpha) * Gamma(beta)) / Gamma(alpha + beta).</span>
<span class="sd">    </span>
<span class="sd">    Parameters:</span>
<span class="sd">        theta (float): The value at which to evaluate the Beta distribution (0 &lt;= theta &lt;= 1).</span>
<span class="sd">        alpha (float): The shape parameter alpha (&gt; 0).</span>
<span class="sd">        beta (float): The shape parameter beta (&gt; 0).</span>

<span class="sd">    Returns:</span>
<span class="sd">        float: The probability density of the Beta distribution at x.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Ensure x is within the valid range</span>
    <span class="k">if</span> <span class="n">theta</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">theta</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;x must be between 0 and 1.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">alpha</span> <span class="o">&lt;=</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">beta</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;alpha and beta must be positive.&quot;</span><span class="p">)</span>
    
    <span class="c1"># Compute the denominator:</span>
    <span class="n">denom</span> <span class="o">=</span> <span class="p">(</span><span class="n">gamma</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">gamma</span><span class="p">(</span><span class="n">beta</span><span class="p">))</span><span class="o">/</span><span class="p">(</span><span class="n">gamma</span><span class="p">(</span><span class="n">alpha</span> <span class="o">+</span> <span class="n">beta</span><span class="p">))</span>  <span class="c1"># The Beta(alpha, beta) = (Gamma(alpha) * Gamma(beta)) / Gamma(alpha + beta) above. And instead of using the factorial, we are using the gamma function that will work with any numbers</span>

    <span class="c1"># Compute the numerator:</span>
    <span class="n">numer</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">**</span> <span class="p">(</span><span class="n">alpha</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">theta</span><span class="p">)</span><span class="o">**</span><span class="p">(</span><span class="n">beta</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Return the probability of beta at this particular value of x with alpha and beta:</span>
    <span class="k">return</span> <span class="n">numer</span><span class="o">/</span><span class="n">denom</span> 
</pre></div>
</div>
</div>
</div>
<p>That doesn’t seem all that crazy after all now does it? You might still wonder what the alpha and beta parameters are for. Well these are parameters you can adjust to control the shape of the distribution. Let’s try to play around with alpha and beta to get a sense of what they do:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">thetas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>  <span class="c1"># Say this is x=theta, we want to get the P(x) at each values of x, for a given value of alpha and beta:</span>
<span class="n">alphas</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>  <span class="c1"># Try values of alpha from 1 to 5</span>
<span class="n">betas</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>  <span class="c1"># Try values of beta from 1 to 5</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">9</span><span class="p">])</span>

<span class="c1"># Vary alpha:</span>
<span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">alphas</span><span class="p">:</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="p">[</span><span class="n">beta_distribution</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="k">for</span> <span class="n">theta</span> <span class="ow">in</span> <span class="n">thetas</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">alpha$=</span><span class="si">{</span><span class="n">alpha</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">Theta$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;P($</span><span class="se">\\</span><span class="s2">Theta$)&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Beta distributions at $</span><span class="se">\\</span><span class="s2">alpha$=3&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="c1"># Vary beta:</span>
<span class="k">for</span> <span class="n">beta</span> <span class="ow">in</span> <span class="n">betas</span><span class="p">:</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="p">[</span><span class="n">beta_distribution</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span> <span class="k">for</span> <span class="n">theta</span> <span class="ow">in</span> <span class="n">thetas</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">beta$=</span><span class="si">{</span><span class="n">beta</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">Theta$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;P($</span><span class="se">\\</span><span class="s2">Theta$)&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Beta distribution at $</span><span class="se">\\</span><span class="s2">beta$=3&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2b0f266fdf49d0640f1da05301b4b90a64846afbb04bc38b268aec31a506978c.png" src="_images/2b0f266fdf49d0640f1da05301b4b90a64846afbb04bc38b268aec31a506978c.png" />
</div>
</div>
<p>So we can see from the graphs above that when we increase alpha, we somehow move the distribution to the right, and when we increase beta, we move the distribution to the left, and that when alpha=beta, we have a symetrical distribution. In our case, we probably want a prior that is symetrical. If we believe that the coin isn’t biased, we think the most likely value is in the middle and that values on the left or on the right are equally unlikely. But in the case above, when we set alpha and beta to 3, the distribution is quite wide, which would mean that we believe that while we believe <span class="math notranslate nohighlight">\(\Theta=0.5\)</span>, we wouldn’t be crazy surprised to learn that it is as large as 0.8, or as low as 0.2. That doesn’t seem to match our initial assumption that we are very confident that the coin isn’t biased. Let’s try other values:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">thetas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>  <span class="c1"># Say this is x=theta, we want to get the P(x) at each values of x, for a given value of alpha and beta:</span>
<span class="n">alphas</span> <span class="o">=</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">18</span><span class="p">,</span> <span class="mi">22</span><span class="p">]</span>  <span class="c1"># Try values of alpha</span>
<span class="n">betas</span> <span class="o">=</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">18</span><span class="p">,</span> <span class="mi">22</span><span class="p">]</span>  <span class="c1"># Try values of beta</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>

<span class="c1"># Vary alpha:</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">alphas</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="p">[</span><span class="n">beta_distribution</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">betas</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">theta</span> <span class="ow">in</span> <span class="n">thetas</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">alpha$=</span><span class="si">{</span><span class="n">alpha</span><span class="si">}</span><span class="s2"> &amp; $</span><span class="se">\\</span><span class="s2">beta$=</span><span class="si">{</span><span class="n">betas</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">Theta$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;P($</span><span class="se">\\</span><span class="s2">Theta$)&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Beta distributions&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/1cc0aa54fdcdb8bafda877d6769d71b0e257577470fcdb7d1e414e117dfd86e0.png" src="_images/1cc0aa54fdcdb8bafda877d6769d71b0e257577470fcdb7d1e414e117dfd86e0.png" />
</div>
</div>
<p>The larger alpha and beta are, the tighter the distribution seems to be getting. Let’s take the values of <span class="math notranslate nohighlight">\(\alpha=22, \beta=22\)</span>, which means we are initially quite confidence that the true value of <span class="math notranslate nohighlight">\(\Theta=0.5\)</span>.</p>
<p>You might wonder: “Why does it matter how confident I am in my original value? I get the value I get in my experiment, and I will just believe what the experiments tells me”. First of all, we have already seen that the experiment might very well give you something else than the true value, and you shouldn’t accept blindly the results of your experiment as the ultimate truth. How much you should trust the results of your experiment very much depend on how much you trust your initial guess. Let’s take the example of a 52 cards deck. Say you have counted each of the cards and confirmed: I have 4 Queens, 4 kings, 4 jacks… In that case, say you want to run an experiment what the probability is to get a king if you draw a card at random. Your prior should be something like that:</p>
<div class="math notranslate nohighlight">
\[P(\Theta) = 4/52\]</div>
<p>Where <span class="math notranslate nohighlight">\(P(\Theta)\)</span> is the prior probability of drawing a king, which 4/52, because you know you have 4 kings out of 52 cards. Now say you draw cards many many times, and somehow you end up with an observed <span class="math notranslate nohighlight">\(\hat{P}(\Theta)=0.5\)</span>. In that scenario, you of course wouldn’t believe that the results of the experiment, because you know for a fact that <span class="math notranslate nohighlight">\(P(\Theta)=4/52\)</span>. So in that example, you shouldn’t change your mind all that much based on experimental results, because you have very high confidence of what the true <span class="math notranslate nohighlight">\(P(\Theta)\)</span> is. In fact, you have absolute confidence in it: you know for a fact that the probability of <span class="math notranslate nohighlight">\(P(\Theta)=4/52\)</span> is 1 while any other values is 0, which is just a very particular probability distribution, which is very peaky.</p>
<p>This is why, if you want to know the value of a parameter(s) of interest given empirical results (<span class="math notranslate nohighlight">\(P(\Theta|y)\)</span>), you should always factor in your prior, because it is going to influence the conclusion quite a bit.</p>
</section>
</section>
<section id="combining-the-prior-and-the-likelihood">
<h2>Combining the prior and the likelihood<a class="headerlink" href="#combining-the-prior-and-the-likelihood" title="Link to this heading">#</a></h2>
<p>So we now know what the two following components are and what they are for in the quest of answering our question of whether a coin is biased:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(y|\Theta)\)</span>: Likelihood (of the observed values given any value of <span class="math notranslate nohighlight">\(\Theta\)</span>)</p></li>
<li><p><span class="math notranslate nohighlight">\(P(\Theta)\)</span>: Prior, our belief about the likelihood of each <span class="math notranslate nohighlight">\(\Theta\)</span> for our coin</p></li>
</ul>
<p>To compute the numerator of the Bayes theorem, we need to mupltiply the prior with our likelihood. We have the following formulae for the likelihood:</p>
<div class="math notranslate nohighlight">
\[P(y | \Theta) = \binom{n}{y} \Theta^y (1 - \Theta)^{n - y}\]</div>
<p>Where <span class="math notranslate nohighlight">\(n\)</span> and <span class="math notranslate nohighlight">\(y\)</span> are the number of throws and the number of success in our experiment, respetively. Let’s say we fix our number of throw at a thousand.</p>
<p>The prior is defined as a beta distribution like so:</p>
<div class="math notranslate nohighlight">
\[P(\Theta) = \frac{\Theta^{\alpha-1}(1-\Theta)^{\beta-1}}{B(\alpha, \beta)}\]</div>
<p>Where <span class="math notranslate nohighlight">\(alpha\)</span> and <span class="math notranslate nohighlight">\(beta\)</span> depend on our degree of confidence, which we said should be <span class="math notranslate nohighlight">\(\alpha=22, \beta=22\)</span>. When you put the two distributions together, you might notice that they are quite similar. In fact, we can rewrite the prior distribution as follows to make it obvious:</p>
<div class="math notranslate nohighlight">
\[P(\Theta) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}\Theta^{\alpha-1}(1-\Theta)^{\beta-1}\]</div>
<p>Everything after the fraction is kind of the same as the binomial distribution. Or you could say that the two distributions are the same, just multiplied by different things:</p>
<ul class="simple">
<li><p>The binomial distribution is something multiplied by the binomial coefficient <span class="math notranslate nohighlight">\(\binom{n}{k}\)</span></p></li>
<li><p>The beta distribution is a similar something, multiplied by a ratio of beta function (illustrated with <span class="math notranslate nohighlight">\(\Gamma\)</span> above)</p></li>
</ul>
<p>The something that is common to both is something like this:</p>
<div class="math notranslate nohighlight">
\[x^{a}(1-x)^b\]</div>
<p>This something is a beta kernel. In general, when you see an expression like this multiplied by something else, it will take the shape of a beta function, similar to what we have above.</p>
<section id="solving-the-numerator">
<h3>Solving the numerator<a class="headerlink" href="#solving-the-numerator" title="Link to this heading">#</a></h3>
<p>To calculate the posterior, we need to solve the numerator:</p>
<p><span class="math notranslate nohighlight">\(P(y|\Theta)P(\Theta)\)</span></p>
<p>This requires a bit of maths. It’s nothing complicated, just plugging in the formula of each term and rearranging stuff following the rules of mathematics, so that we we end up with a compact and simple formulae. But note that you don’t even have to do any of that. You could very well write a chunky function just based on the multiplication of the two formulae and that would be totally fine. But that way, we can write the function more compactly and elegantly. Feel free to skip, but please don’t skip because you are intimidated by maths, it is really simple, believe in yourself!</p>
<p>We can replace the formulae:</p>
<div class="math notranslate nohighlight">
\[P(y|\Theta)P(\Theta) = [\binom{n}{k}\Theta^k(1-\Theta)^{n-k}][\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}\Theta^{\alpha-1}(1-\Theta)^{\beta-1}]\]</div>
<p>First, we can take out all the bits that don’t involve <span class="math notranslate nohighlight">\(\Theta\)</span>:
$<span class="math notranslate nohighlight">\(C=\binom{n}{k}\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}\)</span>$</p>
<p>So we have:</p>
<div class="math notranslate nohighlight">
\[P(y|\Theta)P(\Theta) = C[\Theta^k(1-\Theta)^{n-k} \times \Theta^{\alpha-1}(1-\Theta)^{\beta-1}]\]</div>
<p>We can now combine the exponents of <span class="math notranslate nohighlight">\(\Theta\)</span> and <span class="math notranslate nohighlight">\((1-\Theta)\)</span></p>
<div class="math notranslate nohighlight">
\[P(y|\Theta)P(\Theta) = C[\Theta^{k+\alpha -1}(1-\Theta)^{n-k+\beta-1}]\]</div>
<p>I didn’t add every single step, but that’s basically combining the exponents in the right way. And you don’t even have to do that, you could just stick to the initial function without simplifying it, but that way it’s a bit easier to work with. We write a pretty simple python code to figure out what the numerator of our Bayes theorem would be for any values of all our parameters (<span class="math notranslate nohighlight">\(\Theta, y, \beta, \alpha, n\)</span>):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">binomial_beta_joint</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the joint probability density of the Beta distribution and Binomial distribution at a given value theta, for a given alpha, beta and y (or whichever way you like really.).</span>

<span class="sd">    The joint beta is defined as</span>
<span class="sd">        $$P(y|\Theta) * P(\Theta) = \frac{k \Theta^{y+\alpha-1}(1-\Theta)^{(n-y)+\beta-1}}{B(\alpha, \beta)}$$</span>
<span class="sd">    </span>
<span class="sd">    Parameters:</span>
<span class="sd">        y (int): Number of success (heads in our case)</span>
<span class="sd">        n (int): Total number of throws</span>
<span class="sd">        theta (float): probability of success</span>
<span class="sd">        alpha (float): parameter for our prior belief</span>
<span class="sd">        beta (float): parameter for our prior belief</span>

<span class="sd">    Returns:</span>
<span class="sd">        float: The probability density of the Beta distribution at x.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Ensure x is within the valid range</span>
    <span class="k">if</span> <span class="n">theta</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">theta</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;x must be between 0 and 1.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">alpha</span> <span class="o">&lt;=</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">beta</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;alpha and beta must be positive.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">y</span> <span class="o">&gt;</span> <span class="n">n</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;y cannot be larger than n, because that would mean more successes than we had attempts&quot;</span><span class="p">)</span>
    
    <span class="c1"># Compute the denominator:</span>
    <span class="n">denom</span> <span class="o">=</span> <span class="p">(</span><span class="n">gamma</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">gamma</span><span class="p">(</span><span class="n">beta</span><span class="p">))</span><span class="o">/</span><span class="p">(</span><span class="n">gamma</span><span class="p">(</span><span class="n">alpha</span> <span class="o">+</span> <span class="n">beta</span><span class="p">))</span>  <span class="c1"># The Beta(alpha, beta) = (Gamma(alpha) * Gamma(beta)) / Gamma(alpha + beta) above. And instead of using the factorial, we are using the gamma function that will work with any numbers</span>

    <span class="c1"># Compute the constant k:</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">comb</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>  <span class="c1"># Calculate n choose k: \binom{n}{y}</span>
    
    <span class="c1"># Compute the numerator:</span>
    <span class="n">numer</span> <span class="o">=</span> <span class="n">k</span> <span class="o">*</span> <span class="n">theta</span><span class="o">**</span><span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">theta</span><span class="p">)</span><span class="o">**</span><span class="p">((</span><span class="n">n</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">+</span><span class="n">beta</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Return the probability of beta at this particular value of x with alpha and beta:</span>
    <span class="k">return</span> <span class="n">numer</span><span class="o">/</span><span class="n">denom</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;&gt;:2: SyntaxWarning: invalid escape sequence &#39;\T&#39;
&lt;&gt;:2: SyntaxWarning: invalid escape sequence &#39;\T&#39;
C:\Users\alexander.lepauvre\AppData\Local\Temp\ipykernel_17156\669880927.py:2: SyntaxWarning: invalid escape sequence &#39;\T&#39;
  &quot;&quot;&quot;
</pre></div>
</div>
</div>
</div>
<p>Nothing all that crazy after all. Now let’s play around with a few values and plot the results:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">theta</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="c1"># Our prior about the most likely value of theta</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mi">80</span>  <span class="c1"># Our prior confidence for the theta value</span>
<span class="n">beta</span> <span class="o">=</span> <span class="mi">80</span>  <span class="c1"># Our prior confidence for the theta value</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">1000</span>  <span class="c1"># Say we run an experiment in which we throw the coin a 1000 times</span>
<span class="n">ys</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span> <span class="c1"># We can plot the joint probability of our likelihood and prior for different observation to see what that looks like:</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">),</span> <span class="p">[</span><span class="n">beta_distribution</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span> <span class="k">for</span> <span class="n">theta</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)],</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">alpha$=</span><span class="si">{</span><span class="n">alpha</span><span class="si">}</span><span class="s2"> &amp; $</span><span class="se">\\</span><span class="s2">beta$=</span><span class="si">{</span><span class="n">beta</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;$P(</span><span class="se">\\</span><span class="s2">Theta)$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">Theta$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Prior distribution&quot;</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="p">[</span><span class="n">binomial_distribution</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">10</span><span class="p">)])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;$P(y|</span><span class="se">\\</span><span class="s2">Theta)$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Likelihood&quot;</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ys</span><span class="p">,</span> <span class="p">[</span><span class="n">binomial_beta_joint</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">ys</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;$P(y|</span><span class="se">\\</span><span class="s2">Theta) * P(</span><span class="se">\\</span><span class="s2">Theta)$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;y (i.e. number of success)&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Joint probability of the likelihood and prior for various values of y&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/ea41af9b530b9cef6e8b03f662f3fe78ea9747601d147b7f356408b93dd00e57.png" src="_images/ea41af9b530b9cef6e8b03f662f3fe78ea9747601d147b7f356408b93dd00e57.png" />
</div>
</div>
<p>It might not be self-evident what we are looking at, but hopefully by playing around a bit more and looking at the graphs will give you an intuition about it all. In the above, the first plot is our prior, the second is the likelihood of the data if <span class="math notranslate nohighlight">\(\Theta=0.5\)</span>, and the lower part is the joint probability of the two. By the look of it, the data look very much like the likelihood. Now let’s try to change our prior. Let’s say we got the coin from the magician shop, so we are less confident about the actual theta:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">theta</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="c1"># Our prior about the most likely value of theta</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mi">20</span>  <span class="c1"># Our prior confidence for the theta value</span>
<span class="n">beta</span> <span class="o">=</span> <span class="mi">20</span>  <span class="c1"># Our prior confidence for the theta value</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">1000</span>  <span class="c1"># Say we run an experiment in which we throw the coin a 1000 times</span>
<span class="n">ys</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span> <span class="c1"># We can plot the joint probability of our likelihood and prior for different observation to see what that looks like:</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">),</span> <span class="p">[</span><span class="n">beta_distribution</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span> <span class="k">for</span> <span class="n">theta</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)],</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">alpha$=</span><span class="si">{</span><span class="n">alpha</span><span class="si">}</span><span class="s2"> &amp; $</span><span class="se">\\</span><span class="s2">beta$=</span><span class="si">{</span><span class="n">beta</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;$P(</span><span class="se">\\</span><span class="s2">Theta)$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">Theta$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Prior distribution&quot;</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ys</span><span class="p">,</span> <span class="p">[</span><span class="n">binomial_distribution</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">10</span><span class="p">)])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;$P(y|</span><span class="se">\\</span><span class="s2">Theta)$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Likelihood&quot;</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ys</span><span class="p">,</span> <span class="p">[</span><span class="n">binomial_beta_joint</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">ys</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;$P(y|</span><span class="se">\\</span><span class="s2">Theta) * P(</span><span class="se">\\</span><span class="s2">Theta)$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;y (i.e. number of success)&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Joint probability of the likelihood and prior for various values of y&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/8fe8a537ff20d713fb235b289c78f0ae6af13437b48295acfb7f8d1631158af9.png" src="_images/8fe8a537ff20d713fb235b289c78f0ae6af13437b48295acfb7f8d1631158af9.png" />
</div>
</div>
<p>Not much happens. The prior doesn’t seem to have much of an impact, the joint distribution kind of always look like the likelihood of the data. Now let’s try something else. In the examples so far, the likelihood was always in line with the prior, meaning that the probability distribution of the data was always such that the most likely value in the data is 0.5. That’s just to be expected, becausewe set  the <span class="math notranslate nohighlight">\(\Theta\)</span> parameter of our prior and likelihood to be the same. Now that doesn’t need to be the case. If we have a coin that is biased towards tail, then the true likelihood should be biased away from 0.5 towards lower values, even if we believe that the coin isn’t biased. This may seem like a really strange thing to say: we always know what the <span class="math notranslate nohighlight">\(\Theta\)</span> value is, because we pass it to the binomial distribution, so of course we know that our prior is correct. It wouldn’t make any sense to choose a prior that is different from what we pass to the binomial distribution function.</p>
<p>But in a real life problem, we don’t know the true <span class="math notranslate nohighlight">\(\Theta\)</span> value, remember, that’s the very reason we are doing all of this in the first place. It is really important to understand that the <span class="math notranslate nohighlight">\(\Theta\)</span> in the various bits of the Bayes theorem don’t need to be the same values. In fact, in the prior, <span class="math notranslate nohighlight">\(\Theta\)</span> refer to all possible value of <span class="math notranslate nohighlight">\(\Theta\)</span>  between 0 and 1 and the belief we have in each, while in the likelihood, the <span class="math notranslate nohighlight">\(\Theta\)</span> is fixed for all possible values of y. The likelihood follows a binomial distribution, but the parameters of that distribution depend on the data. Say if we run an experiment in which we throw the coin a 1000 times, and we repeat that experiment a 100 times: we will get the distribution of <span class="math notranslate nohighlight">\(P(\Theta)\)</span> centered on 0.5 only if the coin isn’t biased. If the coin is biased, then of course the center of the distribution won’t be 0.5. Now let’s see what happens if that were to be the case:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prior_theta</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="c1"># Our prior about the most likely value of theta</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mi">80</span>  <span class="c1"># Our prior confidence for the theta value</span>
<span class="n">beta</span> <span class="o">=</span> <span class="mi">80</span>  <span class="c1"># Our prior confidence for the theta value</span>
<span class="n">true_theta</span> <span class="o">=</span> <span class="mf">0.3</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">200</span>  <span class="c1"># Say we run an experiment in which we throw the coin a 1000 times</span>
<span class="n">ys</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span> <span class="c1"># We can plot the joint probability of our likelihood and prior for different observation to see what that looks like:</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">),</span> <span class="p">[</span><span class="n">beta_distribution</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span> <span class="k">for</span> <span class="n">theta</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)],</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">alpha$=</span><span class="si">{</span><span class="n">alpha</span><span class="si">}</span><span class="s2"> &amp; $</span><span class="se">\\</span><span class="s2">beta$=</span><span class="si">{</span><span class="n">beta</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;$P(</span><span class="se">\\</span><span class="s2">Theta)$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">Theta$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Prior distribution&quot;</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ys</span><span class="p">,</span> <span class="p">[</span><span class="n">binomial_distribution</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">true_theta</span><span class="p">)</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="mi">10</span><span class="p">)])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;$P(y|</span><span class="se">\\</span><span class="s2">Theta)$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Likelihood&quot;</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ys</span><span class="p">,</span> <span class="p">[</span><span class="n">binomial_beta_joint</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">true_theta</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">ys</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;$P(y|</span><span class="se">\\</span><span class="s2">Theta) * P(</span><span class="se">\\</span><span class="s2">Theta)$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;y (i.e. number of success)&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Joint probability of the likelihood and prior for various values of y&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/77e12f480ec35848d9228c77c3159d35b936e0b25cc347bbeb0ace4af94dd043.png" src="_images/77e12f480ec35848d9228c77c3159d35b936e0b25cc347bbeb0ace4af94dd043.png" />
</div>
</div>
</section>
</section>
<section id="a-little-aside-we-can-compute-the-empirical-probability-of-obtaining-3-10-heads">
<h2>*A little aside: we can compute the empirical probability of obtaining 3/10 heads+<a class="headerlink" href="#a-little-aside-we-can-compute-the-empirical-probability-of-obtaining-3-10-heads" title="Link to this heading">#</a></h2>
<p>In fact, we can very easily verify that this is true by running another little simulation (very much the same we did before):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Let&#39;s repeat the 10 coin tosses a 10000 times, just to we get closer to the actual value:</span>
<span class="n">n_iteration</span> <span class="o">=</span> <span class="mi">10000</span>

<span class="c1"># ========================================================</span>
<span class="c1"># Experiment 1:</span>
<span class="n">P</span><span class="p">[</span><span class="s2">&quot;X=3/10 heads&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iteration</span><span class="p">):</span>
    <span class="n">n_throw</span> <span class="o">=</span> <span class="mi">10</span> <span class="c1"># 20 throws instead of 10</span>
    <span class="n">n_head</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># Before we start, we have zero head</span>
    <span class="n">n_tail</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># And zero tails</span>

    <span class="k">for</span> <span class="n">ii</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_throw</span><span class="p">):</span>  <span class="c1"># Repeat the same thing 10 times (throwing the coin)</span>
        <span class="n">rnd</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">()</span>  <span class="c1"># Draw a random number between 0 and 1 (following a uniform distribution, so each value between 0 and 1 is equally likely)</span>
        <span class="k">if</span> <span class="n">rnd</span> <span class="o">&lt;=</span> <span class="mf">0.5</span><span class="p">:</span>  <span class="c1"># If our random number is less than 0.5, we consider that our coin landed on head.</span>
            <span class="n">n_head</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>   <span class="c1"># If our random number is more than 0.5, we consider that our coin landed on tail</span>
            <span class="n">n_tail</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">n_head</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
        <span class="n">P</span><span class="p">[</span><span class="s2">&quot;X=3/10 heads&quot;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;P(X = 3/10 heads)=</span><span class="si">{</span><span class="n">P</span><span class="p">[</span><span class="s2">&quot;X=3/10 heads&quot;</span><span class="p">]</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">n_iteration</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>P(X = 3/10 heads)=0.1149
</pre></div>
</div>
</div>
</div>
<p>So as you can see, when we run the same experiment 10000 times, we get about 12% of the times 3 heads, in line with the Binomial distribution. This may seem familiar with how I described above how we can obtain an empirial probability. With binomial distribution, we get <span class="math notranslate nohighlight">\(P(X=3)\)</span>, and with simulation above, we get <span class="math notranslate nohighlight">\(\hat{P}(X=3)\)</span></p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prior-distribution">Prior distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#beta-distribution">Beta distribution</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#combining-the-prior-and-the-likelihood">Combining the prior and the likelihood</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#solving-the-numerator">Solving the numerator</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-little-aside-we-can-compute-the-empirical-probability-of-obtaining-3-10-heads">*A little aside: we can compute the empirical probability of obtaining 3/10 heads+</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Alex Lepauvre, Jan Gabriel Hartel
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>