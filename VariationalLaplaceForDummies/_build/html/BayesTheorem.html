
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>The Bayes theorem &#8212; Variational Laplace For Dummies</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'BayesTheorem';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Probablities and probability distribution" href="ProbabilityDistribution.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Variational Laplace For Dummies - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Variational Laplace For Dummies - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Variational Laplace for Dummies
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="SomeIntuitions.html">Some intuitions: answering questions when faced with uncertainty</a></li>
<li class="toctree-l1"><a class="reference internal" href="ProbabilityDistribution.html">Probablities and probability distribution</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">The Bayes theorem</a></li>

</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2FBayesTheorem.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/BayesTheorem.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>The Bayes theorem</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">The Bayes theorem</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prior-distribution">Prior distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#beta-distribution">Beta distribution</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#combining-the-prior-and-the-likelihood">Combining the prior and the likelihood</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#solving-the-numerator">Solving the numerator</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#another-beta-distribution">Another beta distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-combined-prior-and-likelihood">The combined prior and likelihood</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#marginal-likelihood-a-k-a-model-evidence">Marginal likelihood, a.k.a. model evidence</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deriving-the-marginal-likelihood-for-our-problem">Deriving the marginal likelihood for our problem</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#python-implementation-of-the-likelihood">Python implementation of the likelihood</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#putting-everything-together-computing-the-posterior">Putting everything together: computing the posterior</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#more-maths-computing-p-0-49-theta-0-51">More maths… Computing <span class="math notranslate nohighlight">\(P(0.49&lt;=\Theta&lt;=0.51)\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#aside-a-simplified-version-of-the-posterior">Aside: a simplified version of the posterior</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-integral-of-the-simplified-posterior">The integral of the simplified posterior:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-integral-of-a-probability-distribution-a-k-a-the-cummulative-distribution-function">The integral of a probability distribution, a.k.a. the cummulative distribution function</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="the-bayes-theorem">
<h1>The Bayes theorem<a class="headerlink" href="#the-bayes-theorem" title="Link to this heading">#</a></h1>
<p>In the previous chapter, we defined the concept of probability and probability distributions. We saw that in our experiment where we toss a coin many time, we can use the binomial distribution to obtain the likelihood of experimental outcomes given different probabilities of obtaining tails. However, as we saw in the end, while the likelihood computed with the binomial distribution function is related to how confident we should be that the coin isn’t biased, it is not exactly the same. We need to be able to go from <span class="math notranslate nohighlight">\(P(y|P(X=1))\)</span> to <span class="math notranslate nohighlight">\(P(P(X=1)|y)\)</span>. This is exacty what the Bayes Theorem enables us to do. But before introducing it, we need to revise our notation to be more general purpose.</p>
<p>In our coin toss example, we would like to know the probability of obtaining head, which we wrote as <span class="math notranslate nohighlight">\(P(X=1)\)</span>. However, this notation is very specific to a problem with a binary outcome. In Bayesian inference, we are generally interested in know the value of one or several parameters <span class="math notranslate nohighlight">\(\Theta\)</span>, which happens in our coin toss problem to be <span class="math notranslate nohighlight">\(P(X=1)\)</span>. To that end, we used the binomial distribution to investigate the probability of a number of head out of a total number of throw. These are our empirical data and these are generally written as <span class="math notranslate nohighlight">\(y\)</span>. So when you read <span class="math notranslate nohighlight">\(P(y|\Theta)\)</span>, you can read ‘The probability of our data given the value of our parameter of interest’ and in the specific case of our coin toss example as ‘The probability of getting k times head, given the probability of head’.</p>
<p>The Bayes theorem defined as:
$<span class="math notranslate nohighlight">\(P(\Theta|y) = \frac{P(y|\Theta)*P(\Theta)}{P(y)}\)</span>$</p>
<p>As you can see, it is a way to relate <span class="math notranslate nohighlight">\(P(y|\Theta)\)</span> to <span class="math notranslate nohighlight">\(P(\Theta|y)\)</span>. You will often hear that the Bayes theorem is a mathematical framework to update our beliefs about an unknown parameter based on empirical data. This is exactly what we have been trying to do since the beginning, just phrased in a different way. We want to know if our coin is biased, and for that we run an experiment to try to decide whether it is biased or not. This is the same as saying: I believe that the coin is balanced, and I want to know whether this belief is true based on something I have observed. Not that this is also (almost) the same as saying “I believe that this coin is not balanced, and I want to know whether this belief is correct based on my observations”.</p>
<p>To go from <span class="math notranslate nohighlight">\(P(y|\Theta)\)</span> to <span class="math notranslate nohighlight">\(P(\Theta|y)\)</span>, we need to multiply <span class="math notranslate nohighlight">\(P(y|\Theta)\)</span> with <span class="math notranslate nohighlight">\(P(\Theta)\)</span> and dividing it by <span class="math notranslate nohighlight">\(P(y)\)</span>. <span class="math notranslate nohighlight">\(P(\Theta)\)</span> is the <strong>prior</strong>, and <span class="math notranslate nohighlight">\(P(y)\)</span> is the <strong>marginal likelihood or model evidence</strong>. The <span class="math notranslate nohighlight">\(P(\Theta|y)\)</span> is called the <strong>posterior</strong>,  because it is our updated belief in the true value of <span class="math notranslate nohighlight">\(P(\Theta)\)</span> after seeing the data. We will now explain what these are and then we will see how we can solve the Bayes theorem for our simple problem.</p>
<section id="prior-distribution">
<h2>Prior distribution<a class="headerlink" href="#prior-distribution" title="Link to this heading">#</a></h2>
<p>The prior is the same thing as your belief, or your hypothesis about the true value of the parameter, which you want to test. If your starting hypothesis is that the coin is not biased, then you are basically saying that you believe the most probable value for theta is 0.5. If you are very certain about that, you would say: I believe the probability of <span class="math notranslate nohighlight">\(\Theta=0.5\)</span> is 1. So for this particular coin, you are a 100% sure that it is not biased and that any other values of <span class="math notranslate nohighlight">\(\Theta\)</span> are basically impossible. If you were to express it as a graph, then it would probably look something like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">theta_values</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]</span>  <span class="c1"># Values of theta between 0 and 1</span>
<span class="n">theta_proba</span>  <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]</span>  <span class="c1"># Probability of each value of theta</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">theta_values</span><span class="p">,</span> <span class="n">theta_proba</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">Theta$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;P(X=$</span><span class="se">\\</span><span class="s2">Theta$)&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;I believe that the only possible value for theta is 0.5</span><span class="se">\n</span><span class="s2"> I am certain that my coin is not biased!&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/d9f4afa7ec10351ee8c2e61bb5db216ab6aed679a9cd69e28300fe1770a2b871.png" src="_images/d9f4afa7ec10351ee8c2e61bb5db216ab6aed679a9cd69e28300fe1770a2b871.png" />
</div>
</div>
<p>As we can see, this is kind of representing what we want to say, but this is not perfect. For <span class="math notranslate nohighlight">\(\Theta=0.5\)</span>, we do have a value of 1, but it looks like at <span class="math notranslate nohighlight">\(\Theta=0.45\)</span>, we don’t have zero. That’s to be expected. The simple way we have implemented our belief only specified values for 0.1, 0.2…, but nothing in between, so in the plot above, the dots are connected by taking a straightline between the missing points. We could for sure try to define many more points per hand, but that wouldn’t be very efficient. And it would also never be perfect, except if we were to generate an infinity of points, which we of course can’t do either.</p>
<p>Here again, what we are looking for is a probability distribution. We want to define a function that specifies how likely we believe each value of <span class="math notranslate nohighlight">\(\Theta\)</span> is. We believe that <span class="math notranslate nohighlight">\(\Theta=0.5\)</span> is very likely because we believe that our coin isn’t biased. Note that we could also believe something else, for example that our coin isn’t balanced and that it is more likely to land on head than on tail. Either way, we need to find a function that allow us to specify a probability for any possible values of <span class="math notranslate nohighlight">\(\Theta\)</span>. And here again, we need the probability for each <span class="math notranslate nohighlight">\(\Theta&lt;=1\)</span> and the sum of all probablities to be one, because ultimately, there is only a single true value for <span class="math notranslate nohighlight">\(\Theta\)</span>.</p>
<p>For the sake of the example, we will say that our starting hypothesis is “I believe that the coin isn’t biased, therefore I believe that <span class="math notranslate nohighlight">\(\Theta\)</span> values close to 0.5 are most likely, and values far away from 0.5 are less likely”. We need to find a <strong>probability distribution</strong> to represent our belief about the likelihood of the <span class="math notranslate nohighlight">\(\Theta\)</span> values. There is an infinity of mathematical function that we can use to represent our belief. The question is which one to choose? Well first of all, we need a function that is as simple as possible. There is an infinity of functions we could use, but some of them would require specifying many unintuitive parameters with complex relationships making it difficult to specify what we believe. Ideally a function that takes only a few inputs, with each input corresponding to something that makes intuitive sense would be practical.</p>
<p>You might think, why not use the binomial distribution directly? The binomial distribution won’t work in that case. That’s because unlike the outcome of our experiment, our priors on the true value of theta doesn’t depend on the number of toss we make: the true value of <span class="math notranslate nohighlight">\(\Theta\)</span> is universaly true. There are other reasons as to why the Binomial distribution won’t work in that case, but we won’t go into it here, to keep things simple.</p>
<p>So we need to find a formulae that encodes our belief about the probability of each value of <span class="math notranslate nohighlight">\(\Theta\)</span>, and that function should be simple to work with. There is another thing to consider when selecting a prior: how well does it work with the likelihood we have defined? In the case of our coin toss example, we said that the likelihood is a binomial distribution. For other problems, we will use different likelihood functions. As you see in the Bayes theorem, to obtain the <strong>posterior</strong>, we will need to multiply the <strong>likelihood</strong> with our <strong>prior</strong> and divide the whole thing by the <strong>marginal likelihood</strong>. And as we will see below, depending on which pair of distribution we use as prior and likelihood, doing all of that might be easy, complicated or even impossible. This means that if we can, we should choose a distribution for the <strong>prior</strong> that will make the math down the line easy if we can. Pairs of distributions that work well together are called <strong>conjugate</strong>, and when trying to define a prior, you should first look at whether there is a prior that is the conjugate of the likelihood function you use for your problem. If there is, go for it. As we will see later, very often, there isn’t and it is for such cases that we need to use advanced maths like variational Laplace.</p>
<section id="beta-distribution">
<h3>Beta distribution<a class="headerlink" href="#beta-distribution" title="Link to this heading">#</a></h3>
<p>For the Binomial likelihood, a conjugate does exists, and it is the beta distribution. It is defined like so:</p>
<div class="math notranslate nohighlight">
\[f(\Theta) = \frac{\Theta^{\alpha-1}(1-\Theta)^{\beta-1}}{B(\alpha, \beta)}\]</div>
<p>Where:</p>
<div class="math notranslate nohighlight">
\[B(\alpha, \Beta)=\frac{\Gamma(\alpha)\Gamma(\Beta)}{\Gamma(\alpha+\beta)}\]</div>
<p>Where:</p>
<div class="math notranslate nohighlight">
\[\Gamma(n) = (n-1)!\]</div>
<p>Okay, ouch. So the <span class="math notranslate nohighlight">\(\beta\)</span> distribution is a function which consists of another function, and that other function also contains another function, and we have three different greek letter… That looks intimidating. But in fact, it is really quite alright, you just need to spend time to look at it carefully. And in fact, the reason why we have three functions defined above is just because mathematicians are also frightned by long formulae, so they break them down in bits and pieces that makes it easier to manage for them as well. If that makes you feel better, you can also rewrite the beta distribution in one line:</p>
<div class="math notranslate nohighlight">
\[f(\Theta) = \frac{\Theta^{\alpha-1}(1-\Theta)^{\beta-1}}{\frac{(\alpha-1)!(\beta-1)!}{(\alpha + \beta -1)!}}\]</div>
<p>The way I have written the <span class="math notranslate nohighlight">\(\Gamma\)</span> function above is a bit of a simplification. The formulae I wrote will only work for integer values (1, 2, 3…). But there is a more general form that looks a little bit more complicated that will work for basically any number, but let’s keep the math simple for now.</p>
<p>And we can write the beta distribution as a piece of code:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">gamma</span>

<span class="k">def</span> <span class="nf">beta_distribution</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the probability density of the Beta distribution at a given value theta for parameters alpha and beta.</span>

<span class="sd">    The Beta distribution is defined as:</span>
<span class="sd">        Beta(theta, alpha, beta) = (theta^(alpha - 1) * (1 - theta)^(beta - 1)) / B(alpha, beta)</span>

<span class="sd">    where B(alpha, beta) = (Gamma(alpha) * Gamma(beta)) / Gamma(alpha + beta).</span>
<span class="sd">    </span>
<span class="sd">    Parameters:</span>
<span class="sd">        theta (float): The value at which to evaluate the Beta distribution (0 &lt;= theta &lt;= 1).</span>
<span class="sd">        alpha (float): The shape parameter alpha (&gt; 0).</span>
<span class="sd">        beta (float): The shape parameter beta (&gt; 0).</span>

<span class="sd">    Returns:</span>
<span class="sd">        float: The probability density of the Beta distribution at x.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Ensure x is within the valid range</span>
    <span class="k">if</span> <span class="n">theta</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">theta</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;x must be between 0 and 1.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">alpha</span> <span class="o">&lt;=</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">beta</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;alpha and beta must be positive.&quot;</span><span class="p">)</span>
    
    <span class="c1"># Compute the denominator:</span>
    <span class="n">denom</span> <span class="o">=</span> <span class="p">(</span><span class="n">gamma</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">gamma</span><span class="p">(</span><span class="n">beta</span><span class="p">))</span><span class="o">/</span><span class="p">(</span><span class="n">gamma</span><span class="p">(</span><span class="n">alpha</span> <span class="o">+</span> <span class="n">beta</span><span class="p">))</span>  <span class="c1"># The Beta(alpha, beta) = (Gamma(alpha) * Gamma(beta)) / Gamma(alpha + beta) above. And instead of using the factorial, we are using the gamma function that will work with any numbers</span>

    <span class="c1"># Compute the numerator:</span>
    <span class="n">numer</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">**</span> <span class="p">(</span><span class="n">alpha</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">theta</span><span class="p">)</span><span class="o">**</span><span class="p">(</span><span class="n">beta</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Return the probability of beta at this particular value of x with alpha and beta:</span>
    <span class="k">return</span> <span class="n">numer</span><span class="o">/</span><span class="n">denom</span> 
</pre></div>
</div>
</div>
</div>
<p>That doesn’t seem all that crazy after all now does it? You might still wonder what the alpha and beta parameters are for. Well these are parameters you can adjust to control the shape of the distribution. Let’s try to play around with alpha and beta to get a sense of what they do:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">thetas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>  <span class="c1"># Say this is x=theta, we want to get the P(x) at each values of x, for a given value of alpha and beta:</span>
<span class="n">alphas</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>  <span class="c1"># Try values of alpha from 1 to 5</span>
<span class="n">betas</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>  <span class="c1"># Try values of beta from 1 to 5</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">9</span><span class="p">])</span>

<span class="c1"># Vary alpha:</span>
<span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">alphas</span><span class="p">:</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="p">[</span><span class="n">beta_distribution</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="k">for</span> <span class="n">theta</span> <span class="ow">in</span> <span class="n">thetas</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">alpha$=</span><span class="si">{</span><span class="n">alpha</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">Theta$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;P($</span><span class="se">\\</span><span class="s2">Theta$)&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Beta distributions at $</span><span class="se">\\</span><span class="s2">alpha$=3&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="c1"># Vary beta:</span>
<span class="k">for</span> <span class="n">beta</span> <span class="ow">in</span> <span class="n">betas</span><span class="p">:</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="p">[</span><span class="n">beta_distribution</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span> <span class="k">for</span> <span class="n">theta</span> <span class="ow">in</span> <span class="n">thetas</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">beta$=</span><span class="si">{</span><span class="n">beta</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">Theta$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;P($</span><span class="se">\\</span><span class="s2">Theta$)&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Beta distribution at $</span><span class="se">\\</span><span class="s2">beta$=3&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2b0f266fdf49d0640f1da05301b4b90a64846afbb04bc38b268aec31a506978c.png" src="_images/2b0f266fdf49d0640f1da05301b4b90a64846afbb04bc38b268aec31a506978c.png" />
</div>
</div>
<p>So we can see from the graphs above that when we increase alpha, we somehow move the distribution to the right, and when we increase beta, we move the distribution to the left, and that when alpha=beta, we have a symetrical distribution. In our case, we probably want a prior that is symetrical. If we believe that the coin isn’t biased, we think the most likely value is in the middle and that values on the left or on the right are equally unlikely. But in the case above, when we set alpha and beta to 3, the distribution is quite wide, which would mean that we believe that while we believe <span class="math notranslate nohighlight">\(\Theta=0.5\)</span>, we wouldn’t be crazy surprised to learn that it is as large as 0.8, or as low as 0.2. That doesn’t seem to match our initial assumption that we are very confident that the coin isn’t biased. Let’s try other values:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">thetas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>  <span class="c1"># Say this is x=theta, we want to get the P(x) at each values of x, for a given value of alpha and beta:</span>
<span class="n">alphas</span> <span class="o">=</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">18</span><span class="p">,</span> <span class="mi">22</span><span class="p">]</span>  <span class="c1"># Try values of alpha</span>
<span class="n">betas</span> <span class="o">=</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">18</span><span class="p">,</span> <span class="mi">22</span><span class="p">]</span>  <span class="c1"># Try values of beta</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>

<span class="c1"># Vary alpha:</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">alphas</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="p">[</span><span class="n">beta_distribution</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">betas</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">theta</span> <span class="ow">in</span> <span class="n">thetas</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">alpha$=</span><span class="si">{</span><span class="n">alpha</span><span class="si">}</span><span class="s2"> &amp; $</span><span class="se">\\</span><span class="s2">beta$=</span><span class="si">{</span><span class="n">betas</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">Theta$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;P($</span><span class="se">\\</span><span class="s2">Theta$)&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Beta distributions&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/1cc0aa54fdcdb8bafda877d6769d71b0e257577470fcdb7d1e414e117dfd86e0.png" src="_images/1cc0aa54fdcdb8bafda877d6769d71b0e257577470fcdb7d1e414e117dfd86e0.png" />
</div>
</div>
<p>The larger alpha and beta are, the tighter the distribution seems to be getting. Let’s take the values of <span class="math notranslate nohighlight">\(\alpha=22, \beta=22\)</span>, which means we are initially quite confidence that the true value of <span class="math notranslate nohighlight">\(\Theta=0.5\)</span>.</p>
<p>You might wonder: “Why does it matter how confident I am in my original value? I get the value I get in my experiment, and I will just believe what the experiments tells me”. First of all, we have already seen that the experiment might very well give you something else than the true value, and you shouldn’t accept blindly the results of your experiment as the ultimate truth. How much you should trust the results of your experiment very much depend on how much you trust your initial guess. Let’s take the example of a 52 cards deck. Say you have counted each of the cards and confirmed: I have 4 Queens, 4 kings, 4 jacks… In that case, say you want to run an experiment what the probability is to get a king if you draw a card at random. Your prior should be something like that:</p>
<div class="math notranslate nohighlight">
\[P(\Theta) = 4/52\]</div>
<p>Where <span class="math notranslate nohighlight">\(P(\Theta)\)</span> is the prior probability of drawing a king, which 4/52, because you know you have 4 kings out of 52 cards. Now say you draw cards many many times, and somehow you end up with an observed <span class="math notranslate nohighlight">\(\hat{P}(\Theta)=0.5\)</span>. In that scenario, you of course wouldn’t believe that the results of the experiment, because you know for a fact that <span class="math notranslate nohighlight">\(P(\Theta)=4/52\)</span>. So in that example, you shouldn’t change your mind all that much based on experimental results, because you have very high confidence of what the true <span class="math notranslate nohighlight">\(P(\Theta)\)</span> is. In fact, you have absolute confidence in it: you know for a fact that the probability of <span class="math notranslate nohighlight">\(P(\Theta)=4/52\)</span> is 1 while any other values is 0, which is just a very particular probability distribution, which is very peaky.</p>
<p>This is why, if you want to know the value of a parameter(s) of interest given empirical results (<span class="math notranslate nohighlight">\(P(\Theta|y)\)</span>), you should always factor in your prior, because it is going to influence the conclusion quite a bit.</p>
</section>
</section>
<section id="combining-the-prior-and-the-likelihood">
<h2>Combining the prior and the likelihood<a class="headerlink" href="#combining-the-prior-and-the-likelihood" title="Link to this heading">#</a></h2>
<p>So we now know what the two following components are and what they are for in the quest of answering our question of whether a coin is biased:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(y|\Theta)\)</span>: Likelihood (of the observed values given any value of <span class="math notranslate nohighlight">\(\Theta\)</span>)</p></li>
<li><p><span class="math notranslate nohighlight">\(P(\Theta)\)</span>: Prior, our belief about the likelihood of each <span class="math notranslate nohighlight">\(\Theta\)</span> for our coin</p></li>
</ul>
<p>To compute the numerator of the Bayes theorem, we need to mupltiply the prior with our likelihood. We have the following formulae for the likelihood:</p>
<div class="math notranslate nohighlight">
\[P(y | \Theta) = \binom{n}{y} \Theta^y (1 - \Theta)^{n - y}\]</div>
<p>Where <span class="math notranslate nohighlight">\(n\)</span> and <span class="math notranslate nohighlight">\(y\)</span> are the number of throws and the number of success in our experiment, respetively. Let’s say we fix our number of throw at a thousand.</p>
<p>The prior is defined as a beta distribution like so:</p>
<div class="math notranslate nohighlight">
\[P(\Theta) = \frac{\Theta^{\alpha-1}(1-\Theta)^{\beta-1}}{B(\alpha, \beta)}\]</div>
<p>Where <span class="math notranslate nohighlight">\(alpha\)</span> and <span class="math notranslate nohighlight">\(beta\)</span> depend on our degree of confidence, which we said should be <span class="math notranslate nohighlight">\(\alpha=22, \beta=22\)</span>. When you put the two distributions together, you might notice that they are quite similar. In fact, we can rewrite the prior distribution as follows to make it obvious:</p>
<div class="math notranslate nohighlight">
\[P(\Theta) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}\Theta^{\alpha-1}(1-\Theta)^{\beta-1}\]</div>
<p>Everything after the fraction is kind of the same as the binomial distribution. Or you could say that the two distributions are the same, just multiplied by different things:</p>
<ul class="simple">
<li><p>The binomial distribution is something multiplied by the binomial coefficient <span class="math notranslate nohighlight">\(\binom{n}{k}\)</span></p></li>
<li><p>The beta distribution is a similar something, multiplied by a ratio of beta function (illustrated with <span class="math notranslate nohighlight">\(\Gamma\)</span> above)</p></li>
</ul>
<p>The something that is common to both is something like this:</p>
<div class="math notranslate nohighlight">
\[x^{a}(1-x)^b\]</div>
<p>This something is a beta kernel. In general, when you see an expression like this multiplied by something else, it will take the shape of a beta function, similar to what we have above.</p>
<section id="solving-the-numerator">
<h3>Solving the numerator<a class="headerlink" href="#solving-the-numerator" title="Link to this heading">#</a></h3>
<p>To calculate the posterior, we need to solve the numerator:</p>
<p><span class="math notranslate nohighlight">\(P(y|\Theta)P(\Theta)\)</span></p>
<p>This requires a bit of maths. It’s nothing complicated, just plugging in the formula of each term and rearranging stuff following the rules of mathematics, so that we we end up with a compact and simple formulae. But note that you don’t even have to do any of that. You could very well write a chunky function just based on the multiplication of the two formulae and that would be totally fine. But that way, we can write the function more compactly and elegantly. Feel free to skip, but please don’t skip because you are intimidated by maths, it is really simple, believe in yourself!</p>
<p>We can replace the formulae:</p>
<div class="math notranslate nohighlight">
\[P(y|\Theta)P(\Theta) = [\binom{n}{k}\Theta^k(1-\Theta)^{n-k}][\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}\Theta^{\alpha-1}(1-\Theta)^{\beta-1}]\]</div>
<p>First, we can take out all the bits that don’t involve <span class="math notranslate nohighlight">\(\Theta\)</span>:
$<span class="math notranslate nohighlight">\(C=\binom{n}{k}\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}\)</span>$</p>
<p>So we have:</p>
<div class="math notranslate nohighlight">
\[P(y|\Theta)P(\Theta) = C[\Theta^k(1-\Theta)^{n-k} \times \Theta^{\alpha-1}(1-\Theta)^{\beta-1}]\]</div>
<p>We can now combine the exponents of <span class="math notranslate nohighlight">\(\Theta\)</span> and <span class="math notranslate nohighlight">\((1-\Theta)\)</span></p>
<div class="math notranslate nohighlight">
\[P(y|\Theta)P(\Theta) = C[\Theta^{k+\alpha -1}(1-\Theta)^{n-k+\beta-1}]\]</div>
</section>
<section id="another-beta-distribution">
<h3>Another beta distribution<a class="headerlink" href="#another-beta-distribution" title="Link to this heading">#</a></h3>
<p>The final formulae once again looks quite familiar. We again have a beta kernel:</p>
<div class="math notranslate nohighlight">
\[P(y|\Theta)P(\Theta) = C\times\Theta^{\alpha' -1}(1-\Theta)^{\beta'-1}\]</div>
<p>Where:</p>
<div class="math notranslate nohighlight">
\[\alpha' = k+\alpha\]</div>
<div class="math notranslate nohighlight">
\[\beta' = n-k+\beta\]</div>
<p>This implies that the numerator is going to be a beta distribution as well. As we will see in a little bit, the denominator (i.e. the marginal likelihood) is a constant with respect to <span class="math notranslate nohighlight">\(\Theta\)</span> as you can already guess from the formulae. This means that the numerator will not have an impact on the kind of distribution of the posterior, simply scale it up and down. Accordingly, the family of distribution the posterior follows is dictated by the numerator. And to complete the story, when we mean that the prior is a conjugate prior, we mean that for a given likelihood function, the posterior is in the same probability distribution family as the prior.</p>
</section>
<section id="the-combined-prior-and-likelihood">
<h3>The combined prior and likelihood<a class="headerlink" href="#the-combined-prior-and-likelihood" title="Link to this heading">#</a></h3>
<p>In the combined formulae of the prior and the likelihood, the quantity C does not depend on <span class="math notranslate nohighlight">\(\Theta\)</span>. This implies that the value of C is the same for any value of $\Theta, and accordingly will not impact the shape of the distribution, it will only scale it up or down. This is why it is not strictly neccessary to compute it.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">comb</span>

<span class="k">def</span> <span class="nf">binomial_distribution</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Calculate the binomial probability P(X = k) for n trials, k successes, and success probability p.</span>
<span class="sd">    P(X = k) = \binom{n}{k} p^k (1 - p)^{n - k}</span>
<span class="sd">    :param n: Total number of trials</span>
<span class="sd">    :param k: Number of successes</span>
<span class="sd">    :param p: Probability of success on a single trial</span>
<span class="sd">    :return: Binomial probability P(X = k)</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="c1"># Calculate the binomial coefficient (n choose k)</span>
    <span class="n">binom_coeff</span> <span class="o">=</span> <span class="n">comb</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>  <span class="c1"># Calculate n choose k: \binom{n}{k}</span>
    
    <span class="c1"># Calculate the binomial probability using the formula</span>
    <span class="n">probability</span> <span class="o">=</span> <span class="n">binom_coeff</span> <span class="o">*</span> <span class="p">(</span><span class="n">p</span> <span class="o">**</span> <span class="n">k</span><span class="p">)</span> <span class="o">*</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="n">k</span><span class="p">))</span>
    
    <span class="k">return</span> <span class="n">probability</span>

<span class="k">def</span> <span class="nf">constant_C</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the constant C in the product of the likelihood and prior.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    n (int): Total number of trials.</span>
<span class="sd">    k (int): Number of successes observed.</span>
<span class="sd">    alpha (float): Alpha parameter of the Beta prior.</span>
<span class="sd">    beta (float): Beta parameter of the Beta prior.</span>

<span class="sd">    Returns:</span>
<span class="sd">    float: The computed constant C.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">binom_coeff</span> <span class="o">=</span> <span class="n">comb</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
    <span class="c1"># Corrected the denominator to be the product, not the sum</span>
    <span class="n">beta_ratio</span> <span class="o">=</span> <span class="n">gamma</span><span class="p">(</span><span class="n">alpha</span> <span class="o">+</span> <span class="n">beta</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">gamma</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">gamma</span><span class="p">(</span><span class="n">beta</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">binom_coeff</span> <span class="o">*</span> <span class="n">beta_ratio</span>


<span class="k">def</span> <span class="nf">likelihood_times_prior</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the unnormalized posterior, which is the product of the likelihood and the prior.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    theta (float): The parameter value at which to evaluate.</span>
<span class="sd">    n (int): Total number of trials.</span>
<span class="sd">    k (int): Number of successes observed.</span>
<span class="sd">    alpha (float): Alpha parameter of the Beta prior.</span>
<span class="sd">    beta (float): Beta parameter of the Beta prior.</span>

<span class="sd">    Returns:</span>
<span class="sd">    float: The unnormalized posterior density at theta.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">C</span> <span class="o">=</span> <span class="n">constant_C</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
    <span class="n">alpha_prime</span> <span class="o">=</span> <span class="n">k</span> <span class="o">+</span> <span class="n">alpha</span>
    <span class="n">beta_prime</span> <span class="o">=</span> <span class="n">n</span> <span class="o">-</span> <span class="n">k</span> <span class="o">+</span> <span class="n">beta</span>
    <span class="k">return</span> <span class="n">C</span> <span class="o">*</span> <span class="n">theta</span><span class="o">**</span><span class="p">(</span><span class="n">alpha_prime</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">theta</span><span class="p">)</span><span class="o">**</span><span class="p">(</span><span class="n">beta_prime</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">likelihood_times_prior_n_c</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the unnormalized posterior, which is the product of the likelihood and the prior.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    theta (float): The parameter value at which to evaluate.</span>
<span class="sd">    n (int): Total number of trials.</span>
<span class="sd">    k (int): Number of successes observed.</span>
<span class="sd">    alpha (float): Alpha parameter of the Beta prior.</span>
<span class="sd">    beta (float): Beta parameter of the Beta prior.</span>

<span class="sd">    Returns:</span>
<span class="sd">    float: The unnormalized posterior density at theta.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">alpha_prime</span> <span class="o">=</span> <span class="n">k</span> <span class="o">+</span> <span class="n">alpha</span>
    <span class="n">beta_prime</span> <span class="o">=</span> <span class="n">n</span> <span class="o">-</span> <span class="n">k</span> <span class="o">+</span> <span class="n">beta</span>
    <span class="k">return</span> <span class="n">theta</span><span class="o">**</span><span class="p">(</span><span class="n">alpha_prime</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">theta</span><span class="p">)</span><span class="o">**</span><span class="p">(</span><span class="n">beta_prime</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now let’s try to calculate the numerator with different values. We will keep our orgininal alpha and beta the same as berore, 22 and 22 each. And let’s say that we throw the coin a 100 times and we get 40 times head:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">alpha</span> <span class="o">=</span> <span class="mi">22</span>  <span class="c1"># Our prior confidence for the theta value</span>
<span class="n">beta</span> <span class="o">=</span> <span class="mi">22</span>  <span class="c1"># Our prior confidence for the theta value</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># Say we run an experiment in which we throw the coin a 1000 times</span>
<span class="n">k</span> <span class="o">=</span> <span class="mi">40</span>  <span class="c1"># Say we get a hypothetical 400 heads:</span>

<span class="n">thetas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="p">[</span><span class="n">beta_distribution</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span> <span class="k">for</span> <span class="n">theta</span> <span class="ow">in</span> <span class="n">thetas</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">alpha$=</span><span class="si">{</span><span class="n">alpha</span><span class="si">}</span><span class="s2"> &amp; $</span><span class="se">\\</span><span class="s2">beta$=</span><span class="si">{</span><span class="n">beta</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;$P(</span><span class="se">\\</span><span class="s2">Theta)$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">Theta$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Prior distribution&quot;</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="p">[</span><span class="n">binomial_distribution</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span> <span class="k">for</span> <span class="n">theta</span> <span class="ow">in</span> <span class="n">thetas</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;$P(y|</span><span class="se">\\</span><span class="s2">Theta)$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">Theta$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Likelihood&quot;</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="p">[</span><span class="n">likelihood_times_prior</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span> <span class="k">for</span> <span class="n">theta</span> <span class="ow">in</span> <span class="n">thetas</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;$P(y|</span><span class="se">\\</span><span class="s2">Theta) * P(</span><span class="se">\\</span><span class="s2">Theta)$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">Theta$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Prior time likelihood&quot;</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="p">[</span><span class="n">likelihood_times_prior_n_c</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span> <span class="k">for</span> <span class="n">theta</span> <span class="ow">in</span> <span class="n">thetas</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;$P(y|</span><span class="se">\\</span><span class="s2">Theta) * P(</span><span class="se">\\</span><span class="s2">Theta)$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">Theta$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Prior time likelihood without C&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/640e837dab2d4d39f19b94be4a5067c09206deb662c1900088c841ef1fd38376.png" src="_images/640e837dab2d4d39f19b94be4a5067c09206deb662c1900088c841ef1fd38376.png" />
</div>
</div>
<p>We can clearly see that the distribution has the exact same shape, with and without the constant. The only difference are the values on the y axis. This brings us to the next part of the equation: with and without the scaling, the sum of the probablities over all values of <span class="math notranslate nohighlight">\(\Theta\)</span> do not add up to 1. That is the role of the denominator (P(y)), which acts as a normalizing constant to make sure that the distribution sums up to 1. We will get to this in a bit. But before, let’s look at the results we have above to note a few interesting things.</p>
<p>We can see that all the multplication between the prior and the likelihood seems to give something in between the two, an average of sorts. This is to be expected. As we discussed before, if you have very strong belief about something (say the probability of picking a king of heart in a 52 cards deck that you know if balanced), you should not take the results of your experiment (here shown as the likelihood) at face value. In the current case, the posterior doesn’t take the same shape as the likelihood because we wouldn’t want to change our mind about whether or not the coin is biased based on the outcome of a single experiment, if we have reasonable reasons to believe that the coin is most probably not biased. It would seem logical to suggest that we should only change our minds if we have a lot of data that doesn’t match our belief, or if we don’t have a strong belief to start with. Turns out that that’s exactly how the multiplication of the prior and likelihood behaves. We can illustrate that by keeping everything the same but using more or less strong priors, or keeping the prior the same but increasing the amount of data we have:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Modulating prior &#39;tightness&#39;:</span>
<span class="n">alphas</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">40</span><span class="p">]</span>  <span class="c1"># Our prior confidence for the theta value</span>
<span class="n">betas</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">40</span><span class="p">]</span>  <span class="c1"># Our prior confidence for the theta value</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># Say we run an experiment in which we throw the coin a 1000 times</span>
<span class="n">k</span> <span class="o">=</span> <span class="mi">40</span>  <span class="c1"># Say we get a hypothetical 400 heads:</span>

<span class="n">thetas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">alphas</span><span class="p">):</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="p">[</span><span class="n">beta_distribution</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">betas</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">theta</span> <span class="ow">in</span> <span class="n">thetas</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">alpha$=</span><span class="si">{</span><span class="n">alpha</span><span class="si">}</span><span class="s2"> &amp; $</span><span class="se">\\</span><span class="s2">beta$=</span><span class="si">{</span><span class="n">betas</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;$P(</span><span class="se">\\</span><span class="s2">Theta)$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">Theta$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Prior distribution&quot;</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="p">[</span><span class="n">binomial_distribution</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span> <span class="k">for</span> <span class="n">theta</span> <span class="ow">in</span> <span class="n">thetas</span><span class="p">],</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Likelihood&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">alphas</span><span class="p">):</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="p">[</span><span class="n">likelihood_times_prior</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">betas</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">theta</span> <span class="ow">in</span> <span class="n">thetas</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;P($</span><span class="se">\\</span><span class="s2">Theta; </span><span class="si">{</span><span class="n">alpha</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">betas</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">$)&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;$P(y|</span><span class="se">\\</span><span class="s2">Theta) * P(</span><span class="se">\\</span><span class="s2">Theta)$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">Theta$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Prior time likelihood with various priors&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/132519adfa5dae3c313e6090111b3cc673dad44dfc5803c081b7cfd3ce389478.png" src="_images/132519adfa5dae3c313e6090111b3cc673dad44dfc5803c081b7cfd3ce389478.png" />
</div>
</div>
<p>We can see quite clearly in the plot above the the least ‘tight’ our prior is, the most the outcome ends up being something close to the likelihood (the black dashed line). In fact, if we set a prior with <span class="math notranslate nohighlight">\(\alpha=1\)</span> and <span class="math notranslate nohighlight">\(\beta=1\)</span>, the results is exactly the same as the likelihood. Again, that’s how you would want it to behave. As you can see in the upper plot, setting <span class="math notranslate nohighlight">\(\alpha=1\)</span> and <span class="math notranslate nohighlight">\(\beta=1\)</span> is basically saying that you believe that any possible values of <span class="math notranslate nohighlight">\(\Theta\)</span> are equally likely a priori, or in other words, you have no idea what the value of <span class="math notranslate nohighlight">\(\Theta\)</span> could be. In Bayesian Linguo, we say that very tight priors are very <strong>informative</strong>, and you will often hear things like <strong>mildly informative priors</strong>, <strong>mildly informative priors</strong>. These all refers to how tight the prior is.</p>
<p>As much as a tight prior pulls the data closer to the prior, the amount of data used to generate the likelihood pulls the outcome towards the data. That also makes sense: you should be more willing to change your mind if you see a lot of data. That’s exactly what our intuition told us initially: we should trust estimates about <span class="math notranslate nohighlight">\(P(X=1)\)</span> more if it comes from experiments where we threw the coin many many times. This is also how the multiplication between the prior and the likelihood behaves:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Modulating prior &#39;tightness&#39;:</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mi">22</span>
<span class="n">beta</span> <span class="o">=</span> <span class="mi">22</span>
<span class="n">n</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">]</span>  <span class="c1"># Say we run an experiment in which we throw the coin a 1000 times</span>
<span class="n">k_per_n_throw</span> <span class="o">=</span> <span class="mf">0.4</span>  <span class="c1"># Say we get 40% head when we throw the coin 10, 20, 100, 200 times</span>

<span class="n">thetas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="p">[</span><span class="n">beta_distribution</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span> <span class="k">for</span> <span class="n">theta</span> <span class="ow">in</span> <span class="n">thetas</span><span class="p">],</span> <span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;$P(</span><span class="se">\\</span><span class="s2">Theta)$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">Theta$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Prior distribution&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">n</span><span class="p">:</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="p">[</span><span class="n">binomial_distribution</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">k_per_n_throw</span><span class="o">*</span><span class="n">i</span><span class="p">),</span> <span class="n">theta</span><span class="p">)</span> <span class="k">for</span> <span class="n">theta</span> <span class="ow">in</span> <span class="n">thetas</span><span class="p">],</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Likelihood (n=</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="p">[</span><span class="n">likelihood_times_prior</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">k_per_n_throw</span><span class="o">*</span><span class="n">i</span><span class="p">),</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span> <span class="k">for</span> <span class="n">theta</span> <span class="ow">in</span> <span class="n">thetas</span><span class="p">],</span> <span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;$P(y|</span><span class="se">\\</span><span class="s2">Theta) * P(</span><span class="se">\\</span><span class="s2">Theta)$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">Theta$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Prior time likelihood with various priors&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/851052ce6fb8a164dbec5336a05b70d0c834060cf157dc0f78664c1bd92eb4f0.png" src="_images/851052ce6fb8a164dbec5336a05b70d0c834060cf157dc0f78664c1bd92eb4f0.png" />
</div>
</div>
</section>
</section>
<section id="marginal-likelihood-a-k-a-model-evidence">
<h2>Marginal likelihood, a.k.a. model evidence<a class="headerlink" href="#marginal-likelihood-a-k-a-model-evidence" title="Link to this heading">#</a></h2>
<p>The numerator controls the overall shape of the distribution. As you can see in the examples above, it seems to behave in the way we would expect our <strong>posterior</strong> to behave. Remember, the posterior is what we are after, as it basically tells us how confident we should be in a given value of <span class="math notranslate nohighlight">\(\Theta\)</span> given our data: if the value of <span class="math notranslate nohighlight">\(\Theta\)</span> is highly likely given the data, then we should conclude that our coin isn’t biased. The posterior, like every other bits of the Bayes theorem is a probability distribution. But you may have noticed that the results in the plot above don’t seem to quite fit the bill. That’s because the numerator alone doesn’t sum up to 1. And we saw before that any probability distribution must satisfy:</p>
<div class="math notranslate nohighlight">
\[\int{P(\Theta|y)}d\Theta = 1\]</div>
<p>This means that when we sum up the probability of any values of <span class="math notranslate nohighlight">\(\Theta\)</span>, we should get 1. You might have expected to see <span class="math notranslate nohighlight">\(\sum\)</span> here instead of <span class="math notranslate nohighlight">\(\int\)</span>. The term <span class="math notranslate nohighlight">\(\int\)</span> is basically an extension of  <span class="math notranslate nohighlight">\(\sum\)</span>, as it also works when <span class="math notranslate nohighlight">\(\Theta\)</span> is continuous (i.e. can take an infinity of values within a range). This is what the denominator in the Bayes theorem does: it enforces that the final product is a distribution that sums up to 1. In other words, we need to normalize the nummerator by dividing it by something so that it sums up to 1. This is why the marginal likelihood/model evidence is referred to as the normalizing constant in some cases.</p>
<p>The marginal likelihood is defined as:</p>
<div class="math notranslate nohighlight">
\[P(y) = \int{P(y|\Theta)P(\Theta)}{d\Theta}\]</div>
<p>In other words, the denominator is the integral of the numerator. It is called the marginal likelihood, because it consists in obtaining the probabity of <span class="math notranslate nohighlight">\(y\)</span> while marginalizing over all possible values of <span class="math notranslate nohighlight">\(\Theta\)</span>, weighted by their prior probability <span class="math notranslate nohighlight">\(P(\Theta)\)</span></p>
<section id="deriving-the-marginal-likelihood-for-our-problem">
<h3>Deriving the marginal likelihood for our problem<a class="headerlink" href="#deriving-the-marginal-likelihood-for-our-problem" title="Link to this heading">#</a></h3>
<p>Once again, eventhough the formulae might look intimidating, it is nothing crazy. We just need to plug in the right formulae in the right place, to see how we can then find the integral of that:</p>
<div class="math notranslate nohighlight">
\[P(y) = \int[\binom{n}{k}\Theta^k(1-\Theta)^{n-k}][\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}\Theta^{\alpha-1}(1-\Theta)^{\beta-1}]\]</div>
<p>Same deal as before, we can take out the constants that aren’t concerned by <span class="math notranslate nohighlight">\(\Theta\)</span>, given that we integrate over <span class="math notranslate nohighlight">\(\Theta\)</span>:</p>
<div class="math notranslate nohighlight">
\[P(y) = \binom{n}{k}\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}\int{\Theta^{k+\alpha-1}(1-\Theta)^{n-k+\beta-1}}d\Theta\]</div>
<p>And just as was the case before, what we end up having to integrate is also a Beta function:</p>
<div class="math notranslate nohighlight">
\[P(y) = \binom{n}{k}\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}\int{\Theta^{\alpha'-1}(1-\Theta)^{\beta'-1}}d\Theta\]</div>
<p>where:</p>
<div class="math notranslate nohighlight">
\[\alpha'=k+\alpha\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\beta'=n-k+\beta\]</div>
<p>So we only need to find the integral of that beta distribution. The math behind it is a bit long and complicated (see <a class="reference external" href="https://en.wikipedia.org/wiki/Beta_function">here</a> if you want the full derivations), but it turns out that the derivatives of any beta function is:</p>
<div class="math notranslate nohighlight">
\[\int{\Theta^{\alpha'-1}(1-\Theta)^{\beta'-1}}d\Theta=\frac{\Gamma(\alpha')\Gamma(\beta')}{\Gamma(\alpha' + \beta')}\]</div>
<p>That probably seems very familiar, doesn’t it? We have indeed seen the right hand side of that formulae many times. Whenever we wrote the <span class="math notranslate nohighlight">\(\beta\)</span> distribution, we saw something like this. Indeed, a <span class="math notranslate nohighlight">\(\beta\)</span> distribution is defined like so (see above as well):</p>
<div class="math notranslate nohighlight">
\[f(\Theta) = \frac{\Theta^{\alpha-1}(1-\Theta)^{\beta-1}}{B(\alpha, \beta)}\]</div>
<p>Where:</p>
<div class="math notranslate nohighlight">
\[B(\alpha, \Beta)=\frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}\]</div>
<p>Or:</p>
<div class="math notranslate nohighlight">
\[\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}\Theta^{\alpha-1}(1-\Theta)^{\beta-1}\]</div>
<p>But actually, that makes perfect sense when you look at the first expression. You have the top part that looks like this:</p>
<div class="math notranslate nohighlight">
\[\Theta^{\alpha-1}(1-\Theta)^{\beta-1}\]</div>
<p>But this bit on its own doesn’t sum up to 1. To get that to sum up to 1, you also need to normalize it by dividing it by the integral of the numerator. So you need to divide <span class="math notranslate nohighlight">\({\Theta^{\alpha-1}(1-\Theta)^{\beta-1}}\)</span> by <span class="math notranslate nohighlight">\(\int{\Theta^{\alpha-1}(1-\Theta)^{\beta-1}}\)</span> to give it a probability distribution. And this is why in the formulae above, we divide <span class="math notranslate nohighlight">\({\Theta^{\alpha-1}(1-\Theta)^{\beta-1}}\)</span> by <span class="math notranslate nohighlight">\(B(\alpha, \beta)\)</span>, because <span class="math notranslate nohighlight">\(B(\alpha, \beta)\)</span> is the integral of the numerator.</p>
<p>So that’s it, we’ve solved the denominator:
$<span class="math notranslate nohighlight">\(P(y) = \binom{n}{k}\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}\times\frac{\Gamma(k+\alpha)\Gamma(n-k+\beta)}{\Gamma(k+\alpha+n-k+\beta)}\)</span>$</p>
</section>
<section id="python-implementation-of-the-likelihood">
<h3>Python implementation of the likelihood<a class="headerlink" href="#python-implementation-of-the-likelihood" title="Link to this heading">#</a></h3>
<p>That was quite a bit of maths. But hopefully by getting a detailed overview, you understand that there is nothing too crazy about it. Sure the trick of know that the integral we were looking for was actually the beta function is something you need to know. Alternatively you could try to come up with the integral for that function, but I will grant you that it takes quite a bit of skills in mathematics (I wouldn’t be able to do it myself). But hopefully you got the sense that it isn’t too crazy complicated either, just high school level maths.</p>
<p>We can now implement the function to compute the marginal likelihood for out specific problem:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">marginal_likelihood</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
    <span class="c1"># Binomial coefficient, n chooses k</span>
    <span class="n">binom_coeff</span> <span class="o">=</span> <span class="n">comb</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
    <span class="c1"># Ratio of gammas with alpha and beta</span>
    <span class="n">gammas_ratio1</span> <span class="o">=</span> <span class="n">gamma</span><span class="p">(</span><span class="n">alpha</span> <span class="o">+</span> <span class="n">beta</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">gamma</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">gamma</span><span class="p">(</span><span class="n">beta</span><span class="p">))</span>
    <span class="c1"># Ratio of gammas with alpha&#39; and beta&#39;</span>
    <span class="n">gammas_ratio2</span> <span class="o">=</span> <span class="p">(</span><span class="n">gamma</span><span class="p">(</span><span class="n">k</span> <span class="o">+</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">gamma</span><span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="n">k</span> <span class="o">+</span> <span class="n">beta</span><span class="p">))</span> <span class="o">/</span> <span class="n">gamma</span><span class="p">(</span><span class="n">n</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">+</span> <span class="n">beta</span><span class="p">)</span>  <span class="c1"># The k gets removed</span>

    <span class="k">return</span> <span class="n">binom_coeff</span> <span class="o">*</span> <span class="n">gammas_ratio1</span> <span class="o">*</span> <span class="n">gammas_ratio2</span>
</pre></div>
</div>
</div>
</div>
<p>Nothing too crazy, is it? Now we have all the building blocks to compute our posterior.</p>
</section>
</section>
<section id="putting-everything-together-computing-the-posterior">
<h2>Putting everything together: computing the posterior<a class="headerlink" href="#putting-everything-together-computing-the-posterior" title="Link to this heading">#</a></h2>
<p>Now we have all the pieces necessary to compute the posterior. To recap, we have the following formulae for the numerator:
$<span class="math notranslate nohighlight">\(P(y|\Theta)P(\Theta) = C\times\Theta^{\alpha' -1}(1-\Theta)^{\beta'-1}\)</span>$</p>
<p>Where:</p>
<div class="math notranslate nohighlight">
\[\alpha' = k+\alpha\]</div>
<div class="math notranslate nohighlight">
\[\beta' = n-k+\beta\]</div>
<div class="math notranslate nohighlight">
\[C=\binom{n}{k}\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}\]</div>
<p>For the denominator, we have this:</p>
<div class="math notranslate nohighlight">
\[P(y) = \binom{n}{k}\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}\times\frac{\Gamma(k+\alpha)\Gamma(n-k+\beta)}{\Gamma(k+\alpha+n-k+\beta)}\]</div>
<p>We can stop it right there with the math. We already have a function to compute the numerator and the denominator, so we can compute each separately and get our posterior (spoiler alert, we will get back to the math a bit later).</p>
<p>So let’s put everything together by creating a function that takes in the parameters and returns the posterior probabilty:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_posterior</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
    <span class="c1"># Compute the numerator:</span>
    <span class="n">num</span> <span class="o">=</span> <span class="n">likelihood_times_prior</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>

    <span class="c1"># Compute the denominator, i.e. the model evidence:</span>
    <span class="n">model_evidence</span> <span class="o">=</span> <span class="n">marginal_likelihood</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">num</span><span class="o">/</span><span class="n">model_evidence</span><span class="p">,</span> <span class="n">model_evidence</span>
</pre></div>
</div>
</div>
</div>
<p>Finally, we can answer our question: is the coin biased? For that, let’s run an experiment again, where we throw the coin a 10 times and compute the posterior by fixing our prior to be a beta distribution with <span class="math notranslate nohighlight">\(\alpha=0.5\)</span> and <span class="math notranslate nohighlight">\(\beta=0.5\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">coin_toss</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Simulate teh results of throwing a coin by taking n times a random number between 0 and 1 and it is smaller than 0.5, say it is head</span>
<span class="sd">    Parameters:</span>
<span class="sd">    n (float): how often we throw the coin</span>
<span class="sd">    Returns:</span>
<span class="sd">    k: total number of heads</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># Before we start, we have zero head</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>  <span class="c1"># Repeat the same thing 10 times (throwing the coin)</span>
        <span class="n">rnd</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">()</span> <span class="o">+</span> <span class="n">bias</span>  <span class="c1"># Draw a random number between 0 and 1 (following a uniform distribution, so each value between 0 and 1 is equally likely)</span>
        <span class="k">if</span> <span class="n">rnd</span> <span class="o">&lt;=</span> <span class="mf">0.5</span><span class="p">:</span>  <span class="c1"># If our random number is less than 0.5, we consider that our coin landed on head.</span>
            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Throw </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: Head&quot;</span><span class="p">)</span>
            <span class="n">k</span> <span class="o">+=</span> <span class="mi">1</span>
    
    <span class="k">return</span> <span class="n">k</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define all our parameters:</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># How often we throw our coin </span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mi">22</span>  <span class="c1"># Our prior about the values of theta</span>
<span class="n">beta</span> <span class="o">=</span> <span class="mi">22</span>  <span class="c1"># Our prior about the values of theta</span>
<span class="n">thetas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>  <span class="c1"># Define a range for theta for plotting purposes</span>

<span class="c1"># Let&#39;s run the experiment:</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">coin_toss</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;We obtained </span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2"> heads out of </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2"> tosses&quot;</span><span class="p">)</span>

<span class="c1"># Now that we have some data, let&#39;s compute the posterior:</span>
<span class="n">posterior</span> <span class="o">=</span> <span class="p">[</span><span class="n">compute_posterior</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">theta</span> <span class="ow">in</span> <span class="n">thetas</span><span class="p">]</span>

<span class="c1"># Let&#39;s plot the results:</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="p">[</span><span class="n">beta_distribution</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span> <span class="k">for</span> <span class="n">theta</span> <span class="ow">in</span> <span class="n">thetas</span><span class="p">],</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Prior&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="p">[</span><span class="n">binomial_distribution</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span> <span class="k">for</span> <span class="n">theta</span> <span class="ow">in</span> <span class="n">thetas</span><span class="p">],</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Likelihood&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="n">posterior</span><span class="p">,</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Posterior&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">Theta$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;P(X)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Posterior distribution&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>We obtained 4 heads out of 10 tosses
</pre></div>
</div>
<img alt="_images/3e294737071ca124eb0f6bc4794bdf9db587e4616d75e0731fa9a5f09eb940a8.png" src="_images/3e294737071ca124eb0f6bc4794bdf9db587e4616d75e0731fa9a5f09eb940a8.png" />
</div>
</div>
<p>One thing you probably notice off the bat is that the probability returned by the likelihood function are smaller than that of the prior and posterior. And even weirder, both for the prior and the posterior, there are values that are higher than 1, eventhough we said that a probability distribution must sum up to 1, how can that be? Well for sake of simplicity I omitted a few things. There is a fundamental difference between the distribution we use for the likelihood and the distributions of the prior and posterior. The likelihood is a discrete probability distribution. When we throw the coin n times, there is a finite number of outcomes k: 0 to n. And for each of these outcome, we have a probability. If we take the sum of all these probabilities, we must get one:</p>
<div class="math notranslate nohighlight">
\[\sum\limits_{k=0}^nP(X=k) = 1\]</div>
<p>Accordingly, with such a distribution, the more throw we make, the smaller the value of each outcome ends up being. The binomial distribution is what we call a <strong>probability mass function (PMF)</strong> and each value k has a positive probability. In comparison, both the prior and posterior distributions are Beta distribution which ascribe to each possible values of <span class="math notranslate nohighlight">\(\Theta\)</span> a probability. <span class="math notranslate nohighlight">\(\Theta\)</span> is continuous, it can take an infinity of values between 0 and 1. Because <span class="math notranslate nohighlight">\(\Theta\)</span> can take an infinity of values between 0 and 1, the probability of any one value of <span class="math notranslate nohighlight">\(\Theta\)</span> is 0. Indeed, it is impossible to get exactly 0.5. When dealing with probability distribution of continuous variables, we talk about <strong>probability density function</strong>, because such functions actually inform us about the probability of the value of <span class="math notranslate nohighlight">\(\Theta\)</span> to be between two points. If this is unclear to you, I strongly recommend you check out this <a class="reference external" href="https://www.youtube.com/watch?v=hDjcxi9p0ak">youtube tutorial</a>, which should give you a good intuition about it!</p>
<p>I purposefully displayed the graph above to illustrate that point, but to make things a bit easier to visualize, we can set the likelihood on a different axis:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define all our parameters:</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># How often we throw our coin </span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mi">22</span>  <span class="c1"># Our prior about the values of theta</span>
<span class="n">beta</span> <span class="o">=</span> <span class="mi">22</span>  <span class="c1"># Our prior about the values of theta</span>
<span class="n">thetas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>  <span class="c1"># Define a range for theta for plotting purposes</span>

<span class="c1"># Let&#39;s run the experiment 4 times in a row:</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">coin_toss</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;We obtained </span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2"> heads out of </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2"> tosses&quot;</span><span class="p">)</span>

    <span class="c1"># Now that we have some data, let&#39;s compute the posterior:</span>
    <span class="n">posterior</span> <span class="o">=</span> <span class="p">[</span><span class="n">compute_posterior</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">theta</span> <span class="ow">in</span> <span class="n">thetas</span><span class="p">]</span>

    <span class="c1"># Plot the likelihood:</span>
    <span class="n">ax2</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">twinx</span><span class="p">()</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="p">[</span><span class="n">binomial_distribution</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span> <span class="k">for</span> <span class="n">theta</span> <span class="ow">in</span> <span class="n">thetas</span><span class="p">],</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Likelihood&#39;</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;P(y|$</span><span class="se">\\</span><span class="s1">Theta$)&#39;</span><span class="p">)</span>
    <span class="c1"># Plot the prior</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="p">[</span><span class="n">beta_distribution</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span> <span class="k">for</span> <span class="n">theta</span> <span class="ow">in</span> <span class="n">thetas</span><span class="p">],</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Prior&#39;</span><span class="p">)</span>
    <span class="c1"># Plot the posterior:</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="n">posterior</span><span class="p">,</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Posterior&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">Theta$&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;P($</span><span class="se">\\</span><span class="s1">Theta$) | P($</span><span class="se">\\</span><span class="s1">Theta$|y)&#39;</span><span class="p">)</span>
    
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Experiment </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="n">lines</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">get_legend_handles_labels</span><span class="p">()</span>
<span class="n">lines2</span><span class="p">,</span> <span class="n">labels2</span> <span class="o">=</span> <span class="n">ax2</span><span class="o">.</span><span class="n">get_legend_handles_labels</span><span class="p">()</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">lines</span> <span class="o">+</span> <span class="n">lines2</span><span class="p">,</span> <span class="n">labels</span> <span class="o">+</span> <span class="n">labels2</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>We obtained 4 heads out of 10 tosses
We obtained 4 heads out of 10 tosses
We obtained 3 heads out of 10 tosses
We obtained 7 heads out of 10 tosses
</pre></div>
</div>
<img alt="_images/c877ae73334c15c7987b45e78548231e283c633fe4bb0249e81a658949671006.png" src="_images/c877ae73334c15c7987b45e78548231e283c633fe4bb0249e81a658949671006.png" />
</div>
</div>
<p>That way it’s a bit easier to read. In the figure above, we run the same experiment three times in a row to show that depending on the outcome of the experiment, while our prior remains the same, the likelihood may change depending on the randomness involved in our experiment, which results in different posterior. That all make sense: if you get in one experiment 3/10 heads, then you have more reasons to believe that your coin is biased compared to getting 5/10 heads, make sense.</p>
<p>So now we have our posterior, which tells us how confident we can be that our coin isn’t biased. But wait a minute, how exactly are you supposed to answer the question based on the posterior? It would seem from the plots above that the closer your observations were from a ratio of 5/10, the more centered your posterior is on 0.5, so it would seem that you should be more confident in those results. However, because <span class="math notranslate nohighlight">\(\Theta\)</span> is a continuous variable which can take any values between 0 and 1, we know that the probability of your coin having a probability of head of exactly 0.5 is exactly 0. This sounds like you should actually be certain that your coin is biased? This is probably the biggest let down of everything you have read so far: as counter-intuitive as this may sound, your coin is most certainly biased. Furthermore, the posterior doesn’t really enables you to know how confident you should be that the probability of getting head is exactly 0.5, but that’s because it is the wrong question to ask. The only thing that makes sense to try to establish is how confident can you be that the probability of obtaining head is within a range that you believe is acceptable to pursue with your bet?</p>
<section id="more-maths-computing-p-0-49-theta-0-51">
<h3>More maths… Computing <span class="math notranslate nohighlight">\(P(0.49&lt;=\Theta&lt;=0.51)\)</span><a class="headerlink" href="#more-maths-computing-p-0-49-theta-0-51" title="Link to this heading">#</a></h3>
<p>Say you are 99% confident based on your data that the true value of <span class="math notranslate nohighlight">\(\Theta\)</span> lies between 0.45 and 0.55, this is probably okay for you to pursue with your bet. Of course that would mean that someone is at a slight advantage. But that advantage is so small that it won’t matter if you conduct your bet only once. With your posterior distribution, you can do just that, you can compute the probability of <span class="math notranslate nohighlight">\(\Theta\)</span> to be between say 0.45 and 0.55, or between any values you’d like. Conceptually, it’s quite easy to figure out how you’d do that. You can first figure out the following probability:</p>
<div class="math notranslate nohighlight">
\[P(0.49&lt;=\Theta)\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Let&#39;s toss the coin:</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">coin_toss</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;We obtained </span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2"> heads out of </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2"> tosses&quot;</span><span class="p">)</span>
<span class="c1"># Now that we have some data, let&#39;s compute the posterior:</span>
<span class="n">posterior</span> <span class="o">=</span> <span class="p">[</span><span class="n">compute_posterior</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">theta</span> <span class="ow">in</span> <span class="n">thetas</span><span class="p">]</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="c1"># Plot the posterior:</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="n">posterior</span><span class="p">,</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Posterior&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.45</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">),</span> 
        <span class="n">y1</span><span class="o">=</span><span class="p">[</span><span class="n">compute_posterior</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">theta</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.45</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)],</span>
        <span class="n">color</span><span class="o">=</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span>
        <span class="n">alpha</span><span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="s1">&#39;P($</span><span class="se">\\</span><span class="s1">Theta$&lt;0.49)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">Theta$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;P($</span><span class="se">\\</span><span class="s1">Theta$|y)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Posterior distribution&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>We obtained 8 heads out of 10 tosses
</pre></div>
</div>
<img alt="_images/ea9cf3266e0233172812b24b7ff2793bd16627137574f6eb0115a930b91a0835.png" src="_images/ea9cf3266e0233172812b24b7ff2793bd16627137574f6eb0115a930b91a0835.png" />
</div>
</div>
<p>Now let’s do the same but for <span class="math notranslate nohighlight">\(P(\Theta&lt;=0.51)\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="c1"># Plot the posterior:</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="n">posterior</span><span class="p">,</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Posterior&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.55</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">),</span> 
        <span class="n">y1</span><span class="o">=</span><span class="p">[</span><span class="n">compute_posterior</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">theta</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.55</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)],</span>
        <span class="n">color</span><span class="o">=</span> <span class="s2">&quot;r&quot;</span><span class="p">,</span>
        <span class="n">alpha</span><span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="s1">&#39;P($</span><span class="se">\\</span><span class="s1">Theta$&lt;0.49)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">Theta$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;P($</span><span class="se">\\</span><span class="s1">Theta$|y)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Posterior distribution&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/dee26b936ff9cbb2790f0a748ee85b8c5bfea49e6e62333857575a02fe38c963.png" src="_images/dee26b936ff9cbb2790f0a748ee85b8c5bfea49e6e62333857575a02fe38c963.png" />
</div>
</div>
<p>If we take the difference between these two, we get:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="c1"># Plot the posterior:</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="n">posterior</span><span class="p">,</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Posterior&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.45</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">),</span> 
        <span class="n">y1</span><span class="o">=</span><span class="p">[</span><span class="n">compute_posterior</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">theta</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.45</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)],</span>
        <span class="n">color</span><span class="o">=</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span>
        <span class="n">alpha</span><span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="s1">&#39;P($</span><span class="se">\\</span><span class="s1">Theta$&lt;0.49)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.55</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">),</span> 
        <span class="n">y1</span><span class="o">=</span><span class="p">[</span><span class="n">compute_posterior</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">theta</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.55</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)],</span>
        <span class="n">color</span><span class="o">=</span> <span class="s2">&quot;r&quot;</span><span class="p">,</span>
        <span class="n">alpha</span><span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="s1">&#39;P($</span><span class="se">\\</span><span class="s1">Theta$&lt;0.49)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">Theta$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;P($</span><span class="se">\\</span><span class="s1">Theta$|y)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Posterior distribution&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/cbe4190942890202f6cd430f6ba2e59563cf6ad107fdda4e686f950d056b02d1.png" src="_images/cbe4190942890202f6cd430f6ba2e59563cf6ad107fdda4e686f950d056b02d1.png" />
</div>
</div>
<p>That’s right, when we take the difference between <span class="math notranslate nohighlight">\(P(\Theta&lt;= 0.55) - P(\Theta&lt;= 0.45)\)</span> we get the probability of <span class="math notranslate nohighlight">\(P(0.45&lt;=\Theta&lt;= 0.55)\)</span>, as we remove everything that is common to both (the area overlapping between the two on the graph above). So this means that to obtain the probability of our theta value being within a specific range, we need to integrate the posterior again, once for each of the bounds of our range:</p>
<div class="math notranslate nohighlight">
\[P(0.45&lt;=\Theta&lt;= 0.55) = \int_{\Theta=0}^{0.55}{P(\Theta|y)d\Theta} - \int_{\Theta=0}^{0.45}{P(\Theta|y)d\Theta}\]</div>
<p>Yes, sorry, we aren’t yet done with our integrals and complicated maths (though to be fair, I did warn you above). We started from:</p>
<div class="math notranslate nohighlight">
\[P(\Theta|y) = \frac{P(y|\Theta)P(\Theta)}{P(y)}\]</div>
<p>And we figured out that we can shuffle things around to simplify the operations required to get the numerator and the denominator. And we obtained something like this:</p>
<div class="math notranslate nohighlight">
\[P(\Theta|y) = \frac{\binom{n}{k}\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}\times\Theta^{\alpha' -1}(1-\Theta)^{\beta'-1}}{\binom{n}{k}\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}\times\frac{\Gamma(k+\alpha)\Gamma(n-k+\beta)}{\Gamma(k+\alpha+n-k+\beta)}}\]</div>
<p>Where:</p>
<div class="math notranslate nohighlight">
\[\alpha' = k+\alpha\]</div>
<div class="math notranslate nohighlight">
\[\beta' = n-k+\beta\]</div>
<p>And we need to figure out the integral of that somehow.</p>
<p>Yeah okay, this is one beefy formulae, and it is normal to feel very intimidated by it. How on earth can we figure out what the integral of this monstrosity should be? Once again, we will use a bunch of relatively simple tricks to make this thing more palatable. And if we are lucky, once we have done so, we will realize that maybe finding the integral isn’t as bad as it sounds.</p>
<p>Okay so first things first, in the denominator, it turns out that everything we have between parantheses is also <span class="math notranslate nohighlight">\(\alpha'\)</span> and <span class="math notranslate nohighlight">\(\beta'\)</span>:</p>
<div class="math notranslate nohighlight">
\[P(\Theta|y) = \frac{\binom{n}{k}\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}\times\Theta^{\alpha' -1}(1-\Theta)^{\beta'-1}}{\binom{n}{k}\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}\times\frac{\Gamma(\alpha')\Gamma(\beta')}{\Gamma(\alpha'+\beta')}}\]</div>
<p>Now that looks a little bit better. Now you probably noticed something else: we have both in the numerator and the denominator this C constant <span class="math notranslate nohighlight">\(\binom{n}{k}\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}\)</span>. Even if it’s within an integral, that also cancels out:</p>
<div class="math notranslate nohighlight">
\[P(\Theta|y) = \frac{\Theta^{\alpha' -1}(1-\Theta)^{\beta'-1}}{\frac{\Gamma(\alpha')\Gamma(\beta')}{\Gamma(\alpha'+\beta')}}\]</div>
<p>Now wait a minute, doesn’t that looks familiar? Yes, we have seen it before. Remember when we tried to define what function we should use for our prior, we said that the beta distribution is a good choice, because it is the conjugate of the Binomial distribution. And we defined the Beta distribution like so:</p>
<div class="math notranslate nohighlight">
\[f(\Theta) = \frac{\Theta^{\alpha-1}(1-\Theta)^{\beta-1}}{B(\alpha, \beta)}\]</div>
<p>Where:</p>
<div class="math notranslate nohighlight">
\[B(\alpha, \Beta)=\frac{\Gamma(\alpha)\Gamma(\Beta)}{\Gamma(\alpha+\beta)}\]</div>
<p>So in other words, the posterior distribution is also a beta distribution. This should be too surprising, as we said that a conjugate prior is a prior that yields a posterior of the same distribution family. And so here is the proof of that: we have the C part that simplifies both on the numerator and denominator, which leaves us with a beta function on the top and the integral thereof at the denominator, making the beta function a beta distribution.</p>
</section>
<section id="aside-a-simplified-version-of-the-posterior">
<h3>Aside: a simplified version of the posterior<a class="headerlink" href="#aside-a-simplified-version-of-the-posterior" title="Link to this heading">#</a></h3>
<p>This is a bit of an aside, but this highlights why it is always useful to do the maths to simplify expressions as much as possible. Previously, we stopped simplifying the operation at the point where we felt that we could compute the numerator and the denominator easily enough, and then just divide one by the other. And that worked nicely. But with what we just saw, it should be obvious that there is a much simpler way to compute the posterior: just calculate the new values <span class="math notranslate nohighlight">\(\alpha'\)</span> and <span class="math notranslate nohighlight">\(\beta'\)</span>, which are simple addition and subtractions based on our parameters, and that’s enough to get the posterior distribution. There is no magic nor super duper complicated maths, just simple raranging of the terms and figuring out what we can simplify. We can accordingly write a new function that computes the posterior like so:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_posterior</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the posterior distribution of a parameter given a binomial likelihood and a Beta prior.</span>
<span class="sd">    </span>
<span class="sd">    Parameters:</span>
<span class="sd">    theta (float): The value at which to evaluate the posterior distribution.</span>
<span class="sd">    n (int): Total number of trials (e.g., flips in a coin-toss experiment).</span>
<span class="sd">    k (int): Number of observed successes (e.g., heads in coin tosses).</span>
<span class="sd">    alpha (float): Alpha parameter of the Beta prior distribution.</span>
<span class="sd">    beta (float): Beta parameter of the Beta prior distribution.</span>

<span class="sd">    Returns:</span>
<span class="sd">    float: The probability density of the posterior distribution evaluated at the given theta.</span>
<span class="sd">    </span>
<span class="sd">    Notes:</span>
<span class="sd">    - The function calculates the posterior as a Beta distribution with updated parameters:</span>
<span class="sd">        - alpha&#39; = k + alpha</span>
<span class="sd">        - beta&#39; = n - k + beta</span>
<span class="sd">    - The posterior distribution follows a Beta(alpha&#39;, beta&#39;) distribution.</span>
<span class="sd">    - This function assumes `beta_distribution` is defined elsewhere and represents the Beta distribution PDF.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">alpha_prime</span> <span class="o">=</span> <span class="n">k</span> <span class="o">+</span> <span class="n">alpha</span>
    <span class="n">beta_prime</span> <span class="o">=</span> <span class="n">n</span><span class="o">-</span><span class="n">k</span><span class="o">+</span><span class="n">beta</span>
    <span class="k">return</span> <span class="n">beta_distribution</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">alpha_prime</span><span class="p">,</span> <span class="n">beta_prime</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="the-integral-of-the-simplified-posterior">
<h3>The integral of the simplified posterior:<a class="headerlink" href="#the-integral-of-the-simplified-posterior" title="Link to this heading">#</a></h3>
<p>Okay, back to the integral, so now we have this:</p>
<div class="math notranslate nohighlight">
\[P(\Theta|y) = \frac{\Theta^{\alpha' -1}(1-\Theta)^{\beta'-1}}{\frac{\Gamma(\alpha')\Gamma(\beta')}{\Gamma(\alpha'+\beta')}}\]</div>
<p>Which is equivalent to this:</p>
<div class="math notranslate nohighlight">
\[P(\Theta|y) = {\frac{\Gamma(\alpha'+\beta')}{\Gamma(\alpha')\Gamma(\beta')}}\times\Theta^{\alpha' -1}(1-\Theta)^{\beta'-1}\]</div>
<p>Okay, now we are cooking. We need to solve this:</p>
<div class="math notranslate nohighlight">
\[\int_{\Theta=0}^{x}{P(\Theta|y)} = \int_{\Theta=0}^{x}{{\frac{\Gamma(\alpha'+\beta')}{\Gamma(\alpha')\Gamma(\beta')}}\times\Theta^{\alpha' -1}(1-\Theta)^{\beta'-1}}d\Theta\]</div>
<p>Same trick as before, we can take out the parts that don’t contain <span class="math notranslate nohighlight">\(\Theta\)</span>:
$<span class="math notranslate nohighlight">\(\int_{\Theta=0}^{x}{P(\Theta|y)} = \frac{\Gamma(\alpha'+\beta')}{\Gamma(\alpha')\Gamma(\beta')}\times\int_{\Theta=0}^{x}{\Theta^{\alpha' -1}(1-\Theta)^{\beta'-1}}d\Theta\)</span>$</p>
<p>Once again, some bits of this equation may seem familiar. As we were trying to figure out the marginal likelihood, we also had to figure out a similar integral. And we got out of it by realizing that the beta function is defined like so (we didn’t exactly figure it out ourselves, but we referred to the <a class="reference external" href="https://en.wikipedia.org/wiki/Beta_function">wikipedia</a> page with the proof):</p>
<div class="math notranslate nohighlight">
\[\int_{0\Theta=0}^{x}{\Theta^{\alpha' -1}(1-\Theta)^{\beta'-1}}d\Theta = \frac{\Gamma(\alpha')\Gamma(\beta')}{\Gamma(\alpha'+\beta')}\]</div>
<p>You might be tempted to write:</p>
<div class="math notranslate nohighlight">
\[\int_{\Theta=0}^{x}{P(\Theta|y)} = \frac{\Gamma(\alpha'+\beta')}{\Gamma(\alpha')\Gamma(\beta')}\times\frac{\Gamma(\alpha')\Gamma(\beta')}{\Gamma(\alpha'+\beta')}\]</div>
<p>In which case everything sums up to 1. That’s however <strong>wrong</strong> (or at east not generally true, only in a very specific case). The equality of the beta function (i.e. the integral of the thing with exponent is equal to the gamma function times itself divided by the gamma function) only holds when we integrate from 0 to 1. And when you think about it, it makes sense: if you integrate from 0 to 1, then the integral must be 1 because it is a probability distribution, so you would expect everything to cancel out that way. But since we are interested in finding  the probability of <span class="math notranslate nohighlight">\(\Theta\)</span> to be under a specific probability, we need to find a solution to the equation such that it works to integrate up to any values of i.</p>
<p>This is unfortunately where things start to be a little confusing, and we will need to once again adjust our notation. We will write the beta function as so:</p>
<div class="math notranslate nohighlight">
\[\Beta(\alpha, \beta) = \int_{\Theta=0}^{x}\Theta^{\alpha -1}(1-\Theta)^{\beta-1} = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}\]</div>
<p>So we can rewrite our formulae above like so:</p>
<div class="math notranslate nohighlight">
\[\int_{\Theta=0}^{x}{P(\Theta|y)} = \frac{1}{\Beta(\alpha', \beta')}\times\int_{\Theta=0}^{x}{\Theta^{\alpha' -1}(1-\Theta)^{\beta'-1}}d\Theta\]</div>
<p>The integral on the right handside is known as the incomplete beta function up to i, which is written as:</p>
<div class="math notranslate nohighlight">
\[\int_{\Theta=0}^{x}{\Theta^{\alpha' -1}(1-\Theta)^{\beta'-1}}d\Theta = \Beta(x; \alpha, \beta)\]</div>
<p>Unfortunately, <strong>there is no closed-form expression</strong> for the incomplete beta function. What that means is that there is no way to simplify the expression to something that can easily be computed as we have done so far. The only thing we can do is either brut-force: take the sum of the values obtained from 0 to i in tiny tiny steps and hope that that’s sufficient to get close to the true value. Or we can use other mathematical tricks to find another function that is easy to work with and that should approximate the value the function of interest would take in the range of interest. Either way, let’s not worry about it too much for now. Based on the two functions we have defined, we can rewrite the integral we are interested in as:</p>
<div class="math notranslate nohighlight">
\[\int_{\Theta=0}^{x}{P(\Theta|y)} = \frac{\Beta(x; \alpha', \beta')}{\Beta(\alpha', \beta')}\]</div>
<p>You might say, that doesn’t really help, it’s just the same written different. You would be right, it doesn’t change anything. But it turns out that this function, the ratio of an incomplete beta function normalized by the complete beta function is yet another function with a specific name: the <strong>regularized incomplete beta function</strong>. And in general, in math, functions that received a name are typically very useful but very difficult to figure out, such that when the same formulae pops up in other problems, it can quickly be recognized and the solution that was figured out by other people can simply be applied without having to reinvent the wheel. The <strong>regularized incomplete beta function</strong> is written like so:</p>
<div class="math notranslate nohighlight">
\[I_{x}(\alpha, \beta)\]</div>
<p>Again, this is not a function that can simply be solved, and it would go beyond the scope of this book to dig further into it. But the fact that that function has a name means other people were interested enough in it to find a solution (or rather an approximation). This means that if we want to find the integral of our posterior, we can simply rely on that solution other people found.</p>
</section>
<section id="the-integral-of-a-probability-distribution-a-k-a-the-cummulative-distribution-function">
<h3>The integral of a probability distribution, a.k.a. the cummulative distribution function<a class="headerlink" href="#the-integral-of-a-probability-distribution-a-k-a-the-cummulative-distribution-function" title="Link to this heading">#</a></h3>
<p>So far, we have been dealing with probability distribution, with the understanding that such function give us for any value of a parameter (in our case <span class="math notranslate nohighlight">\(\Theta\)</span> or the probability of heads for our coin), it returns the probability of that parameter. These are what we call <strong>probability mass function (PMF)</strong> if our parameter is a discrete variable and a <strong>probability density function (PDF)</strong> if our parameter is continuous. But we have realized that what would be interesting to know based on our prior is what is the probability of <span class="math notranslate nohighlight">\(\Theta\)</span> being within a range of interest. And we figured out that this require taking the difference between the integral of the probability for each limit of our range. For our particular problem, we determined that the integral of the probability up to a given value of <span class="math notranslate nohighlight">\(\Theta=x\)</span> is the <strong>regularized incomplete beta function</strong>. Importantly, the integral of a probability distribution function has another name: <strong>The cummulative distribution function or CDF</strong>. In contrast to the PDF which returns for each value of <span class="math notranslate nohighlight">\(\Theta\)</span> the probability of that particular value, the CDF returns for each value of <span class="math notranslate nohighlight">\(\Theta\)</span> the probability of <span class="math notranslate nohighlight">\(\Theta\)</span> to be this or less than this.</p>
<p>So this means that for a beta distribution, the cummulative distribution function is the <strong>regularized incomplete beta function</strong>. And if we want to know the probability of <span class="math notranslate nohighlight">\(\Theta\)</span> to be within a particular range, we need to simply do:</p>
<div class="math notranslate nohighlight">
\[P(0.45&lt;=\Theta&lt;=0.55) = CDF(0.55) - CDF(0.45)\]</div>
<p>The reason I am mention this is because if you know the type of distribution you are dealing with and its parameter, you can simply look online for what the <strong>Cummlative distribution function</strong> of that distribution is, rather than having to figure out all the maths to arrive at the conclusion ‘In the case of the beta distribution, I need to use the regularized incomplete beta function, hopefully someone figured it out’. You can instead simply look for ‘What is the CDF of a beta distribution’, that should be quicker, way less math involved. And as it turns out, for most probability distribution families, the CDF is readily implemented in stats packages.</p>
<p>So with all that being said, we can now very easily compute the probability of <span class="math notranslate nohighlight">\(P(0.45&lt;=\Theta&lt;=0.55)\)</span>, we just need to solve this:</p>
<div class="math notranslate nohighlight">
\[P(0.45&lt;=\Theta&lt;= 0.55) = \int_{\Theta=0}^{0.55}{P(\Theta|y)d\Theta} - \int_{\Theta=0}^{0.45}{P(\Theta|y)d\Theta}\]</div>
<div class="math notranslate nohighlight">
\[P(0.45&lt;=\Theta&lt;= 0.55) = I_{0.55}(\alpha', \beta') - I_{0.45}(\alpha', \beta')\]</div>
<p>Where</p>
<div class="math notranslate nohighlight">
\[\alpha' = k+\alpha\]</div>
<div class="math notranslate nohighlight">
\[\beta' = n-k+\beta\]</div>
<p>But instead of looking in python for an implementation of the <strong>regularized incomplete beta function</strong>, we can simply look for the beta distribution CDF. It shouldn’t take you long to land on this <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.beta.html">page</a> and there you can see the CDF! We can very simply illustrate what that function looks like to get a better intuition (we just need to be careful about the meaning of the parameters in scipy, which are a bit different from the way we have implemented it so far):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">beta</span> <span class="k">as</span> <span class="n">beta_scipy</span>
<span class="n">thetas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax2</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">twinx</span><span class="p">()</span>
<span class="c1"># Plot the posterior:</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="p">[</span><span class="n">compute_posterior</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span> <span class="k">for</span> <span class="n">theta</span> <span class="ow">in</span> <span class="n">thetas</span><span class="p">],</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;PDF&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="p">[</span><span class="n">beta_scipy</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">alpha</span><span class="o">+</span><span class="n">k</span><span class="p">,</span> <span class="n">beta</span><span class="o">+</span><span class="n">n</span><span class="o">-</span><span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">theta</span> <span class="ow">in</span> <span class="n">thetas</span><span class="p">],</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;CDF&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">Theta$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;P($</span><span class="se">\\</span><span class="s1">Theta$|y)&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;P($</span><span class="se">\\</span><span class="s1">Theta &lt;= x$|y)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Posterior distribution&#39;</span><span class="p">)</span>
<span class="n">lines</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">get_legend_handles_labels</span><span class="p">()</span>
<span class="n">lines2</span><span class="p">,</span> <span class="n">labels2</span> <span class="o">=</span> <span class="n">ax2</span><span class="o">.</span><span class="n">get_legend_handles_labels</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">lines</span> <span class="o">+</span> <span class="n">lines2</span><span class="p">,</span> <span class="n">labels</span> <span class="o">+</span> <span class="n">labels2</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/21565da80cc6889e1031bffb86bcf40818d45a4fbab4f0e90c1f00a21a5bcbb3.png" src="_images/21565da80cc6889e1031bffb86bcf40818d45a4fbab4f0e90c1f00a21a5bcbb3.png" />
</div>
</div>
<p>Hopefully this makes sense, the CDF is basically summing up all the values under the green curve up until that point</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">beta</span> <span class="k">as</span> <span class="n">beta_scipy</span>

<span class="k">def</span> <span class="nf">proba_theta_range</span><span class="p">(</span><span class="nb">min</span><span class="p">,</span> <span class="nb">max</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the probability that the parameter θ (Theta) falls within a specified range</span>
<span class="sd">    [min, max] given a Beta posterior distribution, which is updated based on observed </span>
<span class="sd">    data and prior parameters.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    ----------</span>
<span class="sd">    min : float</span>
<span class="sd">        The lower bound of the range for θ.</span>
<span class="sd">    max : float</span>
<span class="sd">        The upper bound of the range for θ.</span>
<span class="sd">    n : int</span>
<span class="sd">        The total number of trials or observations.</span>
<span class="sd">    k : int</span>
<span class="sd">        The number of observed successes.</span>
<span class="sd">    alpha : float</span>
<span class="sd">        The shape parameter α of the Beta prior distribution.</span>
<span class="sd">    beta : float</span>
<span class="sd">        The shape parameter β of the Beta prior distribution.</span>

<span class="sd">    Returns:</span>
<span class="sd">    -------</span>
<span class="sd">    float</span>
<span class="sd">        The probability that θ falls within the specified range [min, max] under</span>
<span class="sd">        the Beta posterior distribution.</span>
<span class="sd">    </span>
<span class="sd">    Notes:</span>
<span class="sd">    -----</span>
<span class="sd">    - This function calculates the posterior distribution parameters α&#39; and β&#39; based</span>
<span class="sd">      on the observed data (n and k) and the Beta prior parameters (α and β):</span>
<span class="sd">        α&#39; = α + k</span>
<span class="sd">        β&#39; = β + n - k</span>
<span class="sd">    - The probability is computed using the cumulative distribution function (CDF)</span>
<span class="sd">      of the Beta distribution:</span>
<span class="sd">        P(min ≤ θ ≤ max | y) = CDF(max) - CDF(min)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">alpha_prime</span> <span class="o">=</span> <span class="n">k</span> <span class="o">+</span> <span class="n">alpha</span>
    <span class="n">beta_prime</span> <span class="o">=</span> <span class="n">n</span><span class="o">-</span><span class="n">k</span><span class="o">+</span><span class="n">beta</span>
    <span class="k">return</span> <span class="n">beta_scipy</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="nb">max</span><span class="p">,</span> <span class="n">alpha_prime</span><span class="p">,</span> <span class="n">beta_prime</span><span class="p">)</span> <span class="o">-</span> <span class="n">beta_scipy</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="nb">min</span><span class="p">,</span> <span class="n">alpha_prime</span><span class="p">,</span> <span class="n">beta_prime</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>So now, if we want to find the probability for our parameter to be between 0.45 and 0.55, we can get a simple answer:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;P(0.45 &lt;= $</span><span class="se">\\</span><span class="s2">Theta$ &lt;= 0.55)=</span><span class="si">{</span><span class="n">proba_theta_range</span><span class="p">(</span><span class="mf">0.45</span><span class="p">,</span><span class="w"> </span><span class="mf">0.55</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="p">,</span><span class="w"> </span><span class="n">beta</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>P(0.45 &lt;= $\Theta$ &lt;= 0.55)=0.4036507504715131
</pre></div>
</div>
</div>
</div>
<p>We can be 53% confident that the true value of <span class="math notranslate nohighlight">\(\Theta\)</span> is between 0.45 and 0.55 (note that the values will depend on the outcome of your coin toss, there is some randomness after all ;). Do you want to proceed with your bet? You aren’t 99% sure that it is within a comfortable range. Let’s see how confident you can be that it is between 0.4 and 0.6?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;P(0.40 &lt;= $</span><span class="se">\\</span><span class="s2">Theta$ &lt;= 0.60)=</span><span class="si">{</span><span class="n">proba_theta_range</span><span class="p">(</span><span class="mf">0.40</span><span class="p">,</span><span class="w"> </span><span class="mf">0.60</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="p">,</span><span class="w"> </span><span class="n">beta</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>P(0.40 &lt;= $\Theta$ &lt;= 0.60)=0.7314485372625755
</pre></div>
</div>
</div>
</div>
<p>Yeah okay, 86%. Do you think you should proceed with your bet? Now of course, the answer depends quite a bit on what the bet is about. If you are betting who goes buy coffee beans for the office next, that’s probably good enough, but if you are betting 1000€, you might want to collect a bit more data. But at least now we have a straight forward and complete answer to our question! Congrats, we’ve made it.</p>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="summary">
<h1>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h1>
<p>We have seen quite a lot up until now, and this chapter was a bit much. So let’s recapitulate what we have seen so far. We started with our simple question: <strong>Is our coin biased</strong>? Which is equivalent to asking <strong>Is the probability of getting head of 0.5</strong>? Which is written <span class="math notranslate nohighlight">\(P(X=1)=0.5\)</span>. We then saw that we can run an experiment to obtain data that should help to answer our question. But we realized that the results of our experiment shouldn’t be trusted and that the only thing we can know from our experiment is the probability of our data under different value of <span class="math notranslate nohighlight">\(P(X=1)\)</span>, which we call the <strong>likelihood</strong>, which is described as <span class="math notranslate nohighlight">\(P(y|\Theta)\)</span>. We said that <span class="math notranslate nohighlight">\(\Theta\)</span> is just the general notation to describe any parameters we are interested in.</p>
<p>Then, in this chapter, we introduced the Bayes theorem, which enables us to go from data and their likelihood <span class="math notranslate nohighlight">\(P(y|\Theta)\)</span> to something called the <strong>posterior</strong>, which is the probability of our parameter of interest given the data, <span class="math notranslate nohighlight">\(P(\Theta|y)\)</span>. The reason we are interested in this posterior is because it is the closest we can get from getting a straight answer to our original question <strong>Is our coin biased?</strong> based on experimental data, by telling us how confident we can be that a given value of <span class="math notranslate nohighlight">\(\Theta\)</span> is equal to the true value of <span class="math notranslate nohighlight">\(\Theta\)</span> given our experimental data. In other words, the posterior tells us the probability of every value of <span class="math notranslate nohighlight">\(\Theta\)</span> after seeing the data. If the values close to <span class="math notranslate nohighlight">\(\Theta=0.5\)</span> are highly probable, then we have evidence that our coin isn’t biased. We saw that the Bayes theorem enables to go from the likelihood to the posterior by multiplying the likelihood by the <strong>prior</strong> and dividing the whole by something called the <strong>marginal liklelihood or model evidence</strong>. This is written like so:</p>
<div class="math notranslate nohighlight">
\[P(\Theta|y) = \frac{P(y|\Theta)P(\Theta)}{P(y)}\]</div>
<p>Along the way we introduced the concepts of probability distrubution. Each term in the Bayes theorem is a probability distribution:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(y|\Theta)\)</span> (Likelihood): Probability of the data for each value of <span class="math notranslate nohighlight">\(\Theta\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P(\Theta)\)</span> (Prior): Probability of each value of  <span class="math notranslate nohighlight">\(\Theta\)</span> based on our own belief (or generally previous knowledge before seeing any data)</p></li>
<li><p><span class="math notranslate nohighlight">\(P(y)\)</span> (Posterior): Probability of the data after marginalizing over all values of <span class="math notranslate nohighlight">\(\Theta\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P(\Theta|y)\)</span> (Posterior): Probability of each value of our parameter after we saw the data</p></li>
</ul>
<p>In general, the function to use for the likelihood depends on the data we are describing. In the case of our coin toss problem, the Binomial distribution is the likelihood function to describe the probabiltiy of each possible number of heads out of a given number of coin tosses. In contrast, there are many probability distribution that can be used to specify our prior. Which one to choose depends on the prior, because some prior can combine with the likelihood in ways that are easy to deal with mathetmatically. And if if exists for the likelihood applicable to our problem, we should strive to find a prior distribution that combines with the likelihood to return the same distribution family as the prior. This is what we call a conjugate prior, and it is very handy because then, we only need to figure out how to update the same parameters as our prior distribution to solve the equation. In the case of a binomial distribution, the beta is the conjugate. Everything we did from there on was play around with the mathematical formulae to obtain our prior.</p>
<p>Importantly, when working with Bayesian inference, you won’t have to do all the maths we did above. Scientists have been working with Bayesian stats for decades and have figured out many useful cases already. The reason we went through this exercise is just to get rid of the math-phobia and the feeling of ‘not understanding anything if I can’t follow each of the steps’. In most tutorial that illustrate Bayes stats with the binomial distribution (for coin tosses or else), they simply say something like this:</p>
<div class="math notranslate nohighlight">
\[P(\Theta|y) = \frac{P(y|\Theta)P(\Theta)}{P(y)}\]</div>
<p>So:</p>
<div class="math notranslate nohighlight">
\[P(\Theta|y) = {\frac{\Gamma(\alpha'+\beta')}{\Gamma(\alpha')\Gamma(\beta')}}\times\Theta^{\alpha' -1}(1-\Theta)^{\beta'-1}\]</div>
<p>Or maybe they rely on code and simply say:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_posterior</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the posterior distribution of a parameter given a binomial likelihood and a Beta prior.</span>
<span class="sd">    </span>
<span class="sd">    Parameters:</span>
<span class="sd">    theta (float): The value at which to evaluate the posterior distribution.</span>
<span class="sd">    n (int): Total number of trials (e.g., flips in a coin-toss experiment).</span>
<span class="sd">    k (int): Number of observed successes (e.g., heads in coin tosses).</span>
<span class="sd">    alpha (float): Alpha parameter of the Beta prior distribution.</span>
<span class="sd">    beta (float): Beta parameter of the Beta prior distribution.</span>

<span class="sd">    Returns:</span>
<span class="sd">    float: The probability density of the posterior distribution evaluated at the given theta.</span>
<span class="sd">    </span>
<span class="sd">    Notes:</span>
<span class="sd">    - The function calculates the posterior as a Beta distribution with updated parameters:</span>
<span class="sd">        - alpha&#39; = k + alpha</span>
<span class="sd">        - beta&#39; = n - k + beta</span>
<span class="sd">    - The posterior distribution follows a Beta(alpha&#39;, beta&#39;) distribution.</span>
<span class="sd">    - This function assumes `beta_distribution` is defined elsewhere and represents the Beta distribution PDF.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">alpha_prime</span> <span class="o">=</span> <span class="n">k</span> <span class="o">+</span> <span class="n">alpha</span>
    <span class="n">beta_prime</span> <span class="o">=</span> <span class="n">n</span><span class="o">-</span><span class="n">k</span><span class="o">+</span><span class="n">beta</span>
    <span class="k">return</span> <span class="n">beta_distribution</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">alpha_prime</span><span class="p">,</span> <span class="n">beta_prime</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>And you are left confused, wondering ‘how on earth do we go from the Bayes theorem to this?!’. Now you know for this particular example. When dealing with other likelihood and prior distributions, the exact maths behind it are different, but it’s not black magic. And sure, it might be beyond your mathematical skills to understand and follow, but at least you now know in principle how that works.</p>
<p>However, one of the reason why Bayesian stats are so notoriously difficult is that there aren’t many cases in which we can find solution as easily as we did for our example. That may sound daunting if the above didn’t feel that easy. That’s normal. After all, the above wasn’t exactly easy, it took us hundreds of lines of code, text and equations to get through. But at least we got to a solution in the end, and hopefully using maths that are familiar to most. The thing is that for many interesting problems, there are no easy conjugate priors that play well with the likelihood function of the data, and in many cases, there are <strong>no analytical solutions</strong>, which means we can’t just solve the equation, as we will see in the next chapter. But now that you know how things play out for cases in which we can find a solution, you should be able to understand why in some cases, we can’t find one, which should make it easy to understand why we need <strong>variational Laplace</strong> for, which should in turn make it easier to understand it. Pfiou, quite a program, hey?</p>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="ProbabilityDistribution.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Probablities and probability distribution</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">The Bayes theorem</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prior-distribution">Prior distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#beta-distribution">Beta distribution</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#combining-the-prior-and-the-likelihood">Combining the prior and the likelihood</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#solving-the-numerator">Solving the numerator</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#another-beta-distribution">Another beta distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-combined-prior-and-likelihood">The combined prior and likelihood</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#marginal-likelihood-a-k-a-model-evidence">Marginal likelihood, a.k.a. model evidence</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deriving-the-marginal-likelihood-for-our-problem">Deriving the marginal likelihood for our problem</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#python-implementation-of-the-likelihood">Python implementation of the likelihood</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#putting-everything-together-computing-the-posterior">Putting everything together: computing the posterior</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#more-maths-computing-p-0-49-theta-0-51">More maths… Computing <span class="math notranslate nohighlight">\(P(0.49&lt;=\Theta&lt;=0.51)\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#aside-a-simplified-version-of-the-posterior">Aside: a simplified version of the posterior</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-integral-of-the-simplified-posterior">The integral of the simplified posterior:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-integral-of-a-probability-distribution-a-k-a-the-cummulative-distribution-function">The integral of a probability distribution, a.k.a. the cummulative distribution function</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Alex Lepauvre, Jan Gabriel Hartel
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>