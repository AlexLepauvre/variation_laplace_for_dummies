
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Bayesian statistics &#8212; My sample book</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'BayesianStatistics';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Welcome to your Jupyter Book" href="intro.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="My sample book - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="My sample book - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome to your Jupyter Book
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Bayesian statistics</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2FBayesianStatistics.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/BayesianStatistics.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Bayesian statistics</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#intuitions-about-probabilities-using-a-coin-toss-example">Intuitions about probabilities using a coin toss example</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#running-simulated-experiments">Running (simulated) experiments</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-the-data">Interpreting the data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#running-a-better-experiment">Running a better experiment</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#answering-the-question-based-on-our-data">Answering the question based on our data</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#formalizing-our-intuition">Formalizing our intuition</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#definitions">Definitions:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-bernoulli-distribution-a-single-formulae-to-express-the-probability-of-each-event">The Bernoulli distribution: a single formulae to express the probability of each event</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-binomial-distribution-a-formulae-to-determine-how-likely-a-given-empirical-probability-is-based-on-any-given-true-probability">The Binomial distribution: a formulae to determine how likely a given empirical probability is based on any given true probability</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-we-have-seen-so-far">What we have seen so far:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-probabilities-and-probabilities-of-probabilities">Conditional probabilities and probabilities of probabilities</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-bernoulli-distribution">The Bernoulli distribution:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#usage-of-the-binomial-distribution">Usage of the Binomial distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#graphical-representation">Graphical representation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#on-the-usefulness-of-mathematical-formalism">On the usefulness of mathematical formalism</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-bayes-theorem-from-p-y-theta-to-p-theta-y">The Bayes theorem: from <span class="math notranslate nohighlight">\(P(y|\theta)\)</span>, to <span class="math notranslate nohighlight">\(P(\theta|y)\)</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-little-aside-we-can-compute-the-empirical-probability-of-obtaining-3-10-heads">*A little aside: we can compute the empirical probability of obtaining 3/10 heads+</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="bayesian-statistics">
<h1>Bayesian statistics<a class="headerlink" href="#bayesian-statistics" title="Link to this heading">#</a></h1>
<p>Before we get started with any maths, let’s explore an everyday problem to illustrate what statistics are about. In daily life, we often wonder about things we don’t know. I think most people would like to figure out how much pasta they need to cook for dinner, to finally stop eating pasta for three days when we just wanted a light meal for a single evening. Or when we get dressed in the morning, we would like to know whether it is hot or cold outside, so that we dress appropriately. Sometimes, the answer is straightforward: we can open the window and stick our arm out. We have our answer (it is warm outside) and we can put on our favorit shortpants. Alex can lick my ass hard and wet. Sometimes it is a bit more complicated: if we want to know how much pasta we should cook, we should probably cool different amounts of pasta over different meals (and make sure we are equally hungry for each meal) and check which amount was the most adequate. The field of probability and statistics are basically dedicated to describe this kind of problems: there is something we can’t know directly, but we can gather some observations and based on those make an educated guess about what we want to know. For simple problems (knowing if it’s warm outside), no need for complicated mathematics, but for more complicated problems, we do need them.</p>
<p>We could use many example to explain all the necessary concepts of statistics and to make clear what Bayesian statistics is about and why it is helpful. But it just so happens that some of these examples are a bit easier to wrap ones head around, so we will stick to those at first. Ultimately, brilliant statisticians and scientists main job is to be able to express any interesting and relevant problems into mathematical terms to study them. But that is of course a very high bar that you should not expect the reach at the end of this chapter. But at the very least, you should kind of get the idea of how one does that in principle, even if you are not able to do that yourself.</p>
<p>To keep things simple, we will use the example of tossing a coin. Let’s say we are making a bet with a colleague that our coin is going to land on head, while our colleague bets that it is going to land on tail. Our colleague has trust issues, and he argues that we need to make sure that the coin is fair–meaning it has an equal chance of landing on heads or tail.. But of course if the coin is biased, say it lands more often on head than on tail, then the bet isn’t quite fair, and the person betting on head has an advantage. So before we get started, we need to make sure that the coin is fair.</p>
<section id="intuitions-about-probabilities-using-a-coin-toss-example">
<h2>Intuitions about probabilities using a coin toss example<a class="headerlink" href="#intuitions-about-probabilities-using-a-coin-toss-example" title="Link to this heading">#</a></h2>
<p>We are trying to answer a simple question: <strong>is the coin fair or not?</strong></p>
<section id="running-simulated-experiments">
<h3>Running (simulated) experiments<a class="headerlink" href="#running-simulated-experiments" title="Link to this heading">#</a></h3>
<p>We don’t know the answer and we can’t know the answer just like that. Yet we need to figure it out somehow. One way to do this is to run a little experiment. We could throw the coin in the air twice. If the coin is balanced, then we would expect it to land once on head, once on tail. This is not a very good experiment, because this is not what we mean by fair. If the coin would alternate between head and tail on every second try, then it wouldn’t be fair bet, beacuse if anyone knew what the coin landed on in the previous throw, they could know what it will land on in this throw and would win any bet. What we mean by fair is that the chances for the coin to land on head or tail are equal on every single throw.</p>
<p>So how can we know whether that is truly the case? We can’t just toss the coin twice and expect to get one head and one tail after another. It would seem intuitive to say “let’s throw the coin many times”. Then, we can count how often the coin lands on head and how often it lands on tail to see if across all these tosses, we get half head and half tail. We can conduct this experiment programatically. Even if you don’t understand exactly how the code works, you only need to understand the following:</p>
<ul class="simple">
<li><p>n_throw: how often we throw our simulated coin</p></li>
<li><p>n_head: each time the coin lands on head, add 1, so that then we know the total number of head out of 10 tosses</p></li>
<li><p>n_tail: each time the coin lands on tail, add 1, so that then we know the total number of tail out of 10 tosses</p></li>
<li><p>p_head: <strong>Probability of getting head</strong> out of our 10 tosses, i.e. how often it lands on head out of 10</p></li>
<li><p>p_head: <strong>Probability of getting tail</strong> out of our 10 tosses, i.e. how often it lands on tail out of 10</p></li>
</ul>
<p>Let’s say we do 10 tosses and see what happens:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">n_throw</span> <span class="o">=</span> <span class="mi">10</span> <span class="c1"># We will throw the coin 10 times</span>
<span class="n">n_head</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># Before we start, we have zero head</span>
<span class="n">n_tail</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># And zero tails</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_throw</span><span class="p">):</span>  <span class="c1"># Repeat the same thing 10 times (throwing the coin)</span>
    <span class="n">rnd</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">()</span>  <span class="c1"># Draw a random number between 0 and 1 (following a uniform distribution, so each value between 0 and 1 is equally likely)</span>
    <span class="k">if</span> <span class="n">rnd</span> <span class="o">&lt;=</span> <span class="mf">0.5</span><span class="p">:</span>  <span class="c1"># If our random number is less than 0.5, we consider that our coin landed on head.</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Throw </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: Head&quot;</span><span class="p">)</span>
        <span class="n">n_head</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>   <span class="c1"># If our random number is more than 0.5, we consider that our coin landed on tail</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Throw </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: Tail&quot;</span><span class="p">)</span>
        <span class="n">n_tail</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="c1"># Compute the probability of head and tail:</span>
<span class="n">p_head</span> <span class="o">=</span> <span class="n">n_head</span><span class="o">/</span><span class="n">n_throw</span>   <span class="c1"># The probability of head is simply how often we obtained head in our 10 throw, divided by the number throws</span>
<span class="n">p_tail</span> <span class="o">=</span> <span class="n">n_tail</span><span class="o">/</span><span class="n">n_throw</span>   <span class="c1"># The probability of head is simply how often we obtained head in our 10 throw, divided by the number throws</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">P(Head)=</span><span class="si">{</span><span class="n">p_head</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;P(Tail)=</span><span class="si">{</span><span class="n">p_tail</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Throw 0: Tail
Throw 1: Tail
Throw 2: Tail
Throw 3: Tail
Throw 4: Head
Throw 5: Tail
Throw 6: Head
Throw 7: Tail
Throw 8: Tail
Throw 9: Head

P(Head)=0.3
P(Tail)=0.7
</pre></div>
</div>
</div>
</div>
</section>
<section id="interpreting-the-data">
<h3>Interpreting the data<a class="headerlink" href="#interpreting-the-data" title="Link to this heading">#</a></h3>
<p>After 10 throw, the coin didn’t land half of the time on head and half of the time on head. Instead, it landed 30% of the time on head and 70% of the time on tail. That doesn’t seem very balanced. At the same time, you might argue that maybe the coin is indeed balanced, it is just that in these 10 throws, we got “unlucky” because it landed more often on tail than on head. Indeed, if whether we get head or tail is random in each toss, then even if the coin is balanced, it is not impossible to get 7 times tail out of 10 throws. It is also not impossible to get 10 times head in a row.</p>
<p>That being said, you probably get the intuition that while it is possible to get 10 times head in a row, it is not very likely. And you are even less likely to get 1000 heads in a row if you were to do a 1000 throws.</p>
</section>
<section id="running-a-better-experiment">
<h3>Running a better experiment<a class="headerlink" href="#running-a-better-experiment" title="Link to this heading">#</a></h3>
<p>Back to our initial problem: <strong>we want to know if our coin is fair</strong>. Based on our initial experiment, it is unclear whether we should answer yes, because we don’t know if we just got unlucky and ended up in a case that is not representative of the true head/tail ratio of our coin. How can we do a better experiment? Well just as when we said that throwing the coin twice isn’t enough to tell whether the coin is balanced, it is quite intuitive to think that 10 times isn’t enough either. So we can try to increase the number of coin tosses we make: the more often we repeat our little experiment the more confident we can be in our final answer as to whether the coin is biased.</p>
<p>Let’s try it out (the cose isn’t showing the outcome of each throw anymore, just because that would be a bit too much):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Same as before, but let&#39;s increase the number of throws:</span>
<span class="n">n_throw</span> <span class="o">=</span> <span class="mi">20</span> <span class="c1"># 20 throws instead of 10</span>
<span class="n">n_head</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># Before we start, we have zero head</span>
<span class="n">n_tail</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># And zero tails</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_throw</span><span class="p">):</span>  <span class="c1"># Repeat the same thing 10 times (throwing the coin)</span>
    <span class="n">rnd</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">()</span>  <span class="c1"># Draw a random number between 0 and 1 (following a uniform distribution, so each value between 0 and 1 is equally likely)</span>
    <span class="k">if</span> <span class="n">rnd</span> <span class="o">&lt;=</span> <span class="mf">0.5</span><span class="p">:</span>  <span class="c1"># If our random number is less than 0.5, we consider that our coin landed on head.</span>
        <span class="n">n_head</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>   <span class="c1"># If our random number is more than 0.5, we consider that our coin landed on tail</span>
        <span class="n">n_tail</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="c1"># Compute the probability of head and tail:</span>
<span class="n">p_head</span> <span class="o">=</span> <span class="n">n_head</span><span class="o">/</span><span class="n">n_throw</span>   <span class="c1"># The probability of head is simply how often we obtained head in our 10 throw, divided by the number throws</span>
<span class="n">p_tail</span> <span class="o">=</span> <span class="n">n_tail</span><span class="o">/</span><span class="n">n_throw</span>   <span class="c1"># The probability of head is simply how often we obtained head in our 10 throw, divided by the number throws</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">P(Head)=</span><span class="si">{</span><span class="n">p_head</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;P(Tail)=</span><span class="si">{</span><span class="n">p_tail</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>P(Head)=0.35
P(Tail)=0.65
</pre></div>
</div>
</div>
</div>
<p>We can see that when we increase the number of throws, the final probability change, and it seems to get closer to 50/50. But still not quite 50/50. So the same question applies again: is our coin biased, or is it just that particular draw that didn’t land on 50/50? We can try to increase the number of throws to much more, say a 1000:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Same as before, but let&#39;s increase the number of throws:</span>
<span class="n">n_throw</span> <span class="o">=</span> <span class="mi">1000</span> <span class="c1"># 20 throws instead of 10</span>
<span class="n">n_head</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># Before we start, we have zero head</span>
<span class="n">n_tail</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># And zero tails</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_throw</span><span class="p">):</span>  <span class="c1"># Repeat the same thing 10 times (throwing the coin)</span>
    <span class="n">rnd</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">()</span>  <span class="c1"># Draw a random number between 0 and 1 (following a uniform distribution, so each value between 0 and 1 is equally likely)</span>
    <span class="k">if</span> <span class="n">rnd</span> <span class="o">&lt;=</span> <span class="mf">0.5</span><span class="p">:</span>  <span class="c1"># If our random number is less than 0.5, we consider that our coin landed on head.</span>
        <span class="n">n_head</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>   <span class="c1"># If our random number is more than 0.5, we consider that our coin landed on tail</span>
        <span class="n">n_tail</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="c1"># Compute the probability of head and tail:</span>
<span class="n">p_head</span> <span class="o">=</span> <span class="n">n_head</span><span class="o">/</span><span class="n">n_throw</span>   <span class="c1"># The probability of head is simply how often we obtained head in our 10 throw, divided by the number throws</span>
<span class="n">p_tail</span> <span class="o">=</span> <span class="n">n_tail</span><span class="o">/</span><span class="n">n_throw</span>   <span class="c1"># The probability of head is simply how often we obtained head in our 10 throw, divided by the number throws</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">P(Head)=</span><span class="si">{</span><span class="n">p_head</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;P(Tail)=</span><span class="si">{</span><span class="n">p_tail</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>P(Head)=0.523
P(Tail)=0.477
</pre></div>
</div>
</div>
</div>
<p>When we increase the number of iterations to a 1000, the probability of head and tail seems to get close to 50/50. And if our intuition that we should get more reliable answer if we throw the coin more often is true, then the result of this experiment would lead us to believe that our coin is probably not biased. There is one way in which we can show that increasing the number of throws yields a more reliable answer. We could perform the same experiment several times.</p>
<p>Let’s say we have a first experiment in which we throw the coin 10 times, and a second experiment in which we throw the coin 1000 times. To know which of these two experiments is most reliable, we can repeat each experiment 10 times. A more reliable experiment should gives similar results across repetitions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Same as before, but let&#39;s increase the number of throws:</span>
<span class="n">n_iteration</span> <span class="o">=</span> <span class="mi">5</span>

<span class="c1"># ========================================================</span>
<span class="c1"># Experiment 1:</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">40</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Experiment 1&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iteration</span><span class="p">):</span>
    <span class="n">n_throw</span> <span class="o">=</span> <span class="mi">10</span> <span class="c1"># 20 throws instead of 10</span>
    <span class="n">n_head</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># Before we start, we have zero head</span>
    <span class="n">n_tail</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># And zero tails</span>

    <span class="k">for</span> <span class="n">ii</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_throw</span><span class="p">):</span>  <span class="c1"># Repeat the same thing 10 times (throwing the coin)</span>
        <span class="n">rnd</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">()</span>  <span class="c1"># Draw a random number between 0 and 1 (following a uniform distribution, so each value between 0 and 1 is equally likely)</span>
        <span class="k">if</span> <span class="n">rnd</span> <span class="o">&lt;=</span> <span class="mf">0.5</span><span class="p">:</span>  <span class="c1"># If our random number is less than 0.5, we consider that our coin landed on head.</span>
            <span class="n">n_head</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>   <span class="c1"># If our random number is more than 0.5, we consider that our coin landed on tail</span>
            <span class="n">n_tail</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="c1"># Compute the probability of head and tail:</span>
    <span class="n">p_head</span> <span class="o">=</span> <span class="n">n_head</span><span class="o">/</span><span class="n">n_throw</span>   <span class="c1"># The probability of head is simply how often we obtained head in our 10 throw, divided by the number throws</span>
    <span class="n">p_tail</span> <span class="o">=</span> <span class="n">n_tail</span><span class="o">/</span><span class="n">n_throw</span>   <span class="c1"># The probability of head is simply how often we obtained head in our 10 throw, divided by the number throws</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;P(Head)=</span><span class="si">{</span><span class="n">p_head</span><span class="si">}</span><span class="s2">/P(Tail)=</span><span class="si">{</span><span class="n">p_tail</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># ========================================================</span>
<span class="c1"># Experiment 2:</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">40</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Experiment 2&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iteration</span><span class="p">):</span>
    <span class="n">n_throw</span> <span class="o">=</span> <span class="mi">1000</span> <span class="c1"># 20 throws instead of 10</span>
    <span class="n">n_head</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># Before we start, we have zero head</span>
    <span class="n">n_tail</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># And zero tails</span>

    <span class="k">for</span> <span class="n">ii</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_throw</span><span class="p">):</span>  <span class="c1"># Repeat the same thing 10 times (throwing the coin)</span>
        <span class="n">rnd</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">()</span>  <span class="c1"># Draw a random number between 0 and 1 (following a uniform distribution, so each value between 0 and 1 is equally likely)</span>
        <span class="k">if</span> <span class="n">rnd</span> <span class="o">&lt;=</span> <span class="mf">0.5</span><span class="p">:</span>  <span class="c1"># If our random number is less than 0.5, we consider that our coin landed on head.</span>
            <span class="n">n_head</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>   <span class="c1"># If our random number is more than 0.5, we consider that our coin landed on tail</span>
            <span class="n">n_tail</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="c1"># Compute the probability of head and tail:</span>
    <span class="n">p_head</span> <span class="o">=</span> <span class="n">n_head</span><span class="o">/</span><span class="n">n_throw</span>   <span class="c1"># The probability of head is simply how often we obtained head in our 10 throw, divided by the number throws</span>
    <span class="n">p_tail</span> <span class="o">=</span> <span class="n">n_tail</span><span class="o">/</span><span class="n">n_throw</span>   <span class="c1"># The probability of head is simply how often we obtained head in our 10 throw, divided by the number throws</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;P(Head)=</span><span class="si">{</span><span class="n">p_head</span><span class="si">}</span><span class="s2">/P(Tail)=</span><span class="si">{</span><span class="n">p_tail</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>========================================
Experiment 1
P(Head)=0.6/P(Tail)=0.4
P(Head)=0.5/P(Tail)=0.5
P(Head)=0.6/P(Tail)=0.4
P(Head)=0.6/P(Tail)=0.4
P(Head)=0.5/P(Tail)=0.5
========================================
Experiment 2
P(Head)=0.481/P(Tail)=0.519
P(Head)=0.497/P(Tail)=0.503
P(Head)=0.531/P(Tail)=0.469
P(Head)=0.512/P(Tail)=0.488
P(Head)=0.507/P(Tail)=0.493
</pre></div>
</div>
</div>
</div>
<p>In the first experiment (where we throw the coin 10 times), the results we get differ quite a bit between repetition, sometimes we get 50/50, but sometimes we get 20% head, 80% tail. In comparison, the results of the second experiment (where we throw the coin 1000 times) vary much less across repeats: we get results between 40% and 60% probability for head and tail. So it would seem that our intuition that throwing the coin many times gets us more reliable results. So if we want to know if our coin is biased or not, we should throw the coin many many times.</p>
<p>And based on the 1000 throws experiment, it would seem that we get results that are close to 50/50, which seems to be what we would expect to happen if our coin isn’t biased</p>
</section>
<section id="answering-the-question-based-on-our-data">
<h3>Answering the question based on our data<a class="headerlink" href="#answering-the-question-based-on-our-data" title="Link to this heading">#</a></h3>
<p>Our intuition seems to tell us that our coin isn’t biased, because when we throw it a 1000 times, we get close to 50/50. So a resonable answer is <strong>our coin isn’t biased</strong>.</p>
<p>Along the way, we also noticed a few interesting things. First, we realized that we can’t trust a 100% the results of our experiment, because even if the coin is balanced, we might not get exactly 50/50 head and tail when we throw the coin several times. In a sense, this means that we can never know for sure whether or not our coin is biased:</p>
<ul class="simple">
<li><p>If the coin is perfectly fair, we will not get exactly 50/50 every time</p></li>
<li><p>If the coin isn’t fair, we might still get 50/50 in some experiments</p></li>
</ul>
<p>This is true: we can never know whether for sure if our coin is biased or not. But then, how can we know based on the results of our experiment whether the best answer is Yes or no, if the same outcome can occur with a biased and with a fair coin? The answer is this: while we can not know with certainty whether or not our coin is biased, we can figure out <strong>how likely is it that my coin is not biased, given the results of my experiment</strong>. In other words, we can get the answer to the question: “How confident are you that the coin is not biased?”. If the answer is very, then logically the best answer is to say: my coin is not biased.</p>
<p>This may sound very familiar with the intuition of increasing the number of coin toss to get an answer that we felt we can trust more. This begs the question: why should we trust the results of the experiment where we throw the coin more often more than the one of the experiment where we throw the coin only a few times? A logical answer is that if the coin isn’t biased, we are less likely to observe something very different from the 50/50 ratio when we throw the coin many times. Another way to say that is: <strong>given that our coin is not biased, the probability of observing something close to 50/50 is larger when we throw the coin many times than when we throw the coin a few times</strong></p>
<p>This sentence sounds a bit similar to the one above (<strong>how likely is it that my coin is not biased, given the results of my experiment</strong>), but it is phrased the other way around. In our experiment, because we felt <strong>confident that the results of our experiment accurately reflects the true ratio of the coin and because our final results was about 50/50</strong> we deduce that <strong>we are confident that the true ratio is 50/50, given that we have observed a ratio of 50/50 in our experiment</strong>. It is crucial to understand that these are not the same. This is the same as saying “How confident am I that the sky is cloudy given that it is raining” vs. “How confident am I that it is raining, given that the sky is cloudy?”. While the sky is most often cloudy when it is raining, it is not always true that it is raining when the sky is cloudy. To decide whether or not to proceed with our bet, we need to k now how confident we are that our coin isn’t biased, given the results of our experiment.</p>
<p>This is in a nutshell the goal of Bayesian statistics: figuring out the <strong>probability of a parameter</strong> (such as the true head/tail ratio), given some data (such as the result of an experiment in which we throw the coin a bunch of times). Knowing the probability of a parameter basically tells us how much we should trust the results of our experiment. In our example, we did it all based on intuition, but we will now dig into the actual math that enable to get that probability for any kinds of problems.</p>
</section>
</section>
<section id="formalizing-our-intuition">
<h2>Formalizing our intuition<a class="headerlink" href="#formalizing-our-intuition" title="Link to this heading">#</a></h2>
<p>In the section above, we mentioned a couple of terms and intuition. To do maths we them, we need to define a couple of those terms. We tried to infer the <strong>True</strong> probability of head and tail using experimental data. We threw the coin several times and computed the <strong>experimental</strong> or <strong>empirical</strong> probability of head and tail. And we can clearly see that the <strong>empirical probability</strong> of head or tail is not exactly the same as the <strong>true probability</strong> of head or tail, because the actual values we obtained in our experiment varied across experiments, and depending on the exact parameters of the experiment (the number of throw we used to compute the empirical probability).</p>
<section id="definitions">
<h3>Definitions:<a class="headerlink" href="#definitions" title="Link to this heading">#</a></h3>
<p>In general, a probability is defined as:
$<span class="math notranslate nohighlight">\(P(A) = \frac{|A|}{|\Omega|}\)</span><span class="math notranslate nohighlight">\(
Which reads:
\)</span><span class="math notranslate nohighlight">\(P(event) = \frac{Number\ of\ favorable\ outcomes\ for\ event\ A}{Total\ number\ of\ possible\ outcomes\ in\ the\ sample\ space}\)</span>$</p>
<p>In probability theory, the various terms in the bottom equation refer to quite simple things:</p>
<ul class="simple">
<li><p><strong>Event</strong>: a specific outcome of an experiment. In the case of a coin toss, this is either getting head or tail. But say we want to run another experiment in which we draw a card in a 52 cards deck at random, drawing a king of spade would also be an event for example.</p></li>
<li><p><strong>Sample space</strong>: set of all possible outcomes of the experiment. In our coin toss example, the sample space is basically head and tail, as these are the only two possible outcomes from throwing a coin. Similarly, if we draw a card from a deck, the sample space is basically all the cards in the deck. And so in the case of a coin toss, the total number of possible outcome in the sample space is 2, but in an experiment where we draw a card, it’s 52.</p></li>
<li><p><strong>Number of favorable outcomes for event A</strong>: how often does the event occurs out of all the possible outcomes. This is basically the quantity that defines the probability of that particular event.</p></li>
</ul>
<p>So say we have a fair coin, the number of favorable outcomes for event head should be 1 and the total number of outcomes is 2. So we have:
$<span class="math notranslate nohighlight">\(P(Head) = \frac{1}{2}\)</span>$</p>
<p>And</p>
<div class="math notranslate nohighlight">
\[P(Tail) = \frac{1}{2}\]</div>
<p>And because head and tail are the two possible outcomes, the sum of the probability of both events should sum up to 1. In comparison, the probability of picking up a king of spade is:</p>
<div class="math notranslate nohighlight">
\[P(King\ of\ Spade) = \frac{1}{52}\]</div>
<p>And of picking up a jack of heart:</p>
<div class="math notranslate nohighlight">
\[P(Jack\ of\ Heart) = \frac{1}{52}\]</div>
<p>If we sum the probability of a king of spade and of a jack of heart, we don’t get 1, which means that we are missing some events.</p>
<p>An important thing to understand is that the probability defined above refer to the true probability of each event. This is what we want to figure out in our experiment above. But we would only obtain the true probability of each event if we were to run an infinity of experiments. Since we can’t run an infinity of experiments, we can not get the true probability of each event. Instead, we get the <strong>empirical probability</strong>, which is usually depicted like so:</p>
<div class="math notranslate nohighlight">
\[
\hat{P}(E) = \frac{\text{Number of times event } E \text{ occurs}}{\text{Total number of trials}}
\]</div>
<p>The hat just indicates that this is something we obtained from an experiment and isn’t the true probability of a given event.</p>
</section>
</section>
<section id="the-bernoulli-distribution-a-single-formulae-to-express-the-probability-of-each-event">
<h2>The Bernoulli distribution: a single formulae to express the probability of each event<a class="headerlink" href="#the-bernoulli-distribution-a-single-formulae-to-express-the-probability-of-each-event" title="Link to this heading">#</a></h2>
<p>When we throw a fair coin, the probability of obtaining head should be 0.5 and the probability of obtaining tail is 0.5. So based on the definition we have laid out above:
$<span class="math notranslate nohighlight">\(P(Head) = 0.5\)</span><span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\(P(Tail) = 0.5\)</span><span class="math notranslate nohighlight">\(
And as we have mentioned above, the probability of the two events should be 1, because there are no other possible outcome. So if we know the probability of head, we know the probability of head, we know the probability of tail:
\)</span><span class="math notranslate nohighlight">\(P(Head) + P(Tail) = 1\)</span><span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\(P(Head) = 1 - P(Tail)\)</span>$</p>
<p>Now head and tail are very specific words that only make sense when the experiment is tossing a coin. We can make things a bit more general by rewriting it as follows:</p>
<div class="math notranslate nohighlight">
\[P(X = Head) + P(X = Tail) = 1\]</div>
<p>We can make the above even more general. For any case in which only one of two outcome can occur (head or tail, but also pass or fail a test, is it raining or not…), the above is true. To be more general, we can say that $<span class="math notranslate nohighlight">\(Head = 1\)</span><span class="math notranslate nohighlight">\( and \)</span><span class="math notranslate nohighlight">\(Tail = 0\)</span>$. This doesn’t change anything, but it means that we can use the way of writing for any problem that also has two possible outcomes (note that often times, the outcome associated with 1 is called “success” and 0 as “fail”, these is inconsequential, just conventions). Tossing a fair coin just so happens to be defined as:</p>
<div class="math notranslate nohighlight">
\[P(X=1) = 0.5\]</div>
<div class="math notranslate nohighlight">
\[P(X=0) = 0.5\]</div>
<p>We can use the same notation to describe whether I will pass my exam tomorrow or not (where 1 means I pass, 0 means I fail):</p>
<div class="math notranslate nohighlight">
\[P(X=1) = 0.2\]</div>
<div class="math notranslate nohighlight">
\[P(X=0) = 0.8\]</div>
<p>I didn’t study enough, so it is more likely that I fail than I pass.</p>
<p>We can write a formulae that describes the probability of each event like so:</p>
<div class="math notranslate nohighlight">
\[P(X = x) = P(X = 1)^x*P(X = 0)^{1-x}\]</div>
<p>This is the formulae of the Bernoulli distribution, which describes the outcome of a signle experiment that can have just two possible outcomes</p>
<p>So in the case of a fair coin:</p>
<div class="math notranslate nohighlight">
\[P(X = 0) = 0.5^0*0.5^{1} = 0.5\]</div>
<p>Now at this point you might wonder: what’s the point of the Bernoulli distribution? It’s a function for which we need both <span class="math notranslate nohighlight">\(P(X = 1)\)</span> and <span class="math notranslate nohighlight">\(P(X = 0)\)</span> just so we can compute P(X = 1) or P(X = 0), what’s the point? On it’s own it is not very useful. It is just a way to express the probability of both events using the same formulae. It basically “switches on” P(X=1) and “switches off” P(X=0) when X = 1 and the other way around. And it is something that is very helpful for doing more complicated things, as we will see in a bit.</p>
<p>Let’s just write some code to illustrate the Bernoulli distribution:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="n">ps</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]</span>  <span class="c1"># Showing the probability of P(X=0) and P(X=1) at various values of P(X=1)</span>
<span class="n">P</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">ps</span><span class="p">:</span>
    <span class="n">q</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">p</span>  <span class="c1"># the probability of failure is 1 - the probability of success</span>
    <span class="n">P</span><span class="p">[</span><span class="s2">&quot;X=0&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">p</span><span class="o">**</span><span class="mi">0</span><span class="o">*</span><span class="n">q</span><span class="o">**</span><span class="mi">1</span>  <span class="c1"># P(X=0) = P(x=1)^0 * P(X=0)^(1-0)</span>
    <span class="n">P</span><span class="p">[</span><span class="s2">&quot;X=1&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">p</span><span class="o">**</span><span class="mi">1</span><span class="o">*</span><span class="n">q</span><span class="o">**</span><span class="mi">0</span>  <span class="c1"># P(X=0) = P(x=1)^0 * P(X=0)^(1-0)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">([</span><span class="s2">&quot;P(X=0)&quot;</span><span class="p">,</span> <span class="s2">&quot;P(X=1)&quot;</span><span class="p">],</span> <span class="p">[</span><span class="n">P</span><span class="p">[</span><span class="s2">&quot;X=0&quot;</span><span class="p">],</span> <span class="n">P</span><span class="p">[</span><span class="s2">&quot;X=1&quot;</span><span class="p">]],</span> <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="s1">&#39;green&#39;</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Bernoulli Distribution with P(X=1)=</span><span class="si">{</span><span class="n">p</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Probability&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/c614987ebba094af42fd0db371be1a70038cb66c37b58f00441116ed632d952d.png" src="_images/c614987ebba094af42fd0db371be1a70038cb66c37b58f00441116ed632d952d.png" />
<img alt="_images/47f0c1a320279e0c9aa0543f7a49360203868f59253d8303743a07b532c76ca3.png" src="_images/47f0c1a320279e0c9aa0543f7a49360203868f59253d8303743a07b532c76ca3.png" />
<img alt="_images/62c6931a0e72399d197a6b6955e2828723cf77adc128d3abc5fe7bcea217b898.png" src="_images/62c6931a0e72399d197a6b6955e2828723cf77adc128d3abc5fe7bcea217b898.png" />
</div>
</div>
</section>
<section id="the-binomial-distribution-a-formulae-to-determine-how-likely-a-given-empirical-probability-is-based-on-any-given-true-probability">
<h2>The Binomial distribution: a formulae to determine how likely a given empirical probability is based on any given true probability<a class="headerlink" href="#the-binomial-distribution-a-formulae-to-determine-how-likely-a-given-empirical-probability-is-based-on-any-given-true-probability" title="Link to this heading">#</a></h2>
<section id="what-we-have-seen-so-far">
<h3>What we have seen so far:<a class="headerlink" href="#what-we-have-seen-so-far" title="Link to this heading">#</a></h3>
<p>To recap what we have seen so far: if we want to know whether a coin is biased, we need to figure out if the probability of head is 0.5:</p>
<div class="math notranslate nohighlight">
\[P(X=1) = 0.5\]</div>
<p>And according to the Bernoulli formulae:</p>
<div class="math notranslate nohighlight">
\[P(X = x) = P(X = 1)^x*P(X = 0)^{1-x}\]</div>
<p>But since we know neither P(X = 1) nor P(X = 0), we cannot know for sure whether the coin is biased. Instead, we can run an experiment to obtain the empirical probability of head (and tail):</p>
<div class="math notranslate nohighlight">
\[
\hat{P}(X=1) = \frac{\text{Number of times event } E \text{ occurs}}{\text{Total number of trials}}
\]</div>
<p>As our intuition tells us, the empirical distribution will be more reliable if we have conducted more experiments. In other words, if an event has a given true probability (<span class="math notranslate nohighlight">\(P(X = x)\)</span>), then we are most likely to observe that probability when we run an experiment (<span class="math notranslate nohighlight">\(\hat{P}(X=1)\)</span>). And the more often we repeat that experiment, the less likely it becomes that we observe something different from that probability. This intuition is correct, and as it turns out, we can compute how likely are different experimental outcomes under a given true probability. To put it in the terms of our coin toss experiment, we can compute how likely we are to observe a certain proportion of heads (out of all tosses) if the true probability of head is 0.5, and that likelihood is a direct function of the number of throw we make.</p>
<p>This is written like so:</p>
<p><span class="math notranslate nohighlight">\(P(hat{P}(X=1)|P(X=1))\)</span></p>
<p>Where the <span class="math notranslate nohighlight">\(|\)</span> says given.</p>
</section>
<section id="conditional-probabilities-and-probabilities-of-probabilities">
<h3>Conditional probabilities and probabilities of probabilities<a class="headerlink" href="#conditional-probabilities-and-probabilities-of-probabilities" title="Link to this heading">#</a></h3>
<p>The probability of obtaining a particular ratio of head/tail given the true ratio of head to tail depends on the number of tosses we make. When we do an experiment to obtain the empirical probability of head, we are basically conducting a series of single experiment, where each single experiment can be modelled using the Bernoulli distribution above. It sounds a bit convoluted, because we are now talking about different probabilities at once, and it is important to understand the distinction between them:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(X=1) = 0.5\)</span>: This is the true probability of getting head. We basically want to know if this is true, to be able to answer the question of whether or coin is biased</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{P}(X=1)=heads/nthrow\)</span> / # throw: This is the experimental probability of getting head. We throw the coin n times and we count how often we get head</p></li>
<li><p><span class="math notranslate nohighlight">\(P(hat{P}(X=1)|P(X=1))\)</span>: This is the probability of obtaining a given value for <span class="math notranslate nohighlight">\(\hat{P}(X=1)\)</span> for a given value of <span class="math notranslate nohighlight">\(P(X=1)\)</span>.</p></li>
</ul>
<p>The latter quanity might sound a bit confusing at first. This quantity tells us how likely a given empirical probability of head is if for example our true probability of head is 0.5. Why do we need that? It is because we in fact can never know the true probability of head, and accordingly, we can never know for sure whether the coin is biased. But what we can know however is how likely our observation are if the true probability is 0.5, and in turn, we can know how likely it is that our coin has a probability of head of 0.5.</p>
<p>When we throw our coin a bunch of times, we are generating a sequence of events. But when we zoom out, the sequence we generate is itself random as well, so we are also dealing with probabilities at that level. Say we throw the coin two times, we have the following possible outcomes:</p>
<ul class="simple">
<li><p>Head Head</p></li>
<li><p>Tail Tail</p></li>
<li><p>Tail Head</p></li>
<li><p>Head Tail
So for each single throw, we have 2 possible outcome. The sample space is 2. But when we throw the coin twice, the sample space is 4: we have four possible sequence. If we were to throw the coin 3 times, the possible sequences would be:</p></li>
<li><p>Head Head Head</p></li>
<li><p>Tail Tail Tail</p></li>
<li><p>Head Head Tail</p></li>
<li><p>Head Tail Tail</p></li>
<li><p>Tail Tail Head</p></li>
<li><p>Tail Head Head</p></li>
<li><p>Tail Head Tail</p></li>
<li><p>Head Tail Head</p></li>
</ul>
<p>If we throw the coin three times, the sample space is 8, we have eight possible sequences. We can see a relationship: if we have <span class="math notranslate nohighlight">\(n\)</span> tosses, we have <span class="math notranslate nohighlight">\(2^n\)</span> possible outcomes. Now, say we supposed that our coin is fair (P(X=1) = 0.5), we can know the probability of each given sequence:</p>
<div class="math notranslate nohighlight">
\[P(Head Head Tail) = (P(X = 1)^1*P(X = 0)^0) * (P(X = 1)^1*P(X = 0)^0) * (P(X = 1)^0*P(X = 0)^1)\]</div>
<div class="math notranslate nohighlight">
\[ = P(X = 1)^1 * P(X = 1)^1 * P(X = 0)^1\]</div>
<div class="math notranslate nohighlight">
\[ = 0.5 * 0.5 * 0.5 = 0.125\]</div>
<p>Importantly, for our particular case, we are not so interested in knowing the probability of a particular sequence. We want to know, what is the probability of observing a given number of heads out of the number of toss we make. Accordingly, we want to know how likely we are to observe:</p>
<ul class="simple">
<li><p>Head Head Head (because in all these sequences, we have 3/3 heads)</p></li>
<li><p>Head Head Tail or Tail Head Head or Head Tail Head (because in all these sequences, we have 2/3 heads)</p></li>
<li><p>Tail Tail Head or Tail Head Tail or Head Tail Tail (because in all these sequences, we have 1/3 heads)</p></li>
<li><p>Tail Tail Tail (because in all these sequences, we have 0/3 heads)</p></li>
</ul>
</section>
<section id="the-bernoulli-distribution">
<h3>The Bernoulli distribution:<a class="headerlink" href="#the-bernoulli-distribution" title="Link to this heading">#</a></h3>
<p>Accordingly, we can get the probability for each of these outcomes by summing the probability of sequences in which the number of heads matches:
$<span class="math notranslate nohighlight">\( P(X = 3H) = P(Head Head Head)\)</span><span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\( P(X = 2H) = P(Head Head Tail) + P(Tail Head Head) + P(Head Tail Head)\)</span><span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\(...\)</span>$</p>
<p>As it turns out, there is also a formulae that enables to compute for a given probability of success and a number of observation, what the probability of observing a given number of success is. This is the Binomial distribution:
$<span class="math notranslate nohighlight">\(
P(X = k) = \binom{n}{k} p^k (1 - p)^{n - k}
\)</span><span class="math notranslate nohighlight">\(
Where \)</span>n<span class="math notranslate nohighlight">\( is the number of experiments (i.e. throws), \)</span>k<span class="math notranslate nohighlight">\( is the number of success, p is still \)</span>P(X=1)$ for a single coin toss.</p>
</section>
<section id="usage-of-the-binomial-distribution">
<h3>Usage of the Binomial distribution<a class="headerlink" href="#usage-of-the-binomial-distribution" title="Link to this heading">#</a></h3>
<p>With this formulae, we can compute the quantity we are interested in. Let’s see how:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">math</span>

<span class="k">def</span> <span class="nf">binomial_distribution</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Calculate the binomial probability P(X = k) for n trials, k successes, and success probability p.</span>
<span class="sd">    P(X = k) = \binom{n}{k} p^k (1 - p)^{n - k}</span>
<span class="sd">    :param n: Total number of trials</span>
<span class="sd">    :param k: Number of successes</span>
<span class="sd">    :param p: Probability of success on a single trial</span>
<span class="sd">    :return: Binomial probability P(X = k)</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="c1"># Calculate the binomial coefficient (n choose k)</span>
    <span class="n">binom_coeff</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">comb</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>  <span class="c1"># Calculate n choose k: \binom{n}{k}</span>
    
    <span class="c1"># Calculate the binomial probability using the formula</span>
    <span class="n">probability</span> <span class="o">=</span> <span class="n">binom_coeff</span> <span class="o">*</span> <span class="p">(</span><span class="n">p</span> <span class="o">**</span> <span class="n">k</span><span class="p">)</span> <span class="o">*</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="n">k</span><span class="p">))</span>
    
    <span class="k">return</span> <span class="n">probability</span>
</pre></div>
</div>
</div>
</div>
<p>We have now created a function that can take a given number of toss, a given number of head, a given probability of head and tell us how likely that observation is. We can now go back to the experiments we ran above, and see what that would look like</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Assuming our coin is unbiased:</span>
<span class="n">p</span> <span class="o">=</span> <span class="mf">0.5</span>

<span class="c1"># In the first example, we simulated throwing the coin 10 times, and got 3 times head</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;P(X = 3/10 heads)=</span><span class="si">{</span><span class="n">binomial_distribution</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="n">p</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>P(X = 3/10 heads)=0.1171875
</pre></div>
</div>
</div>
</div>
<p>According to the binomial distribution, we have 11% chances of getting 3 times head out of 10 throws. In other words, if we were to repeat the same experiment (throwing an unbiased coin 10 times) a 100 times, we would obtain 3 heads about 12 times*.</p>
<p>In just the same way as before, we can get the probability of obtaining the results we got when running a 1000 iterations:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># In the first example, we simulated throwing the coin 10 times, and got 3 times head</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;P(X = 500/1000 heads)=</span><span class="si">{</span><span class="n">binomial_distribution</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span><span class="w"> </span><span class="mi">500</span><span class="p">,</span><span class="w"> </span><span class="n">p</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>P(X = 500/1000 heads)=0.0252250181783608
</pre></div>
</div>
</div>
</div>
<p>Interestingly, the probability of getting exactly 50% head when we do a 1000 toss is lower than the probability to get 30% head when we throw the coin only 10 times. This seems counter-intuitive, as we said that the more throw we perform, the more we can trust our estimate. But how can that be true if our estimate is so unlikely according to the Binomial distribution? This is because when we throw the coin a 1000 times, we have many more possible outcomes (<span class="math notranslate nohighlight">\(2^{1000}\)</span>), so ultimately, any single outcome is less likely.</p>
<p>But let’s ask a different question by tweaking the parameters differently. When we throw the coin only 10 times and our true P(X=1) = 0.5, how likely are we to obtain 2/10 heads?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;P(X = 1/10 heads)=</span><span class="si">{</span><span class="n">binomial_distribution</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="n">p</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>P(X = 1/10 heads)=0.0439453125
</pre></div>
</div>
</div>
</div>
<p>We have 4% chances of getting 2/10 heads. Now let’s compare it to the probability of getting 20 heads when we do a 1000 throws:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;P(X = 20% heads)=</span><span class="si">{</span><span class="n">binomial_distribution</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span><span class="w"> </span><span class="mi">200</span><span class="p">,</span><span class="w"> </span><span class="n">p</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>P(X = 20% heads)=6.175550542739598e-86
</pre></div>
</div>
</div>
</div>
<p>We can see that with more throws, it is very very unlikely for us to get 20% heads. So it matches our institution: the more times we repeat the experiment, the less likely we are to get values that are far away from the true probability of head.</p>
</section>
<section id="graphical-representation">
<h3>Graphical representation<a class="headerlink" href="#graphical-representation" title="Link to this heading">#</a></h3>
<p>I showed an example with taking a few random numbers to illustrate my point. But with the Binomial distribution formulae, we know for any given probability of head (%P(X=1)<span class="math notranslate nohighlight">\(, which we will call \)</span>P(\Theta)<span class="math notranslate nohighlight">\(, so that we can refer to the parameter of interest as \)</span>\Theta<span class="math notranslate nohighlight">\( for simplicity) and a given number of throw, what the probability of obtaining a certain observsation (\)</span>Number\ of\ head/Total\ Number\ of\ Throw$)</p>
<p>In other words, we can know:
$<span class="math notranslate nohighlight">\(P(y|P(\Theta))\)</span>$</p>
<p>Let’s say we have 1000 throws, and <span class="math notranslate nohighlight">\(P(\Theta)=0.5\)</span> (i.e. a fair coin), we can compute the probability of occurence of any numbers of head <span class="math notranslate nohighlight">\(y\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_throw</span> <span class="o">=</span> <span class="mi">1000</span>  <span class="c1"># Number of throw</span>
<span class="n">theta</span> <span class="o">=</span> <span class="mf">0.5</span>  <span class="c1"># Probability of obtaining head</span>
<span class="n">n_head</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_throw</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># We want to know the probability of getting 0 heads, 2 heads... up to a 1000 heads out of our 1000 throws</span>


<span class="n">distribution</span> <span class="o">=</span> <span class="p">[</span><span class="n">binomial_distribution</span><span class="p">(</span><span class="n">n_throw</span><span class="p">,</span><span class="n">k</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">n_head</span><span class="p">]</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">k</span><span class="o">/</span><span class="n">n_throw</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">n_head</span><span class="p">)],</span> <span class="n">distribution</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;X = k&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;P(X = k)&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Binomial probability distribution at P(X=1) = 0.5&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;function matplotlib.pyplot.close(fig: &quot;None | int | str | Figure | Literal[&#39;all&#39;]&quot; = None) -&gt; &#39;None&#39;&gt;
</pre></div>
</div>
<img alt="_images/131eb1823db1d964f78146d1f70f172752d0ea1c231787def2fd489fcefb1acd.png" src="_images/131eb1823db1d964f78146d1f70f172752d0ea1c231787def2fd489fcefb1acd.png" />
</div>
</div>
<p>We can see that the probability of getting 0 out of a 1000 throw is very unlikely if the true theta is 0.5, and that the most likely value is getting 500/1000 throw. Makes sense. We can also have a look at what that distribution would look like if we do only 10 throws:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_throw</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># Number of throw</span>
<span class="n">theta</span> <span class="o">=</span> <span class="mf">0.5</span>  <span class="c1"># Probability of obtaining head</span>
<span class="n">n_head</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_throw</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># We want to know the probability of getting 0 heads, 2 heads... up to a 1000 heads out of our 1000 throws</span>

<span class="n">distribution</span> <span class="o">=</span> <span class="p">[</span><span class="n">binomial_distribution</span><span class="p">(</span><span class="n">n_throw</span><span class="p">,</span><span class="n">k</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">n_head</span><span class="p">]</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">k</span><span class="o">/</span><span class="n">n_throw</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">n_head</span><span class="p">)],</span> <span class="n">distribution</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;X = k/n&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;P(X = k/n)&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Binomial probability distribution at P(X=1) = 0.5&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>This also matches our previous intuition: the probability distribution when we do only 10 throws is much broader compared to when we do a 1000 throws, which means that the probability of getting values that are further away from the true <span class="math notranslate nohighlight">\(\theta\)</span> parameter is larger when we have few throws. Accordingly, we should trust the results of the experiment with more throws more than that of the experiment with few throws. Everything falls into place.</p>
</section>
<section id="on-the-usefulness-of-mathematical-formalism">
<h3>On the usefulness of mathematical formalism<a class="headerlink" href="#on-the-usefulness-of-mathematical-formalism" title="Link to this heading">#</a></h3>
<p>Hopefully, by now you realize why adopting some mathematical formalism to what we guessed intuitively is useful. We could also have generated very similar results using the same kind of simulation we did before. Say we want to know how likely it is to get 0.1, 0.2, 0.3… head/tail ratio when we do 10 throws and the true ratio is 50/50. We could just take a coin that we think is fair, repeat the experiment of throwing the coin 10 times many many times, and see how often each outcome occurs. Let’s give that a try:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># We repeat an experiment in which we throw the coin 10 times a 1000 times, and each time we count the proportion of heads</span>
<span class="n">n_iteration</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">n_throws</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iteration</span><span class="p">):</span>
    <span class="n">n_throw</span> <span class="o">=</span> <span class="mi">10</span> <span class="c1"># 20 throws instead of 10</span>
    <span class="n">n_head</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># Before we start, we have zero head</span>
    <span class="n">n_tail</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># And zero tails</span>

    <span class="k">for</span> <span class="n">ii</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_throws</span><span class="p">):</span>  <span class="c1"># Repeat the same thing 10 times (throwing the coin)</span>
        <span class="n">rnd</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">()</span>  <span class="c1"># Draw a random number between 0 and 1 (following a uniform distribution, so each value between 0 and 1 is equally likely)</span>
        <span class="k">if</span> <span class="n">rnd</span> <span class="o">&lt;=</span> <span class="mf">0.5</span><span class="p">:</span>  <span class="c1"># If our random number is less than 0.5, we consider that our coin landed on head.</span>
            <span class="n">n_head</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>   <span class="c1"># If our random number is more than 0.5, we consider that our coin landed on tail</span>
            <span class="n">n_tail</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="c1"># Compute the probability of head and tail:</span>
    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">n_head</span><span class="o">/</span><span class="n">n_throw</span><span class="p">)</span>   <span class="c1"># The probability of head is simply how often we obtained head in our 10 throw, divided by the number throws</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;X = k/n&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">hat</span><span class="si">{P}</span><span class="s2">$(X = k/n)&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Binomial probability distribution at P(X=1) = 0.5&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/6e05d438dca4840d14b6d25218cee720305cb6bc0dfc5d3e9aff2755667b9022.png" src="_images/6e05d438dca4840d14b6d25218cee720305cb6bc0dfc5d3e9aff2755667b9022.png" />
</div>
</div>
<p>That seems a bit similar to the graph we obtained when using the formula, but it is not as clearn. That is because we only used a 1000 repetitions. If we were to use more, we would get something better. Let’s try:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># We repeat an experiment in which we throw the coin 10 times a 1000 times, and each time we count the proportion of heads</span>
<span class="n">n_iteration</span> <span class="o">=</span> <span class="mi">1000000</span>
<span class="n">n_throws</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iteration</span><span class="p">):</span>
    <span class="n">n_throw</span> <span class="o">=</span> <span class="mi">10</span> <span class="c1"># 20 throws instead of 10</span>
    <span class="n">n_head</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># Before we start, we have zero head</span>
    <span class="n">n_tail</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># And zero tails</span>

    <span class="k">for</span> <span class="n">ii</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_throws</span><span class="p">):</span>  <span class="c1"># Repeat the same thing 10 times (throwing the coin)</span>
        <span class="n">rnd</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">()</span>  <span class="c1"># Draw a random number between 0 and 1 (following a uniform distribution, so each value between 0 and 1 is equally likely)</span>
        <span class="k">if</span> <span class="n">rnd</span> <span class="o">&lt;=</span> <span class="mf">0.5</span><span class="p">:</span>  <span class="c1"># If our random number is less than 0.5, we consider that our coin landed on head.</span>
            <span class="n">n_head</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>   <span class="c1"># If our random number is more than 0.5, we consider that our coin landed on tail</span>
            <span class="n">n_tail</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="c1"># Compute the probability of head and tail:</span>
    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">n_head</span><span class="o">/</span><span class="n">n_throw</span><span class="p">)</span>   <span class="c1"># The probability of head is simply how often we obtained head in our 10 throw, divided by the number throws</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;X = k/n&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">hat</span><span class="si">{P}</span><span class="s2">$(X = k/n)&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Binomial probability distribution at P(X=1) = 0.5&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/fb5caf378cd90608bc229894095b47eb25bcb02bf936d3b9afd8eb5011e35ef9.png" src="_images/fb5caf378cd90608bc229894095b47eb25bcb02bf936d3b9afd8eb5011e35ef9.png" />
</div>
</div>
<p>That seems better, closer to what we obtained with the formulae. But still not perfect. But you will notice that there is another problem: it took us several seconds to get to something okay, whereas when using the Binomial formulae, it was hardly a second. Accordingly, having a formulae to compute the probability of observing a particular outcomes, given a value of the parameter of interest (<span class="math notranslate nohighlight">\(P(y|\theta)\)</span>) is twofold:</p>
<ul class="simple">
<li><p>It is exact (while simulations are just approximate)</p></li>
<li><p>It is very very fast: we just need to enter the values we got one and that’s it
That’s what we mean with “having an analytical solution”</p></li>
</ul>
<p>And with the formulae, we can play around with any parameters. Say we want to know <span class="math notranslate nohighlight">\(P(y|\theta)\)</span> for any possible y and any possible <span class="math notranslate nohighlight">\(\theta\)</span>, assuming we threw the coin a 10 times. We can very simply compute that:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Say we want to know the probability of getting any observations between 0 and 1 for any true probability of success between 0 and 1, assuming we throw the coin a 1000 times:</span>
<span class="n">n_throws</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># Number of trials</span>
<span class="n">y_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n_throws</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span> <span class="c1"># Numbers of successes</span>
<span class="n">theta_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.01</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>  <span class="c1"># Probability of success (p) ranging from 0 to 1 in steps of 0.01</span>

<span class="c1"># Create meshgrid for k and p</span>
<span class="n">k_mesh</span><span class="p">,</span> <span class="n">p_mesh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">y_values</span><span class="p">,</span> <span class="n">theta_values</span><span class="p">)</span>

<span class="c1"># Calculate the binomial distribution for each combination of k and p</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">binomial_distribution</span><span class="p">(</span><span class="n">n_throws</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">y_values</span><span class="p">]</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">theta_values</span><span class="p">])</span>

<span class="c1"># Create a 3D plot</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>

<span class="c1"># Plot the surface</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">k_mesh</span><span class="o">/</span><span class="n">n_throws</span><span class="p">,</span> <span class="n">p_mesh</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">)</span>
<span class="c1"># Labeling the axes</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">Theta$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s2">&quot;P(y|$</span><span class="se">\\</span><span class="s2">Theta$)&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Probability of each outcome y, for each $\Theta$ (when we throw the coin 10 times)&quot;</span><span class="p">)</span>

<span class="c1"># Show the plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;&gt;:22: SyntaxWarning: invalid escape sequence &#39;\T&#39;
&lt;&gt;:22: SyntaxWarning: invalid escape sequence &#39;\T&#39;
C:\Users\alexander.lepauvre\AppData\Local\Temp\ipykernel_18920\3306174890.py:22: SyntaxWarning: invalid escape sequence &#39;\T&#39;
  ax.set_title(&quot;Probability of each outcome y, for each $\Theta$ (when we throw the coin 10 times)&quot;)
</pre></div>
</div>
<img alt="_images/af5a8238dcf822f57615753be447cf06d8be92d7af6ef8ba6458854b0ebbc30b.png" src="_images/af5a8238dcf822f57615753be447cf06d8be92d7af6ef8ba6458854b0ebbc30b.png" />
</div>
</div>
<p>This graph may seem a bit confusing at first, but spend some time trying to understand it. I swear it’s not all that complicated</p>
</section>
</section>
<section id="the-bayes-theorem-from-p-y-theta-to-p-theta-y">
<h2>The Bayes theorem: from <span class="math notranslate nohighlight">\(P(y|\theta)\)</span>, to <span class="math notranslate nohighlight">\(P(\theta|y)\)</span><a class="headerlink" href="#the-bayes-theorem-from-p-y-theta-to-p-theta-y" title="Link to this heading">#</a></h2>
<p>Using the binomial formula, we can calculate <span class="math notranslate nohighlight">\(P(y|\theta)\)</span>, which is the probability of observing our data <span class="math notranslate nohighlight">\(y\)</span> (e.g., getting a certain number of heads) given any value of <span class="math notranslate nohighlight">\(\theta\)</span> (the probability of getting heads on a single toss). But again, what we really want to know is how confident we should be about the actual value of <span class="math notranslate nohighlight">\(\theta\)</span>, given the data we have observed. In other words, we’re seeking <span class="math notranslate nohighlight">\(P(\theta|y)\)</span>: the probability of a particular value of <span class="math notranslate nohighlight">\(\theta\)</span> (in our case, <span class="math notranslate nohighlight">\(\theta=0.5\)</span>). So we need to flip <span class="math notranslate nohighlight">\(P(y|\theta)\)</span> to obtain <span class="math notranslate nohighlight">\(P(\theta|y)\)</span>.</p>
<p>The Bayesian theorem enables us to do exactly that. It provides a mathematical formulae to obtain the probability of a parameter (or several) parameters, given the data we have observed. You will often hear that the Bayes theorem is a mathematical framework to update our beliefs about an unknown parameter based on empirical data. This is exactly what we have been trying to do since the beginning, just phrased in a different way. We want to know if our coin is biased, and for that we run an experiment to try to decide whether it is biased or not. This is the same as saying: I believe that the coin is balanced, and I want to know whether this belief is true based on something I have observed. Not that this is also (almost) the same as saying “I believe that this coin is not balanced, and I want to know whether this belief is correct based on my observations”.</p>
<p>The Bayes theorem is a single formulae:
$<span class="math notranslate nohighlight">\(P(\Theta|y) = \frac{P(y|\Theta)*P(\Theta)}{P(y)}\)</span>$</p>
<p>We have already seen <span class="math notranslate nohighlight">\(P(y|\Theta)\)</span>, this is the conditional probability of our observation given any value of <span class="math notranslate nohighlight">\(\theta\)</span>, which is the Binomial distribution. There are two additional terms we haven’t seen before: <span class="math notranslate nohighlight">\(P(\Theta)\)</span> and <span class="math notranslate nohighlight">\(P(y)\)</span>. The first (<span class="math notranslate nohighlight">\(P(\Theta)\)</span>) is referred to as the <strong>prior</strong>, and the second <span class="math notranslate nohighlight">\(P(y)\)</span> is the marginal likelihood or model evidence.</p>
</section>
<section id="a-little-aside-we-can-compute-the-empirical-probability-of-obtaining-3-10-heads">
<h2>*A little aside: we can compute the empirical probability of obtaining 3/10 heads+<a class="headerlink" href="#a-little-aside-we-can-compute-the-empirical-probability-of-obtaining-3-10-heads" title="Link to this heading">#</a></h2>
<p>In fact, we can very easily verify that this is true by running another little simulation (very much the same we did before):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Let&#39;s repeat the 10 coin tosses a 10000 times, just to we get closer to the actual value:</span>
<span class="n">n_iteration</span> <span class="o">=</span> <span class="mi">10000</span>

<span class="c1"># ========================================================</span>
<span class="c1"># Experiment 1:</span>
<span class="n">P</span><span class="p">[</span><span class="s2">&quot;X=3/10 heads&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iteration</span><span class="p">):</span>
    <span class="n">n_throw</span> <span class="o">=</span> <span class="mi">10</span> <span class="c1"># 20 throws instead of 10</span>
    <span class="n">n_head</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># Before we start, we have zero head</span>
    <span class="n">n_tail</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># And zero tails</span>

    <span class="k">for</span> <span class="n">ii</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_throw</span><span class="p">):</span>  <span class="c1"># Repeat the same thing 10 times (throwing the coin)</span>
        <span class="n">rnd</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">()</span>  <span class="c1"># Draw a random number between 0 and 1 (following a uniform distribution, so each value between 0 and 1 is equally likely)</span>
        <span class="k">if</span> <span class="n">rnd</span> <span class="o">&lt;=</span> <span class="mf">0.5</span><span class="p">:</span>  <span class="c1"># If our random number is less than 0.5, we consider that our coin landed on head.</span>
            <span class="n">n_head</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>   <span class="c1"># If our random number is more than 0.5, we consider that our coin landed on tail</span>
            <span class="n">n_tail</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">n_head</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
        <span class="n">P</span><span class="p">[</span><span class="s2">&quot;X=3/10 heads&quot;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;P(X = 3/10 heads)=</span><span class="si">{</span><span class="n">P</span><span class="p">[</span><span class="s2">&quot;X=3/10 heads&quot;</span><span class="p">]</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">n_iteration</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>P(X = 3/10 heads)=0.1149
</pre></div>
</div>
</div>
</div>
<p>So as you can see, when we run the same experiment 10000 times, we get about 12% of the times 3 heads, in line with the Binomial distribution. This may seem familiar with how I described above how we can obtain an empirial probability. With binomial distribution, we get <span class="math notranslate nohighlight">\(P(X=3)\)</span>, and with simulation above, we get <span class="math notranslate nohighlight">\(\hat{P}(X=3)\)</span></p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="intro.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Welcome to your Jupyter Book</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#intuitions-about-probabilities-using-a-coin-toss-example">Intuitions about probabilities using a coin toss example</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#running-simulated-experiments">Running (simulated) experiments</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-the-data">Interpreting the data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#running-a-better-experiment">Running a better experiment</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#answering-the-question-based-on-our-data">Answering the question based on our data</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#formalizing-our-intuition">Formalizing our intuition</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#definitions">Definitions:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-bernoulli-distribution-a-single-formulae-to-express-the-probability-of-each-event">The Bernoulli distribution: a single formulae to express the probability of each event</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-binomial-distribution-a-formulae-to-determine-how-likely-a-given-empirical-probability-is-based-on-any-given-true-probability">The Binomial distribution: a formulae to determine how likely a given empirical probability is based on any given true probability</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-we-have-seen-so-far">What we have seen so far:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-probabilities-and-probabilities-of-probabilities">Conditional probabilities and probabilities of probabilities</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-bernoulli-distribution">The Bernoulli distribution:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#usage-of-the-binomial-distribution">Usage of the Binomial distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#graphical-representation">Graphical representation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#on-the-usefulness-of-mathematical-formalism">On the usefulness of mathematical formalism</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-bayes-theorem-from-p-y-theta-to-p-theta-y">The Bayes theorem: from <span class="math notranslate nohighlight">\(P(y|\theta)\)</span>, to <span class="math notranslate nohighlight">\(P(\theta|y)\)</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-little-aside-we-can-compute-the-empirical-probability-of-obtaining-3-10-heads">*A little aside: we can compute the empirical probability of obtaining 3/10 heads+</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The Jupyter Book Community
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>