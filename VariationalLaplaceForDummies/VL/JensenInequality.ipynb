{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jensen inequality\n",
    "\n",
    "## Back from the beginning (again)\n",
    "To know how we got here, let's recap everything since the beginning. I know, it's a lot of repetition, but I find it helps cement the concepts in ones mind. If you don't agree, skip to the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### The linear model\n",
    "Let's write down again our model:\n",
    "\n",
    "$$y = \\bold{X}\\bold{\\beta} + \\epsilon$$\n",
    "\n",
    "Where:\n",
    "- $y= \\begin{bmatrix}y_1\\\\y_2\\\\...\\end{bmatrix}$: is a vector containing each data point we are trying to model (i.e. penguin weight)\n",
    "- $\\bold{X}= \\begin{bmatrix}x_{0, 1},\\ x_{1, 1},\\ ...\\\\x_{0, 2},\\ x_{1, 2},\\ ...\\\\...,\\ ...,\\ ...\\end{bmatrix}$: is a matrix containing the regressor, also called the **design matrix**. Each column contains all the value of one regressor, each row the value of all regressor associated with each data point. In our example, $x_0$ is the intercept, so it's the same in each row: $x_{0_1} = x_{0_2} = 1$, while $x_1$ is the weight of our penguins, so $x_{1, 1}$ is the weight of the first penguin whose weight is $y_1$ and so on for all our penguins\n",
    "  \n",
    "- $\\bold{\\beta}=\\begin{bmatrix}\\beta_0\\\\\\beta_1\\\\...\\end{bmatrix}$: the weight of our regressors. We don't know the true values, but can estimate the **observed values** of these parameters that generate a line that fits the data the best, i.e. that minimize the residuals sum of square (RSS) by using OLS (see below). The values found by the OLS are the maximum likelihood estimates (MLE), which means that the likelihood of the data is maximized under these parameter (see likelihood function below)\n",
    "  \n",
    "- $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$: the error term follows a normal distribution of mean 0 and of variance $\\sigma^2$. It is an assumption, meaning that if this is not true, we shouldn't use this kind of model.  We assume that the mean of the distribution is 0, which means that negative errors (underestimations) are equally likely as positive errors (overestimations). The $\\sigma^2$ on the other hands is not assumed and depends on the data. The variance parameter of the distribution controls its spread, meaning that large $\\sigma^2$ implies that large error are more likely compared to when $\\sigma^2$ is small. and we can also find the MLE for this parameters, i.e. the value of $\\sigma^2$ under which the data are the most likely\n",
    "\n",
    "In this model, the $y$ and $\\bold{X}$ are fixed: it is the data we observed (i.e. measured, collected...). The likelihood of observing these values depends on the value of the parameters $\\beta$ and $\\sigma^2$. Accordingly, we cvan try to find the $\\beta$ and $\\sigma^2$ under which the data are most likely, which is what we do when we **fit the model** to the data. The formula for finding the MLE for the $\\beta$ is know as the **optimal least square (OLS)**:\n",
    "\n",
    "$$\\hat{\\beta} = (\\bold{X}^T\\bold{X})^-1\\bold{X}^Ty$$\n",
    "\n",
    "This formula looks a bit different to the OLS formulae we saw before for calculating $\\hat{\\beta_1}$ and $\\hat{\\beta_0}$, beacuse it is in vector format and it general to calculate all the $\\beta$ even if you have more than 2.\n",
    "\n",
    "The MLE of the variance term of the $\\epsilon$ distribution is:\n",
    "\n",
    "$$\\hat{\\sigma}^2 = \\frac{(y-\\bold{X}\\bold{\\hat{\\beta}})^2}{n}$$\n",
    "\n",
    "In the case of the $\\epsilon$ of our model, we don't write that it is equal to the normal distribution, because the error follows a distribution or **probability density function** but it is not equal to it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### The Bayes theorem in the case of the linear model\n",
    "When we ask the question: \"Is there a relationship between penguins' fipper lenghts and their weights\", we want to draw an inference on the true value of the $\\beta$ parameter. We can never know the true value, but we can infer the likelihood of the true value of the parameter given the data, for which we need the Bayes theorem, which states that:\n",
    "\n",
    "$$P(\\Theta|y) = \\frac{P(y|\\Theta)P(\\Theta)}{P(y)}$$\n",
    "\n",
    "In our specific case, we have:\n",
    "\n",
    "$$P(\\beta, \\sigma^2|y) = \\frac{P(y|\\beta, \\sigma^2)P(\\beta, \\sigma^2)}{P(y)}$$\n",
    "\n",
    "Where $\\beta$ is a vector containing the values of the various beta of our model. So in our penguin example, it contains both $\\beta_0$ and $\\beta_1$\n",
    "\n",
    "The likelihood of the data is a multivariate normal distribution (because the error is normally distrubuted around 0), defined as:\n",
    "\n",
    "$$P(y|\\beta, \\sigma^2) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}}exp(-\\frac{1}{2\\sigma^2}(y-\\bold{X}\\beta)^T)(y-\\bold{X}\\beta)$$\n",
    "\n",
    "The prior distribution is:\n",
    "\n",
    "$$P(\\beta, \\sigma^2) = P(\\beta) \\times P(\\sigma^2)$$\n",
    "\n",
    "Where $P(\\beta)$ is also a multivariate normal distribution, like so:\n",
    "\n",
    "$$P(\\beta) = \\frac{1}{(2\\pi)^{p/2}|\\mathcal{\\Sigma}|^{1/2}}exp(-\\frac{1}{2}(\\mathcal{\\beta} - \\mathcal{\\mu})^T\\Sigma^{-1}(\\mathcal{\\beta}-\\mathcal{\\mu}))$$\n",
    "\n",
    "    Where:\n",
    "        - $\\mu$ is the prior mean, meaning the value we think is the most likely for each $\\beta$ parameter\n",
    "        - $\\Sigma$ is the prior variance covariance matrix. Along the diagonal, it contains the variance of the prior distribution of each $\\beta$ parameter (i.e. how likely we think the true value of the $\\beta$ parameter is really far away from the mean we declared) and off the diagonal is the covariance between two $\\beta$ parameters. We should set the off diagonal to 0 if we think that each pair of $\\beta$ are independent from each other and to non-zero otherwise. What we mean by $\\beta$ not being independent is if you have reasons to believe that observing a certain value of $\\beta$ for one parameter is likely to result in another $\\beta$ having a larger or smaller value (depending on the sign of the covariance)\n",
    "\n",
    "And $P(\\sigma^2)$ is:\n",
    "\n",
    "$$P(\\sigma^2) = \\frac{b^\\alpha}{\\Gamma(\\alpha)}(\\sigma^2)^{-\\alpha-1}exp(-\\frac{b}{\\sigma^2})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can't compute the marginal likelihood but...\n",
    "The reason the book isn't finished already is because we have a problem: we can't compute the marginal likelihood. The marginal likelihood"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
